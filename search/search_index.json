{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Loreley","text":"<p>Whole-repository Quality-Diversity optimization for real git codebases.</p> <p>Loreley is an automated Quality-Diversity optimization system that evolves entire git repositories, not just single files or scripts. It continuously samples promising commits, asks external agents to plan and implement changes, evaluates them, and archives the best-performing and most diverse variants for later reuse.</p> <p>Use this page as a high-level overview and a navigation hub into the focused module guides under <code>loreley/</code> and <code>script/</code> (see the sidebar navigation).</p>"},{"location":"#challenges-and-core-ideas","title":"Challenges and core ideas","text":"<p>Loreley is built around three core ideas, each designed to address a concrete challenge in real-world code evolution:</p> Challenge in real repositories Loreley core idea Single-file evolution cannot express cross-module refactors and production changes Whole-repo evolution Hand-crafted behaviour descriptors do not generalise across projects Learned behaviour space Demo-style pipelines do not scale to distributed, long-running operation Production-grade distributed loop <p>Related systems include AlphaEvolve, OpenEvolve, and ShinkaEvolve.</p>"},{"location":"#methodology","title":"Methodology","text":"<p>Loreley treats software evolution as quality-diversity search over the commit graph of a real repository, guided by a learned behaviour space and driven by a production-grade distributed loop. Instead of using LLMs as one-shot patch generators, it organises planning, editing, evaluation, and archiving into a repeatable system that can safely explore improvements while remaining auditable (git), testable (evaluator), and operable (scheduler + workers).</p>"},{"location":"#system-overview","title":"System overview","text":"<p>At a high level, Loreley sits between your git repository, a pool of LLM-based agents, and a MAP-Elites archive:</p> <pre><code>flowchart LR\n  repo[\"Git repository&lt;br/&gt;(target project)\"]\n  sched[\"Scheduler&lt;br/&gt;(EvolutionScheduler)\"]\n  queue[\"Redis / Dramatiq&lt;br/&gt;(job queue)\"]\n  w1[\"Evolution worker 1\"]\n  wN[\"Evolution worker N\"]\n  db[(\"PostgreSQL&lt;br/&gt;(experiments + metrics)\")]\n  archive[\"MAP-Elites archive&lt;br/&gt;(learned behaviour space)\"]\n\n  repo --&gt; sched\n  sched --&gt;|enqueue evolution jobs| queue\n  queue --&gt; w1\n  queue --&gt; wN\n\n  w1 --&gt;|checkout + push commits| repo\n  wN --&gt;|checkout + push commits| repo\n\n  w1 --&gt; db\n  wN --&gt; db\n\n  db --&gt; archive\n  archive --&gt;|sample base commits| sched</code></pre> <ul> <li>Scheduler keeps the experiment in sync with the repository, ingests completed jobs, samples new base commits from the MAP-Elites archive, and enqueues evolution jobs.</li> <li>Workers check out base commits, call external planning/coding/evaluation agents, create new commits, and persist metrics.</li> <li>Archive stores a diverse set of high-performing commits in a learned behaviour space that the scheduler uses to inspire the next round of jobs.</li> </ul>"},{"location":"#quick-start","title":"Quick start","text":""},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.11+</li> <li><code>uv</code> for dependency management</li> <li>PostgreSQL and Redis</li> <li>Git (including worktrees; LFS optional)</li> </ul>"},{"location":"#install-dependencies","title":"Install dependencies","text":"<pre><code>git clone &lt;YOUR_FORK_OR_ORIGIN_URL&gt; loreley\ncd loreley\nuv sync\n</code></pre>"},{"location":"#configure","title":"Configure","text":"<p>All runtime configuration is provided via environment variables and loaded by <code>loreley.config.Settings</code>. Start with:</p> <ul> <li><code>APP_NAME</code>, <code>APP_ENV</code>, <code>LOG_LEVEL</code></li> <li><code>DATABASE_URL</code></li> <li><code>TASKS_REDIS_URL</code>, <code>TASKS_QUEUE_NAME</code></li> <li><code>SCHEDULER_REPO_ROOT</code>, <code>WORKER_REPO_REMOTE_URL</code></li> </ul> <p>See: Configuration</p>"},{"location":"#run","title":"Run","text":"<pre><code>uv run python script/run_scheduler.py\nuv run python script/run_worker.py\n</code></pre> <p>See: Running the scheduler, Running the worker</p>"},{"location":"#core-ideas-in-practice","title":"Core ideas in practice","text":""},{"location":"#whole-repo-evolution","title":"Whole-repo evolution","text":"<p>Whole-repo evolution makes the git commit the fundamental unit of search. This solves the practical limitation of single-file optimisation: real improvements often require changing multiple modules, updating configs and build scripts, and keeping tests and tooling intact.</p> <p>Repository-scale evolution has been demonstrated in the literature (for example, SATLUTION), but many repository-scale loops are champion-based and rulebase-driven: a single \u201ccurrent best\u201d becomes the next baseline, and extensive human-authored rules are used to keep the agent on track. This design can limit diversity and makes quality-diversity methods difficult to realise.</p> <p></p> <p>Loreley is designed to be QD-native at repository scale:</p> <ul> <li>it keeps a MAP-Elites archive of multiple elites across behavioural niches (not a single champion line),</li> <li>it samples from those niches as inspirations for new jobs,</li> <li>and it uses evaluator gates + repository semantics as the primary source of constraints, minimising dependence on domain-specific rulebases.</li> </ul>"},{"location":"#learned-behaviour-space","title":"Learned behaviour space","text":"<p>Quality-diversity methods require a behaviour space. Hand-crafted behaviour descriptors (file counts, line deltas, test counts, etc.) are brittle and often project-specific.</p> <p></p> <p>Loreley derives behaviour descriptors from repo-state code embeddings (file-level embeddings cached by git blob SHA and aggregated into a commit vector), optionally reduced with PCA. Summary embeddings remain available as an optional utility, but are not used for MAP-Elites behaviour descriptors in repo-state mode.</p> <p>Under similar fitness, the archive can preserve structurally different improvements (refactors vs micro-optimisations vs feature shifts) as distinct behavioural niches, enabling exploration without collapsing to a single style of change.</p>"},{"location":"#production-grade-distributed-loop","title":"Production-grade distributed loop","text":"<p>Production-grade evolution requires more than an agent loop: it needs distributed execution, resource controls, and persistent traceability.</p> <p>Loreley runs a long-lived loop with:</p> <ul> <li>a scheduler that ingests completed jobs, samples base commits, and enqueues new jobs,</li> <li>a Redis/Dramatiq worker fleet that runs planning/coding/evaluation per job,</li> <li>a PostgreSQL-backed store for experiments, commits, metrics, and archive state,</li> <li>explicit lifecycle controls (max unfinished jobs, optional total job caps, seed population, best-candidate branch export).</li> </ul> <p>You can run a long optimisation campaign on a repository, scaling workers horizontally, while keeping the evolution process reproducible and observable.</p>"},{"location":"#adoption-checklist-is-your-project-a-fit","title":"Adoption checklist (is your project a fit?)","text":"<p>A project is a strong fit for Loreley when these questions have clear, automated answers:</p> <ul> <li>Do you have an evaluator that can run unattended and produce structured metrics (plus pass/fail correctness gates)?</li> <li>Is the evaluation signal comparable across commits and not dominated by noise?</li> <li>Is the per-job evaluation cost acceptable (P50/P95 runtime), and can it be parallelised or staged (smoke test \u2192 full benchmark)?</li> <li>Do meaningful improvements often require cross-file and cross-module changes?</li> <li>Can failures be detected cheaply (compile/test/correctness gates) to avoid wasting full benchmark runs?</li> <li>Can the project tolerate continuous creation of job branches / commits (ideally on a dedicated remote or mirror)?</li> <li>Is there value in keeping multiple diverse strong solutions (trade-offs, strategies, module-level variants), not just a single best commit?</li> </ul>"},{"location":"#what-you-need-to-integrate-a-project","title":"What you need to integrate a project","text":"<p>To hook a repository into Loreley, you typically need:</p> <ul> <li>Repository info: remote URL/branch, LFS/submodules, reproducible environment (toolchains, containers, hardware).</li> <li>Build &amp; test entrypoints: minimal commands for build/test, plus optional staged checks (smoke vs full).</li> <li>Evaluator spec: plugin entrypoint, metrics schema, correctness validation, and any benchmark/data access details.</li> <li>Goal &amp; constraints: the optimisation objective, non-negotiable constraints, acceptance criteria, and forbidden areas.</li> <li>Resources &amp; ops: worker concurrency, CPU/GPU/memory budgets, and runtime/timeouts.</li> </ul>"},{"location":"#estimating-cost-and-roi","title":"Estimating cost and ROI","text":"<p>A practical way to estimate cost/benefit is to run a small pilot (e.g. 20\u201350 jobs) and measure:</p> <ul> <li><code>t_job</code> (time per job):</li> </ul> <pre><code>t_job = t_plan + t_code + t_build + t_eval + t_ingest\njobs_per_day \u2248 workers * 24 / E[t_job]\n</code></pre> <ul> <li><code>p_valid</code> (valid-job rate): fraction of jobs that pass correctness gates and produce usable metrics.</li> <li>improvement distribution <code>\u0394</code>: fitness(new) \u2212 fitness(base) across valid jobs.</li> </ul> <p>From these, you can forecast:</p> <ul> <li>time-to-first-win: how many valid jobs you typically need to see a meaningful improvement,</li> <li>expected best-of-N: how the best improvement grows as you run more valid jobs,</li> <li>$ / improvement: combine LLM + compute costs per job with the observed success rate.</li> </ul>"},{"location":"#documentation-map","title":"Documentation map","text":"<p>Use this index as a quick map of the rest of the documentation:</p> <ul> <li>Configuration</li> <li>Global settings</li> <li>Database</li> <li>Engine and sessions</li> <li>ORM models</li> <li>Experiments</li> <li>Repository &amp; experiment helpers</li> <li>MAP-Elites core</li> <li>Overview &amp; archive</li> <li>Preprocessing</li> <li>Chunking</li> <li>Code embeddings</li> <li>Dimensionality reduction</li> <li>Sampler</li> <li>Snapshots</li> <li>Summary embeddings</li> <li>Worker pipeline</li> <li>Planning agent</li> <li>Coding agent</li> <li>Evaluator</li> <li>Evolution loop</li> <li>Commit summaries</li> <li>Job store</li> <li>Worker repository</li> <li>Scheduler &amp; tasks</li> <li>Scheduler</li> <li>Tasks broker</li> <li>Tasks workers</li> <li>UI (optional)</li> <li>UI API (<code>loreley.api</code>)</li> <li>Streamlit UI (<code>loreley.ui</code>)</li> <li>Operational scripts</li> <li>Run scheduler script</li> <li>Run worker script</li> </ul>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Start by configuring a small test repository and running the scheduler/worker pair locally.</li> <li>Once the basic loop works, plug in a custom evaluator and tune <code>MAPELITES_*</code> settings.</li> <li>When you are ready for production, point the scheduler at a long-lived repository clone and supervise both processes with your preferred process manager.</li> </ul>"},{"location":"loreley/api/","title":"UI API","text":""},{"location":"loreley/api/#ui-api-fastapi","title":"UI API (FastAPI)","text":"<p>Loreley ships an optional read-only HTTP API used by the Streamlit dashboard. The implementation lives in <code>loreley/api</code> and is intentionally scoped to observability: it does not enqueue jobs, stop workers, or mutate the database.</p>"},{"location":"loreley/api/#install","title":"Install","text":"<p>The UI stack dependencies live under the <code>ui</code> extra in <code>pyproject.toml</code>.</p> <pre><code>uv sync --extra ui\n</code></pre>"},{"location":"loreley/api/#run","title":"Run","text":"<p>Start the API via the CLI wrapper:</p> <pre><code>uv run python script/run_api.py\n</code></pre> <p>See also: Running the UI API</p>"},{"location":"loreley/api/#configuration","title":"Configuration","text":"<p>The UI API relies on the standard Loreley settings (<code>loreley.config.Settings</code>), especially database and logs configuration.</p> <p>Common variables:</p> <ul> <li><code>DATABASE_URL</code></li> <li><code>LOGS_BASE_DIR</code> (optional; logs are read from <code>&lt;LOGS_BASE_DIR&gt;/logs</code> or <code>&lt;cwd&gt;/logs</code>)</li> <li><code>LOG_LEVEL</code></li> </ul>"},{"location":"loreley/api/#versioning-and-prefix","title":"Versioning and prefix","text":"<p>All routes are served under the versioned prefix: <code>/api/v1</code>.</p> <p>FastAPI also exposes OpenAPI docs by default:</p> <ul> <li><code>/docs</code> (Swagger UI)</li> <li><code>/redoc</code></li> </ul>"},{"location":"loreley/api/#endpoints-v1","title":"Endpoints (v1)","text":"<ul> <li><code>GET /health</code></li> <li><code>GET /repositories</code></li> <li><code>GET /repositories/{repository_id}/experiments</code></li> <li><code>GET /experiments/{experiment_id}</code></li> <li><code>GET /jobs</code></li> <li><code>GET /jobs/{job_id}</code></li> <li><code>GET /commits</code></li> <li><code>GET /commits/{commit_hash}</code></li> <li><code>GET /archive/islands</code></li> <li><code>GET /archive/records</code></li> <li><code>GET /archive/snapshot_meta</code></li> <li><code>GET /graphs/commit_lineage</code></li> <li><code>GET /logs</code></li> <li><code>GET /logs/tail</code></li> </ul>"},{"location":"loreley/api/#notes","title":"Notes","text":"<ul> <li>Authentication: there is no authentication layer. Deploy behind your internal network controls if exposing remotely.</li> <li>Read-only contract: treat this API as an observability surface, not a control plane.</li> </ul>"},{"location":"loreley/config/","title":"loreley.config","text":"<p>Centralised configuration for the Loreley application, backed by <code>pydantic-settings</code> and environment variables.</p>"},{"location":"loreley/config/#settings","title":"Settings","text":"<ul> <li><code>Settings</code>: <code>BaseSettings</code> subclass that loads core application configuration.</li> <li>Environment: <code>app_name</code>, <code>environment</code>, <code>log_level</code>, <code>logs_base_dir</code>. <code>log_level</code> controls the global Loguru log level used across long-running processes (including the scheduler, workers, and their CLI wrappers); <code>logs_base_dir</code> (via <code>LOGS_BASE_DIR</code>) optionally overrides the base directory where long-running process logs are written, defaulting to a <code>logs/</code> directory under the current working directory. See the scripts documentation under <code>docs/script</code> for concrete examples.</li> <li>OpenAI-compatible API: <code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code>, <code>OPENAI_API_SPEC</code> configure the API key, base URL, and API surface for all OpenAI-compatible LLM and embedding calls, used by <code>loreley.core.map-elites.code_embedding.CodeEmbedder</code>, <code>loreley.core.map-elites.summarization_embedding.SummaryEmbedder</code>, and <code>loreley.core.worker.commit_summary.CommitSummarizer</code>. When unset, <code>OPENAI_API_KEY</code>/<code>OPENAI_BASE_URL</code> fall back to the OpenAI Python client's own environment variable defaults. <code>OPENAI_API_SPEC</code> accepts:<ul> <li><code>\"responses\"</code> (default): use the unified <code>responses</code> API (<code>client.responses.create</code>) for text generation.</li> <li><code>\"chat_completions\"</code>: use the classic Chat Completions API (<code>client.chat.completions.create</code>) while preserving the same high-level behaviour.</li> </ul> </li> <li>Database: either a raw <code>DATABASE_URL</code> or individual <code>DB_*</code> fields (scheme, host, port, username, password, database name, pool options, echo flag).</li> <li>Metrics: <code>metrics_retention_days</code> controls how long metrics are retained.</li> <li>Task queue: <code>TASKS_REDIS_URL</code>, <code>TASKS_REDIS_HOST</code>, <code>TASKS_REDIS_PORT</code>, <code>TASKS_REDIS_DB</code>, <code>TASKS_REDIS_PASSWORD</code>, <code>TASKS_REDIS_NAMESPACE</code>, <code>TASKS_QUEUE_NAME</code>, <code>TASKS_WORKER_MAX_RETRIES</code>, and <code>TASKS_WORKER_TIME_LIMIT_SECONDS</code> configure the Dramatiq Redis broker connection details, logical namespace, queue routing, retry policy, and actor time limits used by <code>loreley.tasks.broker</code> and <code>loreley.tasks.workers</code>. When <code>TASKS_REDIS_URL</code> is set and includes credentials, only a sanitised <code>scheme://host:port/db</code> form is logged, never the raw URL or password. <code>TASKS_WORKER_TIME_LIMIT_SECONDS</code> is interpreted in seconds and converted to Dramatiq's millisecond <code>time_limit</code>: values <code>&lt;= 0</code> disable the time limit (no hard cap on actor runtime), while positive values enforce a per-job wall-clock limit.</li> <li>Scheduler: <code>SCHEDULER_REPO_ROOT</code>, <code>SCHEDULER_POLL_INTERVAL_SECONDS</code>, <code>SCHEDULER_MAX_UNFINISHED_JOBS</code>, <code>SCHEDULER_MAX_TOTAL_JOBS</code>, <code>SCHEDULER_SCHEDULE_BATCH_SIZE</code>, <code>SCHEDULER_DISPATCH_BATCH_SIZE</code>, and <code>SCHEDULER_INGEST_BATCH_SIZE</code> drive <code>loreley.scheduler.main</code>. These options determine which git worktree the scheduler inspects, how often it runs a reconciliation tick, how many unfinished jobs are allowed at once, the optional cap on total jobs, how aggressively it samples new MAP-Elites jobs per tick, how many pending jobs are dispatched to Dramatiq each cycle, and how many completed jobs are ingested back into the archive per tick.</li> <li>Worker repository: <code>WORKER_REPO_REMOTE_URL</code>, <code>WORKER_REPO_BRANCH</code>, <code>WORKER_REPO_WORKTREE</code>, <code>WORKER_REPO_WORKTREE_RANDOMIZE</code>, <code>WORKER_REPO_WORKTREE_RANDOM_SUFFIX_LEN</code>, <code>WORKER_REPO_GIT_BIN</code>, <code>WORKER_REPO_FETCH_DEPTH</code>, <code>WORKER_REPO_CLEAN_EXCLUDES</code>, <code>WORKER_REPO_JOB_BRANCH_PREFIX</code>, <code>WORKER_REPO_ENABLE_LFS</code>, and <code>WORKER_REPO_JOB_BRANCH_TTL_HOURS</code> configure the git worktree used by worker processes (upstream remote and branch, local checkout path, optional randomised suffix for concurrent workers, git binary, shallow clone depth, clean exclusions, job branch naming, optional Git LFS support, and how long remote job branches are retained before pruning), used by <code>loreley.core.worker.repository.WorkerRepository</code>.</li> <li>Worker planning: <code>WORKER_PLANNING_*</code> options configuring how the external Codex CLI planner is invoked (binary path, optional profile, maximum attempts, timeout, extra environment variables, an optional JSON schema override, the Codex schema mode, and the validation mode), used by <code>loreley.core.worker.planning.PlanningAgent</code>. You can override the entire backend via <code>WORKER_PLANNING_BACKEND</code> (dotted <code>module:attr</code>), and <code>WORKER_PLANNING_CODEX_SCHEMA_MODE</code> selects <code>\"auto\"</code> / <code>\"native\"</code> / <code>\"prompt\"</code> / <code>\"none\"</code> when the Codex backend is used. <code>WORKER_PLANNING_VALIDATION_MODE</code> controls how strictly the worker enforces the planner's JSON output:<ul> <li><code>\"strict\"</code>: require the backend to produce JSON that matches the planning schema; both Codex (in native schema mode) and the local Pydantic models validate the payload, and failures cause retries and eventually a hard error.</li> <li><code>\"lenient\"</code> (default): still provide the JSON schema to the backend when applicable, but treat JSON decoding / schema validation failures as non-fatal. The worker first tries to parse the response using the planning schema; if that fails, it synthesises a minimal <code>PlanningPlan</code> from the free-form output while preserving as much structure as possible for downstream consumers.</li> <li><code>\"none\"</code>: disable JSON-based validation entirely. The planner may respond in arbitrary free-form text; the worker skips JSON parsing and always builds a minimal <code>PlanningPlan</code> directly from the raw output and job context.</li> </ul> </li> <li>Worker coding: <code>WORKER_CODING_*</code> options configuring how the external Codex-based coding agent is invoked (binary path, optional profile, maximum attempts, timeout, extra environment variables, an optional JSON schema override, the Codex schema mode, and the validation mode), used by <code>loreley.core.worker.coding.CodingAgent</code>. You can override the backend via <code>WORKER_CODING_BACKEND</code> (dotted <code>module:attr</code>), and <code>WORKER_CODING_CODEX_SCHEMA_MODE</code> selects <code>\"auto\"</code> / <code>\"native\"</code> / <code>\"prompt\"</code> / <code>\"none\"</code> when the Codex backend is used. <code>WORKER_CODING_VALIDATION_MODE</code> follows the same <code>\"strict\"</code> / <code>\"lenient\"</code> / <code>\"none\"</code> semantics as the planning agent, but applied to the coding agent's structured execution report.</li> <li>Cursor backend: <code>WORKER_CURSOR_MODEL</code> selects the model passed to the Cursor Agent CLI (default <code>\"gpt-5.1-codex-max-high\"</code>), used by <code>loreley.core.worker.agent_backend.cursor_backend_from_settings()</code> when wiring Cursor as a backend for planning or coding.   <code>WORKER_CURSOR_FORCE</code> (default <code>true</code>) appends <code>--force</code> so the Cursor agent allows commands unless explicitly denied; set it to <code>false</code> to omit the flag.</li> <li>Worker evaluator: <code>WORKER_EVALUATOR_PLUGIN</code>, <code>WORKER_EVALUATOR_PYTHON_PATHS</code>, <code>WORKER_EVALUATOR_TIMEOUT_SECONDS</code>, and <code>WORKER_EVALUATOR_MAX_METRICS</code> configure the evaluation plugin entry point, additional Python paths, subprocess timeout, and the maximum number of metrics to keep, used by <code>loreley.core.worker.evaluator.Evaluator</code>.</li> <li>Worker evolution commits: <code>WORKER_EVOLUTION_COMMIT_MODEL</code>, <code>WORKER_EVOLUTION_COMMIT_TEMPERATURE</code>, <code>WORKER_EVOLUTION_COMMIT_MAX_OUTPUT_TOKENS</code>, <code>WORKER_EVOLUTION_COMMIT_MAX_RETRIES</code>, <code>WORKER_EVOLUTION_COMMIT_RETRY_BACKOFF_SECONDS</code>, <code>WORKER_EVOLUTION_COMMIT_AUTHOR</code>, <code>WORKER_EVOLUTION_COMMIT_EMAIL</code>, and <code>WORKER_EVOLUTION_COMMIT_SUBJECT_MAX_CHARS</code> configure how the evolution worker generates and records commit subjects (LLM model, sampling behaviour, retry policy, and subject length) and which author identity is used when creating commits, used by <code>loreley.core.worker.commit_summary.CommitSummarizer</code> and <code>loreley.core.worker.repository.WorkerRepository</code>.</li> <li>Worker evolution global goal: <code>WORKER_EVOLUTION_GLOBAL_GOAL</code> provides a single, plain\u2011language evolution objective that is shared across all jobs. When no per\u2011job <code>goal</code>/<code>objective</code>/<code>description</code> is present in the job payload, this value is used as the Global objective in both the planning and coding prompts so that the worker consistently optimises towards a user\u2011defined high\u2011level target.</li> <li>Map-Elites preprocessing: <code>MAPELITES_PREPROCESS_*</code> options controlling which repository code files are considered for feature extraction (limits on file size, allowed extensions/filenames, excluded path globs, whitespace handling, and comment stripping), used by <code>loreley.core.map-elites.preprocess.CodePreprocessor</code> and the repo-state embedding file enumerator.</li> <li>Map-Elites chunking: <code>MAPELITES_CHUNK_*</code> options controlling how preprocessed files are split into chunks (target and minimum lines per chunk, overlap, maximum chunks per file, and boundary keywords used by the chunker), used by <code>loreley.core.map-elites.chunk.CodeChunker</code>.</li> <li>Map-Elites code embedding: <code>MAPELITES_CODE_EMBEDDING_*</code> options configuring the embedding model, optional output dimensions, batch size, per-commit chunk budget, retry count, and exponential backoff for embedding requests, used by <code>loreley.core.map-elites.code_embedding.CodeEmbedder</code>.</li> <li>Map-Elites summary embedding (optional): <code>MAPELITES_SUMMARY_*</code> and <code>MAPELITES_SUMMARY_EMBEDDING_*</code> options configuring the LLM summary model (name, temperature, max output tokens, source excerpt character limit, retries, and backoff) and the summary embedding model (name, dimensions, and batch size), used by <code>loreley.core.map-elites.summarization_embedding.SummaryEmbedder</code>. Repo-state MAP-Elites behaviour descriptors do not include these summary vectors.</li> <li>Map-Elites dimensionality reduction: <code>MAPELITES_DIMENSION_REDUCTION_*</code> options controlling how commit embeddings are normalised, the target feature dimensions, minimum sample count for fitting PCA, rolling history size, and refit cadence, used by <code>loreley.core.map-elites.dimension_reduction.DimensionReducer</code>. In repo-state mode, the input embedding is the repo-state code vector.</li> <li>Map-Elites feature normalization and archive: <code>MAPELITES_FEATURE_TRUNCATION_K</code> sets the symmetric clip radius applied to PCA outputs before they are linearly mapped into <code>[0, 1]^d</code>; <code>MAPELITES_FEATURE_NORMALIZATION_WARMUP_SAMPLES</code> controls the minimum history required before fitting/using PCA (never below <code>MAPELITES_DIMENSION_REDUCTION_MIN_FIT_SAMPLES</code>); and <code>MAPELITES_FEATURE_CLIP</code> toggles defensive clipping. <code>MAPELITES_ARCHIVE_*</code> options continue to configure grid resolution and learning parameters for the underlying MAP-Elites archive, used by <code>loreley.core.map-elites.map-elites.MapElitesManager</code>.</li> <li>Map-Elites fitness and sampling: <code>MAPELITES_FITNESS_*</code> and <code>MAPELITES_SAMPLER_*</code> options that configure which metric to optimise, how to treat fitness direction/floor, and how new jobs are drawn from the archive (inspiration count, neighbour radius, fallback sampling, default priority, and whether to include metadata), used by <code>loreley.core.map-elites.map-elites.MapElitesManager</code> and <code>loreley.core.map-elites.sampler.MapElitesSampler</code>. <code>MAPELITES_SEED_POPULATION_SIZE</code> sets the initial seed population size for new islands.</li> <li>Map-Elites embedding mode and cache: <code>MAPELITES_EMBEDDING_MODE</code> selects how commits are represented for MAP-Elites (currently only supports <code>\"repo_state\"</code>), <code>MAPELITES_FILE_EMBEDDING_CACHE_BACKEND</code> selects the file-level embedding cache backend (<code>\"db\"</code> (default) or <code>\"memory\"</code>), and <code>MAPELITES_REPO_STATE_MAX_FILES</code> optionally caps the number of eligible files considered in repo-state mode (default: unlimited).</li> <li>Map-Elites experiment root commit: <code>MAPELITES_EXPERIMENT_ROOT_COMMIT</code> optionally pins a specific git commit hash as the logical root for a given experiment. When set, the scheduler ensures this commit is recorded in the <code>commits</code> table and ingested into each relevant MAP-Elites island archive before scheduling any evolution jobs, so all subsequent jobs branch from a well-defined starting point.</li> <li><code>database_dsn</code>: computed property that returns a SQLAlchemy-compatible DSN, preferring <code>DATABASE_URL</code> when set and otherwise building one from the individual DB fields (with credentials URL-encoded).</li> <li><code>export_safe()</code>: helper that returns a dict of non-sensitive configuration values suitable for logging.</li> </ul>"},{"location":"loreley/config/#access-helpers","title":"Access helpers","text":"<ul> <li><code>get_settings()</code>: cached factory that instantiates <code>Settings</code>, logs a concise summary of the environment and DB host using <code>rich</code>/<code>loguru</code>, and returns a singleton instance for reuse across the loreley.</li> </ul>"},{"location":"loreley/ui/","title":"Streamlit UI","text":""},{"location":"loreley/ui/#streamlit-ui-loreleyui","title":"Streamlit UI (loreley.ui)","text":"<p>Loreley ships an optional read-only Streamlit dashboard for observability. It calls the UI API and renders tables, charts, and commit lineage graphs.</p> <p>The UI stack is intentionally read-only: it does not enqueue jobs, stop workers, or mutate the database.</p> <pre><code>flowchart LR\n  user[User] --&gt; stApp[Streamlit UI]\n  stApp --&gt; api[\"UI API (FastAPI)\"]\n  api --&gt; db[(PostgreSQL)]\n  api --&gt; logsDir[Logs directory]</code></pre>"},{"location":"loreley/ui/#install","title":"Install","text":"<p>The UI dependencies live under the <code>ui</code> extra in <code>pyproject.toml</code>.</p> <pre><code>uv sync --extra ui\n</code></pre>"},{"location":"loreley/ui/#run","title":"Run","text":"<p>Start the API first:</p> <pre><code>uv run python script/run_api.py\n</code></pre> <p>Then start Streamlit:</p> <pre><code>uv run python script/run_ui.py --api-base-url http://127.0.0.1:8000\n</code></pre> <p>See also:</p> <ul> <li>Running the UI API</li> <li>Running the Streamlit UI</li> </ul>"},{"location":"loreley/ui/#configuration","title":"Configuration","text":""},{"location":"loreley/ui/#ui-variables","title":"UI variables","text":"<ul> <li><code>LORELEY_UI_API_BASE_URL</code>: Base URL for the UI API (default: <code>http://127.0.0.1:8000</code>).</li> </ul>"},{"location":"loreley/ui/#api-runtime-variables","title":"API runtime variables","text":"<p>The API relies on standard Loreley settings (database/logs). See:</p> <ul> <li>Configuration</li> <li>UI API</li> </ul>"},{"location":"loreley/ui/#pages","title":"Pages","text":"<p>The Streamlit UI is multi-page (implemented under <code>loreley/ui/pages</code>):</p> <ul> <li>Overview: quick KPIs, fitness trend, island table.</li> <li>Experiments: experiment list and selected experiment details.</li> <li>Jobs: job table with filters and a details panel.</li> <li>Commits: commit table with search; commit details with charts.</li> <li>Archive: island stats, snapshot metadata, record plots and table.</li> <li>Graphs: fitness scatter and commit lineage graph.</li> <li>Logs: browse role logs and tail a file.</li> <li>Settings: API health and safe settings (<code>Settings.export_safe()</code>).</li> </ul>"},{"location":"loreley/ui/#notes","title":"Notes","text":"<ul> <li>Caching: the Streamlit UI caches API GET calls (default: ~60s); use the sidebar Refresh data button to clear cache.</li> <li>Security: there is no authentication layer. Deploy behind your internal network controls if exposing remotely.</li> </ul>"},{"location":"loreley/core/experiments/","title":"loreley.core.experiments","text":"<p>Helpers for deriving canonical repository and experiment context from the current git worktree and <code>Settings</code>.</p>"},{"location":"loreley/core/experiments/#errors","title":"Errors","text":"<ul> <li><code>ExperimentError</code>: runtime error raised when the repository or experiment context cannot be resolved.   Used for git discovery failures (non\u2011existent or non\u2011repository paths) and database errors when reading or writing <code>Repository</code> / <code>Experiment</code> rows.</li> </ul>"},{"location":"loreley/core/experiments/#repository-normalisation","title":"Repository normalisation","text":"<ul> <li><code>canonicalise_repository(*, settings=None, repo_root=None, repo=None)</code>: resolves or creates a <code>Repository</code> row for a given git worktree.  </li> <li>Expands and normalises the target path, defaulting to <code>Settings.worker_repo_worktree</code> when <code>repo_root</code> is not provided.  </li> <li>Validates that the path is a git repository and extracts the <code>origin</code> remote URL when available.  </li> <li>Uses <code>_normalise_remote_url()</code> to strip credentials, support both HTTPS and SSH scp\u2011style URLs, and produce a canonical <code>remote_url</code> for hashing and storage.  </li> <li>Builds a stable <code>slug</code> from either the canonical remote URL or the local path via <code>_build_slug_from_source()</code>.  </li> <li>Populates an <code>extra</code> JSON payload with the canonical origin, root path, and all remotes (with URLs normalised for safe storage).  </li> <li>Within a DB <code>session_scope()</code>, either:<ul> <li>returns an existing <code>Repository</code> with the same <code>slug</code> after best\u2011effort metadata refresh (remote URL, root path, extra), or  </li> <li>creates and persists a new <code>Repository</code> row with the derived slug, remote URL, root path, and extra metadata.  </li> </ul> </li> <li>Logs concise status messages via <code>rich</code> (for human\u2011friendly console output) and <code>loguru</code> (for structured logs).</li> </ul>"},{"location":"loreley/core/experiments/#experiment-configuration-snapshots","title":"Experiment configuration snapshots","text":"<ul> <li><code>build_experiment_config_snapshot(settings)</code>: extracts just the configuration fields that define an experiment.  </li> <li>Starts from <code>settings.model_dump()</code>.  </li> <li>Keeps only keys with prefixes <code>mapelites_</code> and <code>worker_evaluator_</code>, so that experiments stay stable across unrelated configuration changes (logging, Redis URLs, etc.).  </li> <li> <p>Recursively applies <code>_coerce_json_compatible()</code> so that non\u2011finite floats (NaN/\u00b1inf) and nested containers are converted into JSON\u2011serialisable equivalents, making the snapshot safe for PostgreSQL JSONB.</p> </li> <li> <p><code>hash_experiment_config(snapshot)</code>: computes a stable SHA\u2011256 hash for a configuration snapshot.  </p> </li> <li>Serialises the snapshot with <code>json.dumps(..., sort_keys=True, separators=(\",\", \":\"), default=str)</code> so that key ordering does not affect the result.  </li> <li>Returns a hex digest used as <code>Experiment.config_hash</code> and as part of the default experiment name.</li> </ul>"},{"location":"loreley/core/experiments/#experiment-derivation","title":"Experiment derivation","text":"<ul> <li><code>derive_experiment(settings, repository)</code>: returns or creates an <code>Experiment</code> row for a given repository and settings.  </li> <li>Builds a snapshot via <code>build_experiment_config_snapshot()</code>, hashes it with <code>hash_experiment_config()</code>, and looks for an existing <code>Experiment</code> with the same <code>(repository_id, config_hash)</code>.  </li> <li>When found, returns the existing row unchanged.  </li> <li>Otherwise creates a new <code>Experiment</code> with:<ul> <li><code>name</code> derived from <code>repository.slug</code> plus the first 8 characters of the config hash,  </li> <li><code>config_snapshot</code> set to the JSON\u2011compatible snapshot, and  </li> <li><code>status=\"active\"</code>.  </li> </ul> </li> <li> <p>Logs both to the console and to the structured logger when creating a new experiment.</p> </li> <li> <p><code>get_or_create_experiment(*, settings=None, repo_root=None)</code>: convenience helper that resolves both <code>Repository</code> and <code>Experiment</code> for the current process.  </p> </li> <li>Resolves settings via <code>get_settings()</code> when not provided explicitly.  </li> <li>Chooses the repository root in this order: explicit <code>repo_root</code>, <code>Settings.scheduler_repo_root</code>, then <code>Settings.worker_repo_worktree</code>.  </li> <li>Validates that the chosen root is a git repository, logging and raising <code>ExperimentError</code> when it is not.  </li> <li>Reuses the discovered <code>git.Repo</code> instance when calling <code>canonicalise_repository()</code> to avoid redundant discovery work.  </li> <li>Calls <code>derive_experiment()</code> to obtain the current experiment and logs the selected <code>(repository.slug, experiment.id, experiment.config_hash)</code> pair.  </li> <li>Returns a <code>(Repository, Experiment)</code> tuple that callers (such as <code>loreley.scheduler.main.EvolutionScheduler</code>) pass downstream to the MAP\u2011Elites manager and sampler so that all jobs and archive state share the same experiment identifier.</li> </ul>"},{"location":"loreley/core/experiments/#logging-and-error-handling","title":"Logging and error handling","text":"<ul> <li>All operations are logged through a <code>loguru</code> logger bound with <code>module=\"core.experiments\"</code> plus a <code>rich</code> console for user\u2011facing status messages.  </li> <li>Git and database failures are wrapped into <code>ExperimentError</code> with concise, user\u2011oriented messages while preserving the original exception as the cause.  </li> <li>Configuration snapshots intentionally focus on MAP\u2011Elites and evaluator knobs so that operational tweaks (logging verbosity, queue names, etc.) do not fragment experiments in the database.</li> </ul>"},{"location":"loreley/core/map-elites/chunk/","title":"loreley.core.map-elites.chunk","text":"<p>Chunking utilities for turning preprocessed code into semantically meaningful segments that can be embedded and explored by map-elites.</p>"},{"location":"loreley/core/map-elites/chunk/#data-structures","title":"Data structures","text":"<ul> <li><code>FileChunk</code>: represents one chunked segment of a single file, including its path, stable <code>chunk_id</code>, positional range (<code>start_line</code>/<code>end_line</code>), text <code>content</code>, <code>line_count</code>, and aggregated <code>change_count</code>.</li> <li><code>ChunkedFile</code>: groups all <code>FileChunk</code> instances produced from a single file, tracking the file <code>path</code>, overall <code>change_count</code>, and <code>total_lines</code>.</li> <li><code>PreprocessedArtifact</code>: lightweight protocol describing the preprocessed inputs consumed by the chunker (<code>path</code>, <code>change_count</code>, <code>content</code>).</li> </ul>"},{"location":"loreley/core/map-elites/chunk/#chunker","title":"Chunker","text":"<ul> <li><code>CodeChunker</code>: splits preprocessed files into windows tuned for downstream embedding.</li> <li>Uses <code>Settings</code> map-elites chunk configuration (<code>MAPELITES_CHUNK_*</code>) to control target and minimum lines per chunk, overlap between windows, maximum chunks per file, and keywords that hint at logical boundaries (e.g. <code>def</code>, <code>class</code>).</li> <li>Iterates over each file, selecting break points on blank lines or boundary-looking lines where possible, and falls back to simple windowing when no better break point exists.</li> <li>Produces <code>ChunkedFile</code> records while displaying a <code>rich</code> progress spinner and logging summary statistics via <code>loguru</code>.</li> </ul>"},{"location":"loreley/core/map-elites/chunk/#convenience-api","title":"Convenience API","text":"<ul> <li><code>chunk_preprocessed_files(files, settings=None)</code>: helper that instantiates a <code>CodeChunker</code> and returns the list of <code>ChunkedFile</code> results for a sequence of preprocessed artifacts.</li> </ul>"},{"location":"loreley/core/map-elites/code_embedding/","title":"loreley.core.map-elites.code_embedding","text":"<p>Commit-level code embedding utilities that consume chunked code artifacts and talk to the OpenAI embeddings API as part of the Map-Elites pipeline.</p>"},{"location":"loreley/core/map-elites/code_embedding/#data-structures","title":"Data structures","text":"<ul> <li><code>ChunkEmbedding</code>: embedding vector derived from a single <code>FileChunk</code>, storing the original chunk, its numeric embedding <code>vector</code>, and a scalar <code>weight</code> used during aggregation.</li> <li><code>FileEmbedding</code>: aggregated embedding for one <code>ChunkedFile</code>, including the source file, the tuple of <code>ChunkEmbedding</code> instances, a file-level <code>vector</code>, and an overall <code>weight</code>.</li> <li><code>CommitCodeEmbedding</code>: commit-level representation that bundles all <code>FileEmbedding</code> instances, the final aggregated <code>vector</code>, the embedding <code>model</code> name, and <code>dimensions</code>, plus a <code>chunk_count</code> convenience property.</li> </ul>"},{"location":"loreley/core/map-elites/code_embedding/#embedder","title":"Embedder","text":"<ul> <li><code>CodeEmbedder</code>: orchestrates calls to the OpenAI embeddings API and aggregation logic.</li> <li>Configured via <code>Settings</code> map-elites code embedding options (<code>MAPELITES_CODE_EMBEDDING_*</code>) controlling model name, optional output dimensions, batch size, maximum chunks per commit, retry count, and retry backoff.</li> <li><code>run(chunked_files)</code> filters out empty inputs, flattens chunks into a payload, embeds them in batches with a <code>rich</code> progress spinner, and turns raw vectors into <code>ChunkEmbedding</code>, <code>FileEmbedding</code>, and <code>CommitCodeEmbedding</code> objects using weighted averaging.</li> <li>Logs detailed progress and warnings with <code>loguru</code>, including mismatched response sizes, missing owners for chunks, and empty aggregation results.</li> </ul>"},{"location":"loreley/core/map-elites/code_embedding/#convenience-api","title":"Convenience API","text":"<ul> <li><code>embed_chunked_files(chunked_files, settings=None, client=None)</code>: helper that constructs a <code>CodeEmbedder</code> and returns a <code>CommitCodeEmbedding</code> for the supplied chunked files, or <code>None</code> if there is nothing worth embedding.</li> </ul>"},{"location":"loreley/core/map-elites/dimension_reduction/","title":"loreley.core.map-elites.dimension_reduction","text":"<p>PCA-based dimensionality reduction of commit embeddings before they are fed into the MAP-Elites archive. In repo-state mode, the commit embedding is the repo-state code vector.</p>"},{"location":"loreley/core/map-elites/dimension_reduction/#data-structures","title":"Data structures","text":"<ul> <li><code>PenultimateEmbedding</code>: commit-level embedding before PCA, tracking model metadata and dimension counts. The vector is built from the code embedding and (optionally) a summary embedding; in repo-state mode MAP-Elites uses the code embedding only.</li> <li><code>PCAProjection</code>: serialisable wrapper around a fitted <code>sklearn.decomposition.PCA</code> model, capturing the mean vector, components, explained variance, explained variance ratio, whiten flag, and sample metadata, plus a <code>transform()</code> helper that projects (and when whitening is enabled, scales) new vectors.</li> <li><code>FinalEmbedding</code>: low-dimensional vector that sits on the MAP-Elites grid for a commit, along with the originating <code>PenultimateEmbedding</code> and optional <code>PCAProjection</code> used.</li> </ul>"},{"location":"loreley/core/map-elites/dimension_reduction/#reducer","title":"Reducer","text":"<ul> <li><code>DimensionReducer</code>: maintains rolling history of penultimate embeddings and an optional PCA projection to keep the behaviour space stable.</li> <li>Configured via <code>Settings</code> map-elites dimensionality options (<code>MAPELITES_DIMENSION_REDUCTION_*</code>) plus <code>MAPELITES_FEATURE_NORMALIZATION_WARMUP_SAMPLES</code>: target dimensions, minimum sample count (takes the max of the dimensionality minimum and the warmup), history size, refit interval, and whether to normalise penultimate vectors.</li> <li><code>build_penultimate(...)</code> concatenates the code embedding and an optional summary embedding, normalises when enabled, and returns a <code>PenultimateEmbedding</code> or <code>None</code> when no embeddings are available.</li> <li><code>reduce(penultimate, refit=None)</code> records the embedding in history, (re)fits PCA with <code>whiten=True</code> when needed, and projects into the target space, returning a <code>FinalEmbedding</code> and logging issues via <code>loguru</code> when projection cannot be computed.</li> </ul>"},{"location":"loreley/core/map-elites/dimension_reduction/#convenience-api","title":"Convenience API","text":"<ul> <li><code>reduce_commit_embeddings(...)</code>: one-shot helper that constructs a <code>DimensionReducer</code>, builds the penultimate embedding from a commit's code embedding (and optional summary embedding), and returns the <code>FinalEmbedding</code> together with the updated history and projection so callers can persist state.</li> </ul>"},{"location":"loreley/core/map-elites/map-elites/","title":"loreley.core.map-elites.map-elites","text":"<p>High-level manager that runs the MAP-Elites pipeline on git commits and maintains per-island archives backed by the database.</p>"},{"location":"loreley/core/map-elites/map-elites/#data-structures","title":"Data structures","text":"<ul> <li><code>CommitEmbeddingArtifacts</code>: immutable container bundling lightweight embedding artifacts for a commit. In repo-state mode this includes repo-state stats plus the final low-dimensional embedding.</li> <li><code>MapElitesRecord</code>: snapshot of a single elite stored in the archive, including commit hash, island, cell index, fitness, behaviour measures, solution vector, metadata, and timestamp.</li> <li><code>MapElitesInsertionResult</code>: describes the outcome of attempting to insert a commit into the archive, exposing a status flag, fitness delta, optional <code>MapElitesRecord</code>, any intermediate artifacts, and an optional human-readable message.</li> <li><code>IslandState</code>: internal mutable state attached to each island, holding the <code>GridArchive</code>, behaviour bounds, PCA history/projection, and mappings between commits and archive cell indices.</li> </ul>"},{"location":"loreley/core/map-elites/map-elites/#manager","title":"Manager","text":"<ul> <li><code>MapElitesManager</code>: orchestrates preprocessing, chunking, embedding, dimensionality reduction, archive updates, and snapshot persistence.</li> <li>Configured via <code>Settings</code> map-elites options: preprocessing and repository file filtering, repo-state code embeddings (file cache), dimensionality reduction (PCA with whitening), feature normalisation/truncation (<code>MAPELITES_FEATURE_TRUNCATION_K</code>, <code>MAPELITES_FEATURE_NORMALIZATION_WARMUP_SAMPLES</code>, <code>MAPELITES_FEATURE_CLIP</code>), archive grid, fitness metric, and default island identifiers.</li> <li>Accepts an optional <code>experiment_id</code> at construction time; when provided, all persisted <code>MapElitesState</code> rows are scoped by <code>(experiment_id, island_id)</code>, allowing multiple experiments to maintain independent archives even when they share island identifiers. When omitted, archive state is kept purely in-memory and snapshots are not written.</li> <li><code>ingest(commit_hash, ...)</code> runs the full pipeline for a commit in repo-state embedding mode:<ul> <li>enumerates eligible code files at <code>treeish</code> (respecting <code>.gitignore</code> and basic filtering),</li> <li>reuses a file-level embedding cache keyed by git blob SHA,</li> <li>embeds only cache misses (new/changed blobs),</li> <li>aggregates all file embeddings into a single commit vector via uniform averaging,</li> <li>reduces it to the behaviour space (PCA), resolves fitness, and updates the island's <code>GridArchive</code>.</li> </ul> </li> <li>Tracks per-island PCA history and projection so that new embeddings are consistent with previous ones, logging detailed progress and warnings with <code>loguru</code>.</li> <li>Behaviour descriptors are clipped to <code>[-k, k]</code> (k from <code>MAPELITES_FEATURE_TRUNCATION_K</code>), linearly mapped into <code>[0, 1]^d</code>, and archives are constructed with fixed <code>[0, 1]</code> bounds per dimension to avoid manual per-dimension tuning and boundary crowding.</li> <li>Delegates snapshot serialisation and persistence to <code>loreley.core.map_elites.snapshot</code>, which exposes pure helpers for encoding/decoding archive state plus pluggable storage backends (database or no-op).</li> </ul>"},{"location":"loreley/core/map-elites/map-elites/#query-helpers","title":"Query helpers","text":"<ul> <li><code>get_records(island_id=None)</code>: returns all current <code>MapElitesRecord</code> entries for an island, rebuilding them from the underlying archive.</li> <li><code>sample_records(island_id=None, count=1)</code>: randomly samples up to <code>count</code> elites from an island's archive for downstream planning or analysis.</li> <li><code>clear_island(island_id=None)</code>: clears an island's archive and associated PCA history/projection state, removing all stored elites and mappings for that island.</li> <li><code>describe_island(island_id=None)</code>: returns a small dict of observability stats for an island (ID, occupied cell count, total cells, QD score, and best fitness).</li> </ul>"},{"location":"loreley/core/map-elites/preprocess/","title":"loreley.core.map-elites.preprocess","text":"<p>Preprocessing utilities for turning raw repository code files into cleaned code snippets suitable for embedding and feature extraction.</p>"},{"location":"loreley/core/map-elites/preprocess/#data-structures","title":"Data structures","text":"<ul> <li><code>ChangedFile</code>: lightweight description of a file touched by a commit (path, approximate <code>change_count</code>, optional inline <code>content</code> override).</li> <li><code>PreprocessedFile</code>: output record capturing the repository-relative <code>path</code>, accumulated <code>change_count</code>, and cleaned textual <code>content</code> after preprocessing.</li> </ul>"},{"location":"loreley/core/map-elites/preprocess/#preprocessor","title":"Preprocessor","text":"<ul> <li><code>CodePreprocessor</code>: filters and normalises files before embedding.</li> <li>Uses <code>Settings</code> map-elites preprocessing options to enforce maximum file count/size, allowed extensions/filenames, and excluded glob patterns.</li> <li>Loads file contents either from the working tree or a specific <code>treeish</code> via GitPython, applies comment stripping, tab-to-spaces conversion, blank-line collapse, and basic normalisation.</li> <li>Exposes <code>run(changed_files)</code> which returns a list of <code>PreprocessedFile</code> objects ordered by <code>change_count</code>.</li> <li>Reports progress via a <code>rich</code> <code>Progress</code> spinner and logs structured messages through <code>loguru</code>.</li> </ul>"},{"location":"loreley/core/map-elites/preprocess/#convenience-api","title":"Convenience API","text":"<ul> <li><code>preprocess_changed_files(changed_files, repo_root=None, settings=None, treeish=None, repo=None)</code>: functional helper that instantiates a <code>CodePreprocessor</code> and runs it over the provided list of changed files.</li> </ul>"},{"location":"loreley/core/map-elites/repository_embedding/","title":"Repository-state embeddings (file cache)","text":"<p>This page documents the repo-state embedding pipeline used by MAP-Elites.</p>"},{"location":"loreley/core/map-elites/repository_embedding/#motivation","title":"Motivation","text":"<p>Repo-state embeddings represent the entire repository state at a commit by aggregating file-level embeddings into a single commit vector. This makes the behaviour descriptor depend on the repository snapshot at <code>treeish</code>, not just a subset of changed files.</p>"},{"location":"loreley/core/map-elites/repository_embedding/#high-level-pipeline","title":"High-level pipeline","text":"<p>At a given <code>treeish</code> (typically a commit hash), we:</p> <ol> <li>Enumerate eligible files and resolve their git blob SHA fingerprints.</li> <li>Look up each blob SHA in a file embedding cache.</li> <li>Embed only cache misses (new/changed blobs).</li> <li>Aggregate per-file embeddings into one commit vector via uniform mean.</li> <li>Feed the commit vector into PCA \u2192 MAP-Elites as the behaviour descriptor.</li> </ol>"},{"location":"loreley/core/map-elites/repository_embedding/#file-enumeration-and-filtering","title":"File enumeration and filtering","text":"<p>Implemented by:</p> <ul> <li><code>loreley.core.map_elites.repository_files.RepositoryFileCatalog</code></li> </ul> <p>Eligibility is determined by a combination of:</p> <ul> <li>Root <code>.gitignore</code> (best-effort glob matching).</li> <li><code>MAPELITES_PREPROCESS_ALLOWED_EXTENSIONS</code> / <code>MAPELITES_PREPROCESS_ALLOWED_FILENAMES</code>.</li> <li><code>MAPELITES_PREPROCESS_EXCLUDED_GLOBS</code>.</li> <li><code>MAPELITES_PREPROCESS_MAX_FILE_SIZE_KB</code> (oversized blobs are skipped).</li> <li><code>MAPELITES_REPO_STATE_MAX_FILES</code> (optional cap; when set, the eligible list is deterministically sub-sampled).</li> </ul> <p>Note</p> <p><code>.gitignore</code> filtering is currently best-effort and only uses the repository root <code>.gitignore</code> at the requested <code>treeish</code>. Nested <code>.gitignore</code> files and global excludes are not applied.</p> <p>For each eligible file we keep:</p> <ul> <li><code>path</code> (repo-root relative)</li> <li><code>blob_sha</code> (content fingerprint)</li> <li><code>size_bytes</code></li> </ul>"},{"location":"loreley/core/map-elites/repository_embedding/#file-embedding-cache","title":"File embedding cache","text":"<p>Implemented by:</p> <ul> <li><code>loreley.core.map_elites.file_embedding_cache.InMemoryFileEmbeddingCache</code></li> <li><code>loreley.core.map_elites.file_embedding_cache.DatabaseFileEmbeddingCache</code></li> <li>ORM table: <code>loreley.db.models.MapElitesFileEmbeddingCache</code></li> </ul> <p>Cache key:</p> <ul> <li><code>blob_sha</code></li> <li><code>embedding_model</code></li> <li><code>dimensions</code> (actual output vector length guard)</li> <li><code>pipeline_signature</code></li> </ul> <p><code>pipeline_signature</code> is a SHA-256 hash over the preprocessing/chunking/embedding knobs that affect the produced vectors (so cache entries are invalidated when the pipeline changes).</p> <p>Backend selection:</p> <ul> <li><code>MAPELITES_FILE_EMBEDDING_CACHE_BACKEND=db|memory</code> (default: <code>db</code>)</li> </ul>"},{"location":"loreley/core/map-elites/repository_embedding/#commit-aggregation","title":"Commit aggregation","text":"<p>Implemented by:</p> <ul> <li><code>loreley.core.map_elites.repository_state_embedding.RepositoryStateEmbedder</code></li> </ul> <p>Let (v_i) be the embedding vector for eligible file (i), and (N) be the number of eligible files with available vectors. The repo-state commit vector is the uniform mean:</p> <p>[ v_{commit} = \\frac{1}{N}\\sum_{i=1}^{N} v_i ]</p> <p>If multiple paths point at the same blob SHA, the corresponding (v_i) is the same vector but still contributes once per file path (uniform per-file weighting).</p>"},{"location":"loreley/core/map-elites/sampler/","title":"loreley.core.map-elites.sampler","text":"<p>Sampler that turns MAP-Elites archive records into concrete <code>EvolutionJob</code> rows for further evolution.</p>"},{"location":"loreley/core/map-elites/sampler/#protocols","title":"Protocols","text":"<ul> <li><code>SupportsMapElitesRecord</code>: protocol describing the record interface consumed by the sampler (commit hash, cell index, fitness, measures, solution, metadata, timestamp).</li> <li><code>SupportsMapElitesManager</code>: protocol that exposes a <code>get_records(island_id)</code> method, allowing the sampler to be used against <code>MapElitesManager</code> or any compatible implementation.</li> </ul>"},{"location":"loreley/core/map-elites/sampler/#sampling","title":"Sampling","text":"<ul> <li><code>ScheduledSamplerJob</code>: immutable descriptor for a newly scheduled job, exposing the <code>EvolutionJob</code> ID, island, base record, inspiration records, and the payload sent to the scheduler.</li> <li><code>MapElitesSampler</code>: coordinates archive sampling and job persistence.</li> <li>Configured via <code>Settings</code> map-elites options for dimensionality, truncation/normalization, archive grid, and sampler behaviour (<code>MAPELITES_DIMENSION_REDUCTION_*</code>, <code>MAPELITES_FEATURE_TRUNCATION_K</code>, <code>MAPELITES_FEATURE_NORMALIZATION_WARMUP_SAMPLES</code>, <code>MAPELITES_FEATURE_CLIP</code>, <code>MAPELITES_ARCHIVE_*</code>, and <code>MAPELITES_SAMPLER_*</code>).</li> <li><code>schedule_job(island_id=None, payload_overrides=None, priority=None, experiment_id=None)</code> pulls records from the manager, chooses a base record, selects neighbours as inspirations using a configurable neighbourhood radius with optional fallback sampling, builds a rich JSON payload (including grid shape and <code>[0, 1]^d</code> feature bounds, sampling statistics, and normalised base/inspiration records), and persists a new <code>EvolutionJob</code> via <code>session_scope</code>.</li> <li>When <code>experiment_id</code> is provided, it is coerced to a <code>UUID</code> and stored on the <code>EvolutionJob</code> row so that downstream components (scheduler ingestion, worker, MAP-Elites manager) can reliably group jobs and commits by experiment. When omitted, jobs are still scheduled and <code>experiment_id</code> remains <code>NULL</code>, which is useful for legacy or single-experiment deployments.</li> <li>Uses <code>loguru</code> for structured logging and <code>rich</code> to print a concise confirmation when a job is enqueued.</li> </ul>"},{"location":"loreley/core/map-elites/sampler/#neighbourhood-selection","title":"Neighbourhood selection","text":"<ul> <li><code>_select_inspirations(...)</code>: internal helper that walks outward from the base cell over the discretised behaviour grid, gathering nearby elites up to the requested inspiration count and recording selection statistics.</li> <li><code>_neighbor_indices(center_index, radius)</code>: converts a flat cell index and radius into neighbouring cell indices using numpy's <code>unravel_index</code>/<code>ravel_multi_index</code>, respecting grid bounds.</li> </ul>"},{"location":"loreley/core/map-elites/snapshot/","title":"loreley.core.map-elites.snapshot","text":"<p>Helpers and backends for serialising and persisting MAP-Elites archive snapshots.</p>"},{"location":"loreley/core/map-elites/snapshot/#responsibilities","title":"Responsibilities","text":"<ul> <li>Serialisation helpers:</li> <li>Convert per-island PCA history (<code>PenultimateEmbedding</code>), <code>PCAProjection</code>, and <code>GridArchive</code> contents into JSON-compatible snapshot payloads.</li> <li><code>PCAProjection</code> payloads include components, mean, explained variance, explained variance ratio, whiten flag, sample count, and fitted timestamp.</li> <li>Restore bounds, history, projection, archive entries, and commit-to-cell mappings from previously stored snapshots.</li> <li>Backends:</li> <li>Define a small <code>SnapshotBackend</code> interface with <code>load(island_id)</code> and <code>save(island_id, snapshot)</code> methods.</li> <li>Provide an optional <code>apply_update(island_id, *, state, update)</code> method:<ul> <li>Default implementation falls back to building a full snapshot and calling <code>save(...)</code>.</li> <li>Database backends override it to persist incremental updates without rewriting large JSON blobs.</li> </ul> </li> <li>Provide a <code>NullSnapshotBackend</code> that disables persistence and simply returns <code>None</code> on <code>load</code>.</li> <li>Provide a <code>DatabaseSnapshotBackend</code> that stores lightweight metadata in <code>map_elites_states</code> (<code>MapElitesState</code>)     and persists the large payload incrementally in dedicated tables:<ul> <li><code>map_elites_archive_cells</code> (<code>MapElitesArchiveCell</code>): one row per occupied archive cell.</li> <li><code>map_elites_pca_history</code> (<code>MapElitesPcaHistory</code>): one row per commit hash for PCA reconstruction.</li> </ul> </li> </ul>"},{"location":"loreley/core/map-elites/snapshot/#snapshot-schema-versions","title":"Snapshot schema versions","text":"<p><code>MapElitesState.snapshot</code> carries a small <code>schema_version</code> flag:</p> <ul> <li>schema_version = 1 (legacy): <code>snapshot</code> may embed large <code>archive</code>/<code>history</code> lists directly.</li> <li>schema_version &gt;= 2 (incremental): <code>snapshot</code> is lightweight metadata only (bounds, projection, knobs),   while <code>archive</code> and <code>history</code> are stored in the incremental tables.</li> </ul> <p>When loading a legacy payload that still embeds <code>archive</code>/<code>history</code>, the database backend performs lazy migration on read: it upserts rows into the incremental tables, strips the large fields from <code>snapshot</code>, and bumps <code>schema_version</code> to 2.</p>"},{"location":"loreley/core/map-elites/snapshot/#integration-with-mapelitesmanager","title":"Integration with <code>MapElitesManager</code>","text":"<ul> <li><code>MapElitesManager</code> constructs a backend through <code>build_snapshot_backend(experiment_id)</code>:</li> <li>When <code>experiment_id</code> is <code>None</code>, a <code>NullSnapshotBackend</code> is returned and all snapshot operations become in-memory only.</li> <li>When <code>experiment_id</code> is set, a <code>DatabaseSnapshotBackend</code> is used and snapshots are scoped by <code>(experiment_id, island_id)</code>.</li> <li>The manager decides when to persist:</li> <li>On island initialisation it calls <code>backend.load(island_id)</code> and, if a payload exists, applies it with <code>apply_snapshot(...)</code>.</li> <li>After ingestion and <code>clear_island()</code>, it emits a <code>SnapshotUpdate</code> and calls <code>backend.apply_update(...)</code>:<ul> <li>PCA history/projection updates are persisted frequently (small metadata + per-commit history row).</li> <li>Archive cell writes are persisted only when a commit actually improves a cell (single cell upsert).</li> <li>Clearing an island deletes that island's cell/history rows and resets projection metadata.</li> </ul> </li> </ul>"},{"location":"loreley/core/map-elites/summarization_embedding/","title":"loreley.core.map-elites.summarization_embedding","text":"<p>Optional summary-level embedding utilities that turn preprocessed code into structured natural-language summaries and a commit-level embedding vector. In repo-state mode (<code>MAPELITES_EMBEDDING_MODE=repo_state</code>), MAP-Elites uses code-only repo-state embeddings and does not currently include these summary vectors.</p>"},{"location":"loreley/core/map-elites/summarization_embedding/#data-structures","title":"Data structures","text":"<ul> <li><code>FileSummary</code>: immutable record describing one file-level summary (repository-relative <code>path</code>, approximate <code>change_count</code>, and the generated markdown <code>summary</code> text). The path is normalised to a <code>pathlib.Path</code>.</li> <li><code>SummaryEmbedding</code>: embedding derived from a <code>FileSummary</code>, containing the original <code>file_summary</code>, its numeric <code>vector</code>, and a scalar <code>weight</code> used during aggregation.</li> <li><code>CommitSummaryEmbedding</code>: commit-level representation bundling all <code>SummaryEmbedding</code> instances, the final aggregated <code>vector</code>, the <code>summary_model</code> and <code>embedding_model</code> names, <code>dimensions</code>, and a <code>file_count</code> convenience property.</li> </ul>"},{"location":"loreley/core/map-elites/summarization_embedding/#embedder","title":"Embedder","text":"<ul> <li><code>SummaryEmbedder</code>: orchestrates summarisation of preprocessed files and embedding of the resulting summaries.</li> <li>Configured via <code>Settings</code> Map-Elites summary options (<code>MAPELITES_SUMMARY_*</code>) controlling the LLM model, temperature, maximum output tokens, source excerpt character limit, and retry/backoff behaviour, and summary embedding options (<code>MAPELITES_SUMMARY_EMBEDDING_*</code>) controlling the embedding model, optional output dimensions, and batch size.</li> <li>Uses the global OpenAI API configuration from <code>loreley.config.Settings</code> (<code>OPENAI_API_KEY</code>, <code>OPENAI_BASE_URL</code>, <code>OPENAI_API_SPEC</code>) when constructing the shared <code>OpenAI</code> client. <code>OPENAI_API_SPEC</code> selects whether summarisation uses:<ul> <li>the unified Responses API (<code>client.responses.create</code>, default), passing <code>_SUMMARY_INSTRUCTIONS</code> as <code>instructions</code> and the file prompt as <code>input</code>; or</li> <li>the classic Chat Completions API (<code>client.chat.completions.create</code>), mapping <code>_SUMMARY_INSTRUCTIONS</code> to a <code>system</code> message and the file prompt to a <code>user</code> message while keeping temperature and token limits aligned.</li> </ul> </li> <li><code>run(files)</code> skips empty input, calls <code>_summarize_files</code> to build <code>FileSummary</code> objects with a <code>rich</code> progress spinner and <code>loguru</code> debug/warning logs (using either Responses or Chat Completions under the hood), then calls <code>_embed_summaries</code> to batch summaries through the OpenAI embeddings API, weighting them by change count or summary length.</li> <li>Aggregates per-file vectors into a single commit-level vector using <code>_weighted_average</code>, after sorting entries by path for stable output; returns a <code>CommitSummaryEmbedding</code> or <code>None</code> if no usable summaries or embeddings were produced.</li> </ul>"},{"location":"loreley/core/map-elites/summarization_embedding/#convenience-api","title":"Convenience API","text":"<ul> <li><code>summarize_preprocessed_files(files, settings=None, client=None)</code>: helper that constructs a <code>SummaryEmbedder</code> (optionally injecting custom settings or OpenAI client) and returns a <code>CommitSummaryEmbedding</code> for the supplied <code>PreprocessedFile</code> sequence, or <code>None</code> when there is nothing to summarise or embed.</li> </ul>"},{"location":"loreley/core/worker/agent_backend/","title":"loreley.core.worker.agent_backend","text":"<p>Shared abstractions and helpers for structured planning/coding agents plus the default Codex CLI backend.</p>"},{"location":"loreley/core/worker/agent_backend/#core-types","title":"Core types","text":"<ul> <li> <p><code>SchemaMode</code>: type alias restricting schema handling modes to <code>\"native\"</code>, <code>\"prompt\"</code>, or <code>\"none\"</code>.   Used by structured agents to decide whether JSON schema is enforced by the backend API itself (<code>\"native\"</code>), injected into the prompt (<code>\"prompt\"</code>), or completely disabled (<code>\"none\"</code>).</p> </li> <li> <p><code>AgentInvocation</code>: immutable result of a single backend invocation.  </p> </li> <li>Captures the executed <code>command</code> (argv tuple), captured <code>stdout</code> and <code>stderr</code>, and total <code>duration_seconds</code>.  </li> <li> <p>Serves as the low\u2011level record returned by all <code>AgentBackend</code> implementations.</p> </li> <li> <p><code>StructuredAgentTask</code>: backend\u2011agnostic description of a structured agent call.  </p> </li> <li>Fields:<ul> <li><code>name</code>: human\u2011readable label used in logs and error messages.  </li> <li><code>prompt</code>: full natural\u2011language prompt passed to the backend.  </li> <li><code>schema</code>: optional JSON schema dict describing the expected output structure.  </li> <li><code>schema_mode</code>: one of the <code>SchemaMode</code> values controlling how <code>schema</code> should be applied.  </li> </ul> </li> <li> <p>Planning and coding agents construct a <code>StructuredAgentTask</code> and delegate execution to a concrete backend.</p> </li> <li> <p><code>AgentBackend</code>: <code>Protocol</code> defining the contract for all agent backends.  </p> </li> <li>Requires a single method <code>run(task, *, working_dir)</code> that:<ul> <li>validates and normalises the <code>working_dir</code>,  </li> <li>executes the task using a concrete backend mechanism (CLI, HTTP API, etc.), and  </li> <li>returns an <code>AgentInvocation</code> with the raw output and timing information.</li> </ul> </li> </ul>"},{"location":"loreley/core/worker/agent_backend/#backend-resolution-helpers","title":"Backend resolution helpers","text":"<ul> <li><code>resolve_schema_mode(configured_mode, api_spec)</code>: determines the effective schema mode based on configuration and the selected API surface.  </li> <li>If <code>configured_mode</code> is anything other than <code>\"auto\"</code>, it is returned unchanged.  </li> <li>When <code>configured_mode=\"auto\"</code> and <code>api_spec=\"chat_completions\"</code>, it returns <code>\"prompt\"</code> so that schemas are enforced via prompt engineering.  </li> <li> <p>When <code>configured_mode=\"auto\"</code> and <code>api_spec=\"responses\"</code>, it returns <code>\"native\"</code> so that native tool/JSON capabilities are used.</p> </li> <li> <p><code>load_agent_backend(ref, *, label)</code>: resolves and instantiates an <code>AgentBackend</code> from a dotted reference string.  </p> </li> <li>Accepts both <code>\"module:attr\"</code> and <code>\"module.attr\"</code> forms.  </li> <li>Internally splits the reference, imports the target module, and walks the attribute path.  </li> <li>Supports three patterns:<ul> <li>an already\u2011instantiated backend object exposing a callable <code>run</code> method,  </li> <li>a class implementing the <code>AgentBackend</code> protocol (constructed with no arguments), or  </li> <li>a factory callable that returns a backend instance when called with no arguments.  </li> </ul> </li> <li>Validates that the resulting object has a callable <code>run</code> attribute and raises a descriptive <code>RuntimeError</code> otherwise.</li> </ul>"},{"location":"loreley/core/worker/agent_backend/#cli-backends","title":"CLI backends","text":"<ul> <li><code>CodexCliBackend</code>: concrete <code>AgentBackend</code> implementation that delegates to the external Codex CLI.  </li> <li>Configuration fields:<ul> <li><code>bin</code>: CLI executable to invoke (for example, <code>\"codex\"</code>).  </li> <li><code>profile</code>: optional profile name passed as <code>--profile</code> to select a Codex configuration.  </li> <li><code>timeout_seconds</code>: hard timeout for the subprocess invocation.  </li> <li><code>extra_env</code>: dict of additional environment variables merged into the subprocess environment.  </li> <li><code>schema_override</code>: optional path to a JSON schema file to use instead of the task\u2011provided schema.  </li> <li><code>error_cls</code>: concrete <code>RuntimeError</code> subtype used for all user\u2011facing errors (planning/coding use module\u2011specific error types).  </li> <li><code>full_auto</code>: when <code>True</code>, appends <code>--full-auto</code> so the CLI can drive multi\u2011step interactions autonomously.</li> </ul> </li> <li> <p><code>run(task, *, working_dir)</code>:</p> <ul> <li>Validates that <code>working_dir</code> exists, is a directory, and contains a <code>.git</code> folder via <code>_validate_workdir()</code>.  </li> <li>Builds the CLI <code>command</code> list starting from <code>[bin, \"exec\"]</code>, optionally adding <code>--full-auto</code>.  </li> <li>When <code>task.schema_mode==\"native\"</code>:</li> <li>uses <code>schema_override</code> if provided and points to an existing file, or  </li> <li>materialises <code>task.schema</code> into a temporary JSON file via <code>_materialise_schema_to_temp()</code> and passes it as <code>--output-schema</code>.  </li> <li>Adds <code>--profile</code> when a profile is configured, merges <code>extra_env</code> into a copy of <code>os.environ</code>, and feeds <code>task.prompt</code> on stdin.  </li> <li>Runs the process with <code>subprocess.run(...)</code>, capturing stdout/stderr, enforcing the timeout, and deleting any temporary schema file afterwards.  </li> <li>Raises <code>error_cls</code> when the process exits non\u2011zero or times out; even when stdout is empty it still returns an <code>AgentInvocation</code>, leaving it to higher\u2011level agents (and their validation modes) to decide whether an empty payload is acceptable.</li> </ul> </li> <li> <p><code>CursorCliBackend</code>: concrete <code>AgentBackend</code> implementation that delegates to the Cursor Agent CLI (<code>cursor-agent</code>).  </p> </li> <li>Configuration fields:<ul> <li><code>bin</code>: CLI executable to invoke (default <code>\"cursor-agent\"</code>).  </li> <li><code>model</code>: model identifier passed as <code>--model</code>; defaults to <code>\"gpt-5.1-codex-max-high\"</code> and can be overridden (for example, <code>\"gpt-5\"</code>), including via the <code>cursor_backend_from_settings()</code> helper that reads <code>WORKER_CURSOR_MODEL</code>.  </li> <li><code>timeout_seconds</code>: hard timeout for the subprocess invocation.  </li> <li><code>extra_env</code>: dict of additional environment variables merged into the subprocess environment.  </li> <li><code>output_format</code>: value passed as <code>--output-format</code> (default <code>\"text\"</code>), typically left as <code>\"text\"</code> so the agent can emit a single JSON object as plain text.  </li> <li><code>force</code>: when <code>True</code> (default), appends <code>--force</code> so the Cursor agent allows commands unless explicitly denied; set to <code>False</code> to omit the flag.  </li> <li><code>error_cls</code>: concrete <code>RuntimeError</code> subtype used for all user\u2011facing errors.</li> </ul> </li> <li><code>run(task, *, working_dir)</code>:<ul> <li>Validates that <code>working_dir</code> exists, is a directory, and contains a <code>.git</code> folder via <code>_validate_workdir()</code>.  </li> <li>Builds the CLI <code>command</code> list starting from <code>[bin]</code>, adding:</li> <li><code>-p &lt;prompt&gt;</code> to forward <code>task.prompt</code> to the Cursor agent,  </li> <li><code>--model</code> when <code>model</code> is configured, and  </li> <li><code>--output-format</code> when <code>output_format</code> is set, and  </li> <li><code>--force</code> when <code>force=True</code>.  </li> <li>Merges <code>extra_env</code> into a copy of <code>os.environ</code> and runs <code>cursor-agent</code> in the provided working directory.  </li> <li>Captures stdout/stderr and enforces the timeout via <code>subprocess.run(...)</code>.  </li> <li>Raises <code>error_cls</code> when the process exits non\u2011zero or times out; even when stdout is empty it still returns an <code>AgentInvocation</code>, leaving it to higher\u2011level agents and validation logic to decide how to handle the result.  </li> <li>Does not pass JSON Schema to the CLI directly; structured agents are expected to enforce schemas via prompt engineering (for example by embedding the schema in the prompt when using <code>\"prompt\"</code> schema mode).</li> </ul> </li> <li>Factory helper: <code>cursor_backend_from_settings()</code> builds a <code>CursorCliBackend</code> using application settings (notably <code>WORKER_CURSOR_MODEL</code>) so deployments can pick a Cursor model without writing custom wiring.</li> </ul>"},{"location":"loreley/core/worker/agent_backend/#internal-utilities","title":"Internal utilities","text":"<ul> <li><code>_validate_workdir()</code>: expands and checks the working directory path, ensuring it exists, is a directory, and contains a <code>.git</code> subdirectory; raises the configured error type with clear messages when any condition fails.  </li> <li><code>_materialise_schema_to_temp()</code>: writes a JSON schema dict to a temporary file with a deterministic prefix/suffix and returns its path, used only when running in <code>\"native\"</code> schema mode without a <code>schema_override</code>.  </li> <li><code>_split_backend_reference()</code> and <code>_import_backend_target()</code>: low\u2011level helpers used by <code>load_agent_backend()</code> to parse dotted references and resolve the final object, with detailed error messages for invalid module names or attribute paths.  </li> <li>These helpers are internal but form part of the stable backend loading behaviour relied on by the planning and coding agents.</li> </ul>"},{"location":"loreley/core/worker/coding/","title":"loreley.core.worker.coding","text":"<p>Execution engine for Loreley's autonomous worker, responsible for driving the Codex-based coding agent that applies a planning agent's plan to a real git worktree.</p>"},{"location":"loreley/core/worker/coding/#domain-types","title":"Domain types","text":"<ul> <li><code>CodingError</code>: custom runtime error raised when the coding agent cannot successfully execute the plan (invalid schema, Codex failures, bad working directory, timeouts, etc.).</li> <li><code>StepExecutionStatus</code>: string <code>Enum</code> describing how each plan step was handled (<code>\"completed\"</code>, <code>\"partial\"</code>, or <code>\"skipped\"</code>).</li> <li><code>CodingStepReport</code>: dataclass capturing the outcome of a single step (<code>step_id</code>, <code>status</code>, human-readable <code>summary</code>, and optional <code>files</code> / <code>commands</code> touched by that step).</li> <li><code>CodingPlanExecution</code>: aggregate result for the whole run, including the overall <code>implementation_summary</code>, optional <code>commit_message</code>, tuple of <code>step_results</code>, <code>tests_executed</code>, <code>tests_recommended</code>, <code>follow_up_items</code>, and <code>notes</code>.</li> <li><code>CodingAgentRequest</code>: input payload given to the coding agent (<code>goal</code>, <code>plan</code> from <code>PlanningPlan</code>, <code>base_commit</code>, optional <code>constraints</code>, <code>acceptance_criteria</code>, <code>iteration_hint</code>, and <code>additional_notes</code>); the <code>goal</code> is the same global evolution objective that the planning agent sees, resolved by the evolution worker from either explicit job payload fields or <code>Settings.worker_evolution_global_goal</code>. All sequence fields are normalised to tuples in <code>__post_init__</code>.</li> <li><code>CodingAgentResponse</code>: envelope returned from the agent combining the structured <code>execution</code>, raw backend <code>raw_output</code>, rendered <code>prompt</code>, executed backend <code>command</code>, captured <code>stderr</code>, number of <code>attempts</code>, and total <code>duration_seconds</code>.</li> </ul>"},{"location":"loreley/core/worker/coding/#json-schema-and-validation","title":"JSON schema and validation","text":"<ul> <li><code>CODING_OUTPUT_SCHEMA</code>: JSON schema describing the expected coding agent output (top-level <code>implementation_summary</code>, optional <code>commit_message</code>, array of <code>step_results</code>, plus optional <code>tests_executed</code>, <code>tests_recommended</code>, <code>follow_up_items</code>, and <code>notes</code>), used to validate the backend response.</li> <li><code>_StepResultModel</code> / <code>_CodingOutputModel</code>: internal frozen <code>pydantic</code> models that validate the JSON payload against <code>CODING_OUTPUT_SCHEMA</code> and provide a typed bridge into the domain dataclasses.</li> <li>Agent backend: coding relies on <code>loreley.core.worker.agent_backend</code> for shared backend abstractions (<code>AgentBackend</code>, <code>StructuredAgentTask</code>, <code>AgentInvocation</code>) and the default <code>CodexCliBackend</code> implementation used by the coding agent; see that module's documentation for backend configuration details.</li> </ul>"},{"location":"loreley/core/worker/coding/#coding-agent","title":"Coding agent","text":"<ul> <li><code>CodingAgent</code>: high-level orchestrator that turns a <code>CodingAgentRequest</code> and <code>PlanningPlan</code> into a sequence of edits via a configurable backend.</li> <li>Instantiated with a <code>Settings</code> object and an optional <code>AgentBackend</code> implementation. When no backend is provided, it uses <code>CodexCliBackend</code> configured via <code>WORKER_CODING_CODEX_BIN</code>, <code>WORKER_CODING_CODEX_PROFILE</code>, <code>WORKER_CODING_MAX_ATTEMPTS</code>, <code>WORKER_CODING_TIMEOUT_SECONDS</code>, <code>WORKER_CODING_EXTRA_ENV</code>, <code>WORKER_CODING_SCHEMA_PATH</code>, and <code>WORKER_CODING_VALIDATION_MODE</code>. You can override the default by setting <code>WORKER_CODING_BACKEND</code> to a dotted Python path (<code>module:attr</code> or <code>module.attr</code>) that resolves to either an <code>AgentBackend</code> instance, a class implementing the <code>AgentBackend</code> protocol (constructed with no arguments), or a factory callable that returns such an instance.</li> <li><code>implement(request, *, working_dir)</code>: resolves the git worktree path, renders a detailed natural-language prompt describing the goal, constraints, acceptance criteria, plan steps, focus metrics, guardrails, validation bullets, risks, additional notes, handoff notes, and any fallback plan, builds a <code>StructuredAgentTask</code> whose schema usage depends on <code>WORKER_CODING_VALIDATION_MODE</code>, and asks the backend to execute it (for <code>CodexCliBackend</code> this means <code>codex exec --full-auto</code>):<ul> <li>In <code>\"strict\"</code> mode, the coding JSON schema is passed to the backend (using Codex's native schema support when available), and the response must parse cleanly as <code>_CodingOutputModel</code> or the agent will retry and eventually fail.</li> <li>In <code>\"lenient\"</code> mode, the same schema is still provided to the backend, but JSON decoding / validation failures are treated as non-fatal: the agent first attempts to parse the response into <code>_CodingOutputModel</code>, and on failure synthesises a minimal <code>CodingPlanExecution</code> from the free-form output.</li> <li>In <code>\"none\"</code> mode, no schema is enforced at the backend level and the agent skips JSON parsing entirely, always building a minimal <code>CodingPlanExecution</code> directly from the raw free-form output and job context.</li> </ul> </li> <li>Retries the backend invocation up to <code>max_attempts</code> times when the process fails or, in strict/lenient modes, when JSON decoding / schema validation errors occur, logging warnings via <code>loguru</code> and showing concise progress output with <code>rich</code>.</li> <li>On success, either parses JSON into <code>_CodingOutputModel</code> and converts it into a <code>CodingPlanExecution</code>, or (for lenient/none modes when parsing is not possible) returns a best-effort synthetic <code>CodingPlanExecution</code>; on repeated failure or timeout, raises <code>CodingError</code> with a descriptive message.</li> <li>Detects and treats a run that leaves the worktree unchanged as a failure, persisting debug artifacts and retrying until attempts are exhausted.</li> <li>Merges any configured extra environment variables into the backend subprocess environment and enforces bounded prompt and log sizes via <code>_truncate</code>.</li> </ul>"},{"location":"loreley/core/worker/coding/#exceptions-and-helpers","title":"Exceptions and helpers","text":"<ul> <li><code>_parse_output()</code>, <code>_log_invalid_output()</code>, <code>_to_domain()</code>, <code>_format_plan_step()</code>, <code>_format_bullets()</code>, and <code>_truncate()</code>: utilities that format human-readable prompt sections, enforce length limits, convert the raw JSON model into domain types, and provide rich logging when backend output cannot be validated.   Debug artifacts are written under <code>logs/worker/coding</code>, including the effective <code>schema_mode</code> and <code>validation_mode</code> used for each run.</li> </ul>"},{"location":"loreley/core/worker/commit_summary/","title":"loreley.core.worker.commit_summary","text":"<p>Commit summarisation utilities used by the evolution worker to derive concise git commit subjects from planning and coding context.</p>"},{"location":"loreley/core/worker/commit_summary/#domain-types-and-errors","title":"Domain types and errors","text":"<ul> <li><code>CommitSummaryError</code>: runtime error raised when the summariser cannot produce a subject line (for example, due to API errors, empty model output, or repeated failures across retries).</li> </ul>"},{"location":"loreley/core/worker/commit_summary/#commitsummarizer","title":"CommitSummarizer","text":"<ul> <li><code>CommitSummarizer</code>: LLM-backed helper responsible for generating short, imperative git subjects.</li> <li>Configured via <code>loreley.config.Settings</code> worker evolution commit options:<ul> <li><code>WORKER_EVOLUTION_COMMIT_MODEL</code>: model identifier used with the <code>OpenAI</code> client.</li> <li><code>WORKER_EVOLUTION_COMMIT_TEMPERATURE</code>: sampling temperature.</li> <li><code>WORKER_EVOLUTION_COMMIT_MAX_OUTPUT_TOKENS</code>: upper bound on model output tokens (clamped to at least 32).</li> <li><code>WORKER_EVOLUTION_COMMIT_MAX_RETRIES</code>: maximum number of retry attempts on failure (minimum 1).</li> <li><code>WORKER_EVOLUTION_COMMIT_RETRY_BACKOFF_SECONDS</code>: linear backoff applied between retries.</li> <li><code>WORKER_EVOLUTION_COMMIT_SUBJECT_MAX_CHARS</code>: hard character limit for subject lines (minimum 32).</li> </ul> </li> <li>Lazily initialises an <code>OpenAI</code> client and applies a local <code>_truncate_limit</code> when building prompts to keep context sizes reasonable.</li> <li>Respects the global OpenAI API surface setting <code>OPENAI_API_SPEC</code>:<ul> <li><code>\"responses\"</code> (default) uses the unified Responses API (<code>client.responses.create</code>) with an <code>instructions</code> string and full prompt as <code>input</code>.</li> <li><code>\"chat_completions\"</code> uses the Chat Completions API (<code>client.chat.completions.create</code>), mapping the same instruction text to a <code>system</code> message and the prompt to a <code>user</code> message, extracting the assistant's reply from the first choice.</li> </ul> </li> </ul>"},{"location":"loreley/core/worker/commit_summary/#subject-generation","title":"Subject generation","text":"<ul> <li><code>generate(job, plan, coding)</code>:</li> <li>Constructs a detailed prompt that includes:<ul> <li>The global job <code>goal</code>.</li> <li>Plan <code>summary</code>, <code>rationale</code>, focus metrics, guardrails, constraints, acceptance criteria, and notes.</li> <li>Coding execution summary, per-step outcomes (step IDs, statuses, summaries), and the list of tests executed.</li> <li>The coding agent's own suggested <code>commit_message</code> as a fallback hint.</li> </ul> </li> <li>Calls the <code>OpenAI</code> responses API with the configured model, temperature, and token limit, plus an <code>instructions</code> string that asks for a single imperative git subject bounded by <code>WORKER_EVOLUTION_COMMIT_SUBJECT_MAX_CHARS</code> (minimum 32; enforced both in the prompt and when normalising the final subject).</li> <li>Retries up to <code>_max_retries</code> times on <code>OpenAIError</code> or <code>CommitSummaryError</code>, waiting for <code>retry_backoff * attempt</code> seconds between attempts, regardless of whether Responses or Chat Completions is selected.</li> <li>On success, strips and normalises whitespace, then enforces the subject character limit via <code>_normalise_subject()</code>, logging the attempt count via <code>loguru</code>.</li> <li>On exhausting retries, raises <code>CommitSummaryError</code> with a descriptive message including the number of attempts.</li> </ul>"},{"location":"loreley/core/worker/commit_summary/#normalisation-helpers","title":"Normalisation helpers","text":"<ul> <li><code>coerce_subject(text, *, default)</code>:</li> <li>Used as a safer fallback when LLM-based summarisation fails or when an existing subject must be clamped to a valid git style.</li> <li>Collapses whitespace, falls back to a provided <code>default</code> when <code>text</code> is empty, and applies <code>_normalise_subject()</code> to respect the configured character limit.</li> <li><code>_normalise_subject(text)</code>:</li> <li>Collapses consecutive whitespace to single spaces and trims leading/trailing spaces.</li> <li>If the cleaned subject exceeds <code>_subject_limit</code>, truncates it and appends an ellipsis to signal truncation.</li> <li><code>_build_prompt(job, plan, coding)</code> / <code>_truncate(text, limit=None)</code>:</li> <li>Internal helpers which format the rich multi-section prompt while bounding long summaries and step descriptions, ensuring that the most relevant context is preserved for the LLM.</li> </ul>"},{"location":"loreley/core/worker/evaluator/","title":"loreley.core.worker.evaluator","text":"<p>Evaluation utilities for Loreley's autonomous worker, responsible for running user-defined evaluation plugins in an isolated subprocess and turning their outputs into structured metrics.</p>"},{"location":"loreley/core/worker/evaluator/#domain-types","title":"Domain types","text":"<ul> <li><code>EvaluationMetric</code>: single metric reported by the evaluation plugin (<code>name</code>, numeric <code>value</code>, optional <code>unit</code>, <code>higher_is_better</code> flag, and optional structured <code>details</code> mapping). Provides <code>as_dict()</code> to produce a JSON-serialisable representation.</li> <li><code>EvaluationContext</code>: immutable-ish context object passed into plugins, including the git <code>worktree</code> path, optional <code>base_commit_hash</code> and <code>candidate_commit_hash</code>, optional <code>job_id</code> and high-level <code>goal</code>, an arbitrary <code>payload</code> dict (typically containing job and plan information), an optional <code>plan_summary</code>, and a free-form <code>metadata</code> dict. Paths and mappings are normalised and resolved in <code>__post_init__</code>.</li> <li><code>EvaluationResult</code>: structured result returned from evaluation, containing a mandatory <code>summary</code>, a tuple of <code>metrics</code>, a tuple of <code>tests_executed</code>, a tuple of textual <code>logs</code>, and an <code>extra</code> dict for arbitrary details; its <code>__post_init__</code> enforces a non-empty summary and normalises all collections.</li> </ul>"},{"location":"loreley/core/worker/evaluator/#exceptions-and-protocols","title":"Exceptions and protocols","text":"<ul> <li><code>EvaluationError</code>: custom runtime error raised when the evaluator cannot run the plugin successfully (import failures, bad configuration, timeouts, invalid payloads, etc.).</li> <li><code>EvaluationPlugin</code>: protocol type describing callables that accept an <code>EvaluationContext</code> and return either an <code>EvaluationResult</code> or a plain mapping compatible with <code>EvaluationResult</code> fields.</li> <li><code>EvaluationCallable</code>: internal alias for the concrete callable signature used by the evaluator.</li> </ul>"},{"location":"loreley/core/worker/evaluator/#evaluator","title":"Evaluator","text":"<ul> <li><code>Evaluator</code>: adapter around user-defined evaluation plugins that handles import, isolation, timeouts, and coercion into <code>EvaluationResult</code>.</li> <li>Configured via <code>loreley.config.Settings</code> worker evaluator options (<code>WORKER_EVALUATOR_PLUGIN</code>, <code>WORKER_EVALUATOR_PYTHON_PATHS</code>, <code>WORKER_EVALUATOR_TIMEOUT_SECONDS</code>, <code>WORKER_EVALUATOR_MAX_METRICS</code>).</li> <li><code>evaluate(context)</code>: validates that the <code>worktree</code> exists and is a directory, resolves or imports the plugin callable, logs the run via <code>loguru</code> and <code>rich</code>, executes the plugin in a separate process with a strict timeout, and converts the returned payload into an <code>EvaluationResult</code>, truncating the number of metrics to <code>max_metrics</code> when necessary.</li> <li>Supports two configuration modes:<ul> <li>A dotted string reference such as <code>package.module:plugin</code> or <code>package.module.plugin</code> via <code>WORKER_EVALUATOR_PLUGIN</code>.</li> <li>An inline callable passed at construction time (useful for tests or in-process usage), in which case no import is performed in the subprocess.</li> </ul> </li> <li>Extends <code>sys.path</code> using <code>WORKER_EVALUATOR_PYTHON_PATHS</code> before importing plugins, allowing evaluation logic to live outside the main application package.</li> </ul>"},{"location":"loreley/core/worker/evaluator/#plugin-execution-model","title":"Plugin execution model","text":"<ul> <li>The evaluator always runs plugins in a dedicated subprocess created via <code>multiprocessing.get_context(\"spawn\")</code>:</li> <li><code>_plugin_subprocess_entry()</code> prepares the Python path, imports or reuses the plugin callable, executes it with the provided <code>EvaluationContext</code>, and sends either an <code>(\"ok\", payload)</code> or <code>(\"error\", {message, traceback})</code> tuple back through a <code>multiprocessing.Queue</code>.</li> <li>The parent process waits up to <code>timeout</code> seconds for the subprocess to finish, and a small additional grace period to read from the queue.</li> <li>If the subprocess is still alive after the timeout, the evaluator terminates it and raises <code>EvaluationError</code> with a clear timeout message.</li> </ul>"},{"location":"loreley/core/worker/evaluator/#payload-coercion-helpers","title":"Payload coercion helpers","text":"<ul> <li><code>_coerce_result(payload)</code>: converts whatever the plugin returned into an <code>EvaluationResult</code>.</li> <li>Accepts an existing <code>EvaluationResult</code> instance as-is.</li> <li>When given a mapping, expects at least a non-empty <code>summary</code>, plus optional <code>metrics</code>, <code>tests_executed</code>, <code>logs</code>, and <code>extra</code> entries.</li> <li>Raises <code>EvaluationError</code> when the payload is missing a summary or is of an unsupported type.</li> <li><code>_coerce_metrics(metrics_payload)</code>: accepts a single <code>EvaluationMetric</code>, a mapping, or an iterable of these, and always returns a tuple of <code>EvaluationMetric</code> instances.</li> <li><code>_metric_from_mapping(payload)</code>: turns a mapping into an <code>EvaluationMetric</code>, enforcing presence and validity of <code>name</code> and <code>value</code> fields, and validating the shape of <code>unit</code>, <code>higher_is_better</code>, and <code>details</code>.</li> <li><code>_normalise_sequence(values, label)</code>: utility used to normalise <code>tests_executed</code> and <code>logs</code> into tuples of non-empty strings, accepting either a single string or an arbitrary iterable.</li> <li><code>_coerce_extra(payload)</code>: normalises the <code>extra</code> field into a plain dict, rejecting non-mapping inputs with <code>EvaluationError</code>.</li> <li><code>_validate_context(context)</code>: ensures that the <code>worktree</code> exists and is a directory before any plugin is run, failing fast with <code>EvaluationError</code> otherwise.</li> </ul>"},{"location":"loreley/core/worker/evolution/","title":"loreley.core.worker.evolution","text":"<p>Autonomous evolution worker that orchestrates planning, coding, evaluation, repository management, and persistence for a single evolution job.</p>"},{"location":"loreley/core/worker/evolution/#domain-types","title":"Domain types","text":"<ul> <li><code>CommitSnapshot</code>: immutable snapshot of commit-related data used to build planning context (<code>commit_hash</code>, derived <code>summary</code>, optional <code>evaluation_summary</code>, a tuple of text <code>highlights</code>, a tuple of <code>CommitMetric</code> instances, and an <code>extra_context</code> dict that may include both DB and MAP-Elites metadata). Exposes <code>to_planning_context()</code> to convert into a <code>CommitPlanningContext</code>.</li> <li><code>JobContext</code>: in-memory representation of a locked evolution job containing:</li> <li><code>job_id</code>, <code>base_commit_hash</code>, optional <code>island_id</code>, optional <code>experiment_id</code> and <code>repository_id</code>, and the raw job <code>payload</code>.</li> <li><code>base_snapshot</code> and <code>inspiration_snapshots</code> that wrap DB records and/or MAP-Elites payloads.</li> <li>user-facing <code>goal</code>, <code>constraints</code>, <code>acceptance_criteria</code>, optional <code>iteration_hint</code>, free-form <code>notes</code>, and <code>tags</code>, all normalised to tuples of strings.</li> <li>a boolean <code>is_seed_job</code> flag indicating whether the job is a cold-start seed job (root base commit, no inspirations, and/or an explicit <code>seed_job</code> marker in the payload).</li> <li><code>EvolutionWorkerResult</code>: structured success payload returned from <code>EvolutionWorker.run()</code>, combining the <code>job_id</code>, <code>base_commit_hash</code>, resulting <code>candidate_commit_hash</code>, the full <code>PlanningAgentResponse</code>, <code>CodingAgentResponse</code>, <code>EvaluationResult</code>, <code>CheckoutContext</code>, and the final <code>commit_message</code> used for the worker commit.</li> </ul>"},{"location":"loreley/core/worker/evolution/#public-worker-api","title":"Public worker API","text":"<ul> <li><code>EvolutionWorker</code>: service-layer entry point for running an evolution job synchronously end-to-end.</li> <li>Constructor wires together dependencies, all of which may be overridden for tests or custom orchestration:<ul> <li><code>WorkerRepository</code> for git operations.</li> <li><code>PlanningAgent</code> / <code>CodingAgent</code> for Codex-powered planning and coding.</li> <li><code>Evaluator</code> for running evaluation plugins.</li> <li><code>CommitSummarizer</code> for generating concise commit messages.</li> <li><code>EvolutionJobStore</code> for DB persistence of job status and results.</li> </ul> </li> <li><code>run(job_id)</code>:<ul> <li>Coerces the <code>job_id</code> into a <code>UUID</code>.</li> <li>Calls <code>_start_job()</code> to lock and validate the job row, building a <code>JobContext</code>.</li> <li>Checks out the base commit via <code>WorkerRepository.checkout_for_job()</code>.</li> <li>Runs planning (<code>_run_planning()</code>), coding (<code>_run_coding()</code>), and evaluation (<code>_run_evaluation()</code>) in sequence.</li> <li>Prepares a commit message via <code>_prepare_commit_message()</code>, then creates and pushes a new commit via <code>_create_commit()</code>.</li> <li>Persists success artifacts and metrics through <code>EvolutionJobStore.persist_success()</code> and prunes stale job branches.</li> <li>Returns an <code>EvolutionWorkerResult</code> when everything succeeds.</li> <li>On failure, records the error via <code>_mark_job_failed()</code> and re-raises, or directly propagates job lock/precondition errors.</li> </ul> </li> </ul>"},{"location":"loreley/core/worker/evolution/#orchestration-helpers","title":"Orchestration helpers","text":"<ul> <li><code>_start_job(job_id)</code>: uses <code>EvolutionJobStore.start_job()</code> to lock the job row, validates its status, and constructs a <code>JobContext</code> by:</li> <li>Loading commit metadata and metrics from the DB via <code>_load_commit_snapshot()</code>.</li> <li>Merging optional MAP-Elites record payloads from the job <code>payload</code> into <code>extra_context</code>.</li> <li>Deriving the job <code>goal</code>, <code>constraints</code>, <code>acceptance_criteria</code>, <code>iteration_hint</code>, <code>notes</code>, and <code>tags</code> from the payload and extra context using a set of coercion helpers; when no explicit goal is provided in the payload or <code>extra_context</code>, the worker falls back to the configured <code>Settings.worker_evolution_global_goal</code>. For cold-start seed jobs (root base commit with no inspirations and/or an explicit <code>seed_job</code> flag in <code>extra_context</code>), <code>_start_job</code> sets <code>is_seed_job=True</code> and appends a short seed hint to <code>iteration_hint</code> to make the cold-start semantics visible to downstream agents.</li> <li><code>_run_planning(job_ctx, checkout)</code>: builds a <code>PlanningAgentRequest</code> from commit snapshots and job fields, invokes <code>PlanningAgent.plan()</code>, and wraps <code>PlanningError</code> into <code>EvolutionWorkerError</code>. For seed jobs, <code>_run_planning</code> clears metrics, highlights, and evaluation details from the base planning context, drops all inspirations, and passes <code>cold_start=True</code> so that the planning agent treats the request as a cold-start seed population design run.</li> <li><code>_run_coding(job_ctx, plan, checkout)</code>: builds a <code>CodingAgentRequest</code> from the plan and job context, runs <code>CodingAgent.implement()</code>, and wraps <code>CodingError</code> into <code>EvolutionWorkerError</code>.</li> <li><code>_prepare_commit_message(job_ctx, plan, coding)</code>: delegates to <code>CommitSummarizer.generate()</code> to generate an LLM-backed git subject line; if summarisation fails, falls back to the coding agent's suggested <code>commit_message</code>, plan <code>summary</code>, or a generic <code>\"Evolution job &lt;id&gt;\"</code> string.</li> <li><code>_create_commit(checkout, commit_message)</code>: ensures the checkout is on a branch and that the repository contains changes, stages everything, creates a commit, and pushes the per-job branch using <code>force-with-lease</code>.</li> <li><code>_run_evaluation(job_ctx, checkout, plan, candidate_commit)</code>: constructs an <code>EvaluationContext</code> payload that includes job metadata and a normalised plan payload (via <code>build_plan_payload()</code>), then calls <code>Evaluator.evaluate()</code> and wraps <code>EvaluationError</code> into <code>EvolutionWorkerError</code>.</li> <li><code>_prune_job_branches()</code>: calls <code>WorkerRepository.prune_stale_job_branches()</code> and logs the number of branches removed, swallowing repository errors into warnings.</li> <li><code>_mark_job_failed(job_id, exc)</code>: logs a red failure message and forwards the concise error text to <code>EvolutionJobStore.mark_job_failed()</code>, ensuring job rows still capture failures even when other parts of the worker raise.</li> </ul>"},{"location":"loreley/core/worker/evolution/#data-extraction-and-normalisation","title":"Data extraction and normalisation","text":"<ul> <li><code>_load_commit_snapshot(commit_hash, fallback)</code>: pulls <code>CommitMetadata</code> and <code>Metric</code> rows for a given commit hash via <code>session_scope()</code>, merges DB and fallback MAP-Elites data into a <code>CommitSnapshot</code>, and derives:</li> <li>A human-readable <code>summary</code> built from commit message, fallback metadata, or a plain <code>\"Commit &lt;hash&gt;\"</code> string.</li> <li>A set of <code>highlights</code> assembled from various <code>highlights</code>/<code>snippets</code>/<code>notes</code> fields in DB and payload metadata.</li> <li>A list of <code>CommitMetric</code> values taken either from DB rows or fallback payload metrics.</li> <li>Additional helpers such as <code>_extract_goal()</code>, <code>_extract_iteration_hint()</code>, <code>_map_inspiration_payloads()</code>, <code>_extract_mapping()</code>, <code>_extract_highlights()</code>, <code>_first_non_empty()</code>, <code>_coerce_str_sequence()</code>, and <code>_coerce_uuid()</code> encapsulate common logic for turning loosely-structured job payloads into the strongly-typed structures that the planning and coding agents expect. In particular, <code>_extract_goal()</code> derives the goal from explicit job fields and the global configuration and does not use commit messages as a fallback.</li> </ul>"},{"location":"loreley/core/worker/job_store/","title":"loreley.core.worker.job_store","text":"<p>Persistence adapter for the evolution worker, responsible for locking jobs, storing results, and recording job failures in the database.</p>"},{"location":"loreley/core/worker/job_store/#domain-types-and-errors","title":"Domain types and errors","text":"<ul> <li><code>EvolutionWorkerError</code>: base runtime error used when the worker cannot complete or persist a job due to configuration, database, or repository issues.</li> <li><code>JobLockConflict</code>: raised when <code>start_job()</code> fails to obtain a NOWAIT lock on a job row, indicating that another worker is already processing the same job.</li> <li><code>JobPreconditionError</code>: raised when a job cannot start because preconditions are not satisfied (missing row, unsupported status, missing <code>base_commit_hash</code>, etc.).</li> <li><code>LockedJob</code>: dataclass snapshot of the locked <code>EvolutionJob</code> row containing the <code>job_id</code>, <code>base_commit_hash</code>, optional <code>island_id</code>, optional <code>experiment_id</code> and <code>repository_id</code>, the deserialised JSON <code>payload</code>, and the tuple of <code>inspiration_commit_hashes</code>. This is used by <code>EvolutionWorker</code> to build its <code>JobContext</code>.</li> </ul>"},{"location":"loreley/core/worker/job_store/#serialization-helpers","title":"Serialization helpers","text":"<ul> <li><code>build_plan_payload(response)</code>: converts a <code>PlanningAgentResponse</code> into a JSON-serialisable dict.</li> <li>Serialises the underlying <code>PlanningPlan</code> via <code>as_dict()</code>.</li> <li>Adds the planner <code>prompt</code>, raw Codex <code>raw_output</code>, CLI <code>command</code>, <code>stderr</code>, number of <code>attempts</code>, and total <code>duration_seconds</code> so downstream systems can introspect planner behaviour.</li> <li><code>build_coding_payload(response)</code>: converts a <code>CodingAgentResponse</code> into a dict that flattens both the <code>CodingPlanExecution</code> and transport metadata.</li> <li>Includes <code>implementation_summary</code>, final <code>commit_message</code>, detailed per-step outcomes, executed tests, recommended tests, follow-up items, notes, raw Codex output, prompt, command, stderr, attempts, and duration.</li> <li><code>build_evaluation_payload(result)</code>: converts an <code>EvaluationResult</code> into a dict containing its textual <code>summary</code>, a list of metric dicts via <code>metric.as_dict()</code>, <code>tests_executed</code>, <code>logs</code>, and an <code>extra</code> mapping.</li> </ul>"},{"location":"loreley/core/worker/job_store/#evolutionjobstore","title":"EvolutionJobStore","text":"<ul> <li><code>EvolutionJobStore</code>: database-facing adapter that encapsulates the lifecycle of an evolution job.</li> <li>Constructed with <code>Settings</code> to attach worker/application metadata when persisting results.</li> <li>Uses <code>session_scope()</code> and the ORM models from <code>loreley.db.models</code> (<code>EvolutionJob</code>, <code>CommitMetadata</code>, <code>Metric</code>, <code>JobStatus</code>) to modify rows transactionally.</li> </ul>"},{"location":"loreley/core/worker/job_store/#job-lifecycle-methods","title":"Job lifecycle methods","text":"<ul> <li><code>start_job(job_id)</code>:</li> <li>Acquires a row-level lock on the <code>EvolutionJob</code> using <code>SELECT ... FOR UPDATE NOWAIT</code>.</li> <li>Validates that the job exists, that <code>base_commit_hash</code> is present, and that the current <code>status</code> is in <code>{PENDING, QUEUED}</code>.</li> <li>Marks the job as <code>RUNNING</code>, records <code>started_at</code>, clears any <code>last_error</code>, and returns a <code>LockedJob</code> snapshot.</li> <li> <p>Wraps SQL errors into <code>JobLockConflict</code> when they indicate a lock-not-available condition, or <code>EvolutionWorkerError</code> otherwise.</p> </li> <li> <p><code>persist_success(job_ctx, plan, coding, evaluation, commit_hash, commit_message)</code>:</p> </li> <li>Updates the <code>EvolutionJob</code> row to <code>SUCCEEDED</code>, sets <code>completed_at</code>, stores the plan summary, updated job <code>payload</code> (including a compact <code>result</code> section with commit/metric/test summaries), and clears <code>last_error</code>.</li> <li>Inserts a new <code>CommitMetadata</code> row representing the produced commit, with parent commit hash, island ID, author/email from settings, commit message, evaluation summary, tags, and a rich <code>extra_context</code> payload that includes:</li> <li>Job context (goal, constraints, acceptance criteria, notes, tags, raw payload) plus the resolved <code>experiment_id</code> and <code>repository_id</code> for the job when available.</li> <li>An <code>experiment</code> block containing stable experiment metadata such as <code>id</code>, <code>repository_id</code>, <code>name</code>, <code>config_hash</code>, and <code>repository_slug</code> when the experiment can be resolved from the database.</li> <li>Base and inspiration commit hashes.</li> <li>Detailed plan, coding, and evaluation payloads via the helper functions above.</li> <li>Worker metadata such as <code>app_name</code>, environment, and completion timestamp.</li> <li>Inserts one <code>Metric</code> row per evaluation metric for the new commit, copying numeric <code>value</code>, <code>unit</code>, <code>higher_is_better</code>, and any structured <code>details</code>.</li> <li> <p>Wraps SQLAlchemy errors into <code>EvolutionWorkerError</code> so the caller can surface persistence failures cleanly.</p> </li> <li> <p><code>mark_job_failed(job_id, message)</code>:</p> </li> <li>Best-effort helper that records a failure reason on an <code>EvolutionJob</code> row.</li> <li>If the job no longer exists or has already reached <code>SUCCEEDED</code> or <code>CANCELLED</code>, the call becomes a no-op.</li> <li>Otherwise sets <code>status</code> to <code>FAILED</code>, stamps <code>completed_at</code>, and stores the latest <code>last_error</code> message.</li> <li>Swallows and logs any SQL errors rather than propagating them, to avoid masking the original worker exception.</li> </ul>"},{"location":"loreley/core/worker/job_store/#lock-conflict-detection","title":"Lock conflict detection","text":"<ul> <li><code>_is_lock_conflict(exc)</code>: inspects the original DB error to determine whether it represents a NOWAIT lock conflict.</li> <li>For PostgreSQL, checks for error code <code>\"55P03\"</code> (lock_not_available).</li> <li>Falls back to substring checks on the exception message for phrases like <code>\"could not obtain lock\"</code> or <code>\"database is locked\"</code>, covering other backends.</li> </ul>"},{"location":"loreley/core/worker/job_store/#time-helpers","title":"Time helpers","text":"<ul> <li><code>_utc_now()</code>: returns the current UTC <code>datetime</code> and is used consistently when stamping <code>started_at</code>, <code>completed_at</code>, and worker metadata timestamps.</li> </ul>"},{"location":"loreley/core/worker/planning/","title":"loreley.core.worker.planning","text":"<p>Planning utilities for Loreley's autonomous worker, responsible for turning commit history and evaluation results into a structured, multi-step plan that a coding agent can execute.</p>"},{"location":"loreley/core/worker/planning/#domain-types","title":"Domain types","text":"<ul> <li><code>CommitMetric</code>: lightweight value object describing a single evaluation metric (<code>name</code>, numeric <code>value</code>, optional <code>unit</code>, <code>higher_is_better</code> flag, and human-readable <code>summary</code>).</li> <li><code>CommitPlanningContext</code>: shared context for one commit, including the <code>commit_hash</code>, high-level <code>summary</code>, optional textual <code>highlights</code>, an optional <code>evaluation_summary</code>, a sequence of <code>CommitMetric</code> instances, and an <code>extra_context</code> dict for arbitrary structured details; normalises all collections to tuples/dicts on initialisation.</li> <li><code>PlanningAgentRequest</code>: input payload for the planning agent containing the <code>base</code> commit context, a sequence of <code>inspirations</code>, the plain-language global evolution <code>goal</code> (resolved by the evolution worker from either per-job payload fields or the <code>Settings.worker_evolution_global_goal</code> configuration), optional <code>constraints</code> and <code>acceptance_criteria</code> bullet lists, an optional <code>iteration_hint</code>, and a boolean <code>cold_start</code> flag; when <code>cold_start=True</code>, the planning agent treats the request as a cold-start seed population design run and adjusts the prompt accordingly. All list-like fields are normalised to tuples.</li> <li><code>PlanStep</code>: single actionable step in the generated plan (<code>step_id</code>, <code>title</code>, <code>intent</code>, <code>actions</code>, <code>files</code>, <code>dependencies</code>, <code>validation</code>, <code>risks</code>, <code>references</code>) with an <code>as_dict()</code> helper that converts all tuples back to plain lists for serialisation.</li> <li><code>PlanningPlan</code>: structured planning output that aggregates the global <code>summary</code>, <code>rationale</code>, <code>focus_metrics</code>, <code>guardrails</code>, <code>risks</code>, overall <code>validation</code> bullets, the ordered <code>steps</code>, optional <code>handoff_notes</code>, and an optional free-form <code>fallback_plan</code>, again with <code>as_dict()</code> for JSON-friendly output.</li> <li><code>PlanningAgentResponse</code>: envelope returned from the planner containing the domain <code>plan</code>, raw backend JSON <code>raw_output</code>, the rendered <code>prompt</code>, executed backend <code>command</code>, captured <code>stderr</code>, number of <code>attempts</code>, and total <code>duration_seconds</code>.</li> </ul>"},{"location":"loreley/core/worker/planning/#json-schema-and-validation","title":"JSON schema and validation","text":"<ul> <li><code>PLANNING_OUTPUT_SCHEMA</code>: JSON schema describing the expected shape of the planning output (top-level fields like <code>plan_summary</code>, <code>rationale</code>, <code>focus_metrics</code>, <code>guardrails</code>, <code>risks</code>, <code>validation</code>, <code>steps</code>, <code>handoff_notes</code>, and <code>fallback_plan</code>, plus constraints on each step's fields), used when invoking the external Codex CLI.</li> <li><code>_PlanStepModel</code> / <code>_PlanModel</code>: internal <code>pydantic</code> models that validate the Codex JSON payload against the schema and provide a typed bridge from raw JSON into the <code>PlanStep</code> / <code>PlanningPlan</code> domain objects.</li> <li>Agent backend: planning relies on <code>loreley.core.worker.agent_backend</code> for shared backend abstractions (<code>AgentBackend</code>, <code>StructuredAgentTask</code>, <code>AgentInvocation</code>) and the default <code>CodexCliBackend</code> implementation that talks to the <code>codex</code> CLI; see that module's documentation for backend configuration details.</li> </ul>"},{"location":"loreley/core/worker/planning/#planning-agent","title":"Planning agent","text":"<ul> <li><code>PlanningAgent</code>: high-level orchestration layer that prepares a structured planning request and delegates execution to a configurable backend.</li> <li>Instantiated with a <code>Settings</code> object and an optional <code>AgentBackend</code> implementation. When no backend is provided, it uses <code>CodexCliBackend</code> configured via <code>WORKER_PLANNING_CODEX_BIN</code>, <code>WORKER_PLANNING_CODEX_PROFILE</code>, <code>WORKER_PLANNING_MAX_ATTEMPTS</code>, <code>WORKER_PLANNING_TIMEOUT_SECONDS</code>, <code>WORKER_PLANNING_EXTRA_ENV</code>, <code>WORKER_PLANNING_SCHEMA_PATH</code>, and <code>WORKER_PLANNING_VALIDATION_MODE</code>. You can override the default by setting <code>WORKER_PLANNING_BACKEND</code> to a dotted Python path (<code>module:attr</code> or <code>module.attr</code>) that resolves to either an <code>AgentBackend</code> instance, a class implementing the <code>AgentBackend</code> protocol (constructed with no arguments), or a factory callable that returns such an instance.</li> <li><code>plan(request, *, working_dir)</code>: resolves the git worktree path, renders a rich natural-language prompt from the request (including base commit, inspiration commits, constraints, and acceptance criteria), builds a <code>StructuredAgentTask</code> whose schema usage depends on <code>WORKER_PLANNING_VALIDATION_MODE</code>, and asks the backend to execute it:<ul> <li>In <code>\"strict\"</code> mode, the planning JSON schema is passed to the backend (using Codex's native schema support when available), and the response must parse cleanly as <code>_PlanModel</code> or the agent will retry and eventually fail.</li> <li>In <code>\"lenient\"</code> mode, the same schema is still provided to the backend, but JSON decoding / validation failures are treated as non-fatal: the agent first attempts to parse the response into <code>_PlanModel</code>, and on failure synthesises a minimal <code>PlanningPlan</code> from the free-form output while keeping as much structure as possible.</li> <li>In <code>\"none\"</code> mode, no schema is enforced at the backend level and the agent skips JSON parsing entirely, always building a minimal <code>PlanningPlan</code> directly from the raw free-form output and job context.</li> </ul> </li> <li>Retries the backend invocation up to <code>max_attempts</code> times when process-level <code>PlanningError</code> / <code>ValidationError</code> / JSON decoding issues occur, logging warnings via <code>loguru</code> and printing concise progress messages with <code>rich</code>.</li> <li>On success, either parses JSON into <code>_PlanModel</code> and converts it into a <code>PlanningPlan</code>, or (for lenient/none modes when parsing is not possible) returns a best-effort synthetic <code>PlanningPlan</code>; on repeated failure or timeout, raises <code>PlanningError</code> with a descriptive message.</li> <li>Performs basic truncation of long text fields to keep prompts and summaries bounded and writes detailed debug artifacts under <code>logs/worker/planning</code>, including the effective <code>schema_mode</code> and <code>validation_mode</code>.</li> </ul>"},{"location":"loreley/core/worker/planning/#exceptions-and-helpers","title":"Exceptions and helpers","text":"<ul> <li><code>PlanningError</code>: custom runtime error raised when validation fails, the backend returns an error, the planning schema path is invalid, or the working directory is not a git repository.</li> <li><code>_truncate()</code>, <code>_format_commit_block()</code>, and <code>_format_metrics()</code>: internal utilities that format commit context and metrics into human-readable sections for the prompt while enforcing length limits and providing clear fallbacks when no metrics or highlights are available.</li> </ul>"},{"location":"loreley/core/worker/repository/","title":"loreley.core.worker.repository","text":"<p>Git worktree management for Loreley worker processes, responsible for cloning, syncing, cleaning, and publishing the upstream repository used for evolutionary jobs.</p>"},{"location":"loreley/core/worker/repository/#types","title":"Types","text":"<ul> <li><code>RepositoryError</code>: custom runtime error raised when a git operation fails, capturing the command, return code, stdout, and stderr for easier debugging.</li> <li><code>CheckoutContext</code>: frozen dataclass describing the result of preparing a job checkout (<code>job_id</code>, derived <code>branch_name</code>, selected <code>base_commit</code>, and local <code>worktree</code> path).</li> </ul>"},{"location":"loreley/core/worker/repository/#repository","title":"Repository","text":"<ul> <li><code>WorkerRepository</code>: high-level manager for the worker git worktree built on top of <code>git.Repo</code>.</li> <li>Configured via <code>loreley.config.Settings</code> worker repository options (<code>WORKER_REPO_REMOTE_URL</code>, <code>WORKER_REPO_BRANCH</code>, <code>WORKER_REPO_WORKTREE</code>, <code>WORKER_REPO_WORKTREE_RANDOMIZE</code>, <code>WORKER_REPO_WORKTREE_RANDOM_SUFFIX_LEN</code>, <code>WORKER_REPO_GIT_BIN</code>, <code>WORKER_REPO_FETCH_DEPTH</code>, <code>WORKER_REPO_CLEAN_EXCLUDES</code>, <code>WORKER_REPO_JOB_BRANCH_PREFIX</code>, <code>WORKER_REPO_ENABLE_LFS</code>, <code>WORKER_REPO_JOB_BRANCH_TTL_HOURS</code>) and honours commit author settings for worker-produced commits. <code>WORKER_REPO_REMOTE_URL</code> is mandatory; when it is absent the repository raises <code>RepositoryError</code> during construction. When <code>WORKER_REPO_WORKTREE_RANDOMIZE</code> is true, the final path segment of <code>WORKER_REPO_WORKTREE</code> gains a random hexadecimal suffix (length controlled by <code>WORKER_REPO_WORKTREE_RANDOM_SUFFIX_LEN</code>, default 8) so multiple workers on the same host can use isolated clones.</li> <li><code>prepare()</code> ensures the worktree directory exists, clones the remote with the configured depth/branch if necessary, aligns the local tracking branch with the configured upstream, and refreshes tags/LFS where enabled, logging progress via <code>rich</code> and <code>loguru</code>.</li> <li><code>checkout_for_job(job_id, base_commit, create_branch=True)</code> cleans the worktree, ensures the <code>base_commit</code> object is available locally (unshallows the repository when needed), then either checks out that commit in detached mode or creates a per-job branch under the configured job-branch prefix, returning a <code>CheckoutContext</code>.</li> <li><code>clean_worktree()</code> hard-resets tracked files and runs <code>git clean -xdf</code>, preserving any paths configured in <code>WORKER_REPO_CLEAN_EXCLUDES</code>.</li> <li><code>current_commit()</code> returns the current HEAD commit hash for observability and scheduling.</li> <li><code>has_changes()</code> reports whether the worktree is dirty (including untracked files), which the evolution worker uses to decide if there is anything to commit after coding.</li> <li><code>stage_all()</code> stages all tracked and untracked changes, and <code>commit(message)</code> creates a commit and returns its hash, using GitPython under the hood.</li> <li><code>push_branch(branch_name, remote=\\\"origin\\\", force_with_lease=False)</code> pushes the current branch to the configured remote (optionally with <code>--force-with-lease</code>), and <code>delete_remote_branch(branch_name, remote=\\\"origin\\\")</code> removes remote job branches without affecting local history.</li> <li><code>prune_stale_job_branches()</code> enumerates remote job branches under the configured job-branch prefix and deletes those whose last commit is older than <code>WORKER_REPO_JOB_BRANCH_TTL_HOURS</code>, logging a concise summary of how many branches were pruned.</li> <li>Internal helpers such as <code>_ensure_worktree_ready()</code>, <code>_sync_upstream()</code>, <code>_ensure_remote_origin()</code>, <code>_fetch()</code>, <code>_sync_lfs()</code>, <code>_ensure_commit_available()</code>, and <code>_wrap_git_error()</code> encapsulate the GitPython integration, remote configuration, LFS sync, shallow/unshallow behaviour, and consistent error wrapping with sanitised git commands.</li> </ul>"},{"location":"loreley/db/base/","title":"loreley.db.base","text":"<p>Database engine and session management for Loreley.</p>"},{"location":"loreley/db/base/#engine-and-session-factory","title":"Engine and session factory","text":"<ul> <li><code>_sanitize_dsn(raw_dsn)</code>: masks the password portion of a database DSN so it can be safely logged.</li> <li><code>engine</code>: global SQLAlchemy engine created from <code>Settings.database_dsn</code>, configured with <code>pool_pre_ping</code>, connection pool sizing, timeouts, and optional SQL echoing.</li> <li><code>SessionLocal</code>: scoped session factory bound to <code>engine</code>, with <code>autocommit=False</code>, <code>autoflush=False</code>, and <code>expire_on_commit=False</code> to make ORM usage predictable in long-running workers.</li> </ul>"},{"location":"loreley/db/base/#declarative-base-and-context-manager","title":"Declarative base and context manager","text":"<ul> <li><code>Base</code>: shared declarative base class used by all ORM models in <code>loreley.db.models</code>.</li> <li><code>session_scope()</code>: context manager that yields a <code>Session</code>, commits on success, rolls back on exception, logs failures with <code>loguru</code>, and always disposes of the session via <code>SessionLocal.remove()</code>.</li> </ul>"},{"location":"loreley/db/models/","title":"loreley.db.models","text":"<p>ORM models and enums for tracking evolutionary jobs, commits, and associated metrics.</p>"},{"location":"loreley/db/models/#shared-mixins-and-enums","title":"Shared mixins and enums","text":"<ul> <li><code>TimestampMixin</code>: adds <code>created_at</code> and <code>updated_at</code> columns that default to <code>now()</code> and automatically update on modification.</li> <li><code>JobStatus</code>: string-based <code>Enum</code> capturing the lifecycle of an evolution job (<code>PENDING</code>, <code>QUEUED</code>, <code>RUNNING</code>, <code>SUCCEEDED</code>, <code>FAILED</code>, <code>CANCELLED</code>).</li> </ul>"},{"location":"loreley/db/models/#core-models","title":"Core models","text":"<ul> <li><code>Repository</code> (<code>repositories</code> table): normalised view of a source code repository.</li> <li>Stores a stable <code>slug</code> derived from either the canonical remote URL or local worktree path, the current <code>remote_url</code>, optional <code>root_path</code>, and an <code>extra</code> JSONB payload with additional metadata (canonical origin, remotes, etc.).</li> <li>Owns a collection of <code>Experiment</code> rows and is treated as the top-level key when reasoning about experiments in a multi-repository deployment.</li> <li><code>Experiment</code> (<code>experiments</code> table): captures a single experiment configuration within a repository.</li> <li>References a <code>repository_id</code>, a stable <code>config_hash</code> computed from a subset of <code>Settings</code>, an optional human-readable <code>name</code>, a JSONB <code>config_snapshot</code> of the relevant settings, and a free-form <code>status</code>.</li> <li>Relates to <code>EvolutionJob</code>, <code>CommitMetadata</code>, and <code>MapElitesState</code> so that jobs, commits, and archive state can all be grouped by experiment.</li> <li><code>CommitMetadata</code> (<code>commits</code> table): stores git commit metadata and evolution context.</li> <li>Tracks commit hash, parent hash, optional island identifier, optional <code>experiment_id</code>, author, message, evaluation summary, free-form tags, and arbitrary JSONB <code>extra_context</code>.</li> <li>Defines relationships to associated <code>Metric</code> records and jobs that use this commit as their base, and back to the owning <code>Experiment</code> when one exists.</li> <li><code>Metric</code> (<code>metrics</code> table): records individual evaluation metrics for a commit.</li> <li>Stores metric <code>name</code>, numeric <code>value</code>, optional <code>unit</code>, whether higher values are better, and a JSONB <code>details</code> payload.</li> <li>Links back to <code>CommitMetadata</code> via <code>commit_hash</code> and maintains uniqueness per <code>(commit_hash, name)</code>.</li> <li><code>EvolutionJob</code> (<code>evolution_jobs</code> table): represents a single evolution iteration scheduled by the system.</li> <li>Tracks current <code>status</code>, base commit, island ID, optional <code>experiment_id</code>, inspiration commit hashes, request <code>payload</code>, human-readable <code>plan_summary</code>, priority, scheduling/processing timestamps, and last error if any.</li> <li>Relates back to <code>CommitMetadata</code> via <code>base_commit_hash</code> and to <code>Experiment</code> via <code>experiment_id</code>, enabling efficient queries per base commit or experiment.</li> <li><code>MapElitesState</code> (<code>map_elites_states</code> table): persists per-experiment, per-island snapshots of the MAP-Elites archive.</li> <li>Uses a composite primary key <code>(experiment_id, island_id)</code> so that multiple experiments can maintain independent archives even when they share island identifiers.</li> <li>Stores a JSONB <code>snapshot</code> payload containing lightweight metadata (feature bounds, PCA projection payload, schema version, and other knobs).</li> <li>For <code>schema_version &gt;= 2</code>, the large <code>archive</code>/<code>history</code> payloads are stored incrementally in separate tables and     reconstructed on load by <code>loreley.core.map_elites.snapshot.DatabaseSnapshotBackend</code>.</li> <li>For legacy rows (<code>schema_version &lt; 2</code>) that still embed <code>archive</code>/<code>history</code> lists, the loader performs lazy migration     into the incremental tables on first read and strips the large fields from <code>snapshot</code>.</li> <li><code>MapElitesArchiveCell</code> (<code>map_elites_archive_cells</code> table): one row per occupied MAP-Elites archive cell.</li> <li>Primary key: <code>(experiment_id, island_id, cell_index)</code>.</li> <li>Stores the cell's <code>commit_hash</code>, <code>objective</code>, behaviour <code>measures</code>, stored <code>solution</code> vector, free-form <code>metadata</code>, and <code>timestamp</code>.</li> <li>Enables cheap per-cell upserts when a commit improves a specific cell.</li> <li><code>MapElitesPcaHistory</code> (<code>map_elites_pca_history</code> table): incremental PCA history entries used to restore dimensionality reduction state.</li> <li>Primary key: <code>(experiment_id, island_id, commit_hash)</code>.</li> <li>Stores the commit's penultimate embedding <code>vector</code> plus embedding provenance (dimensions/models) and a <code>last_seen_at</code> marker used     to restore ordered, bounded history windows across restarts.</li> <li><code>MapElitesFileEmbeddingCache</code> (<code>map_elites_file_embedding_cache</code> table): persistent file-level embedding cache keyed by git blob SHA.</li> <li>Uses a composite primary key <code>(blob_sha, embedding_model, dimensions, pipeline_signature)</code>.</li> <li>Stores a float array <code>vector</code> containing the file embedding, allowing repo-state embeddings to reuse unchanged file vectors across commits.</li> </ul>"},{"location":"loreley/scheduler/ingestion/","title":"loreley.scheduler.ingestion","text":"<p>Result ingestion and MAP-Elites maintenance logic extracted from the central evolution scheduler.</p> <p>The <code>MapElitesIngestion</code> class owns how succeeded jobs are discovered, mapped to git commits, and folded into the MAP-Elites archives, as well as how the configured experiment root commit is initialised.</p>"},{"location":"loreley/scheduler/ingestion/#mapelitesingestion","title":"MapElitesIngestion","text":"<pre><code>from loreley.scheduler.ingestion import MapElitesIngestion\n</code></pre> <ul> <li>Purpose: ingest completed evolution jobs into MAP-Elites, record rich   ingestion state back onto the job payload, and ensure the experiment's root   commit is registered and evaluated as a baseline in the database.</li> <li>Construction: created by <code>EvolutionScheduler</code> with:</li> <li>the shared <code>Settings</code> instance,</li> <li>the interactive <code>rich</code> console,</li> <li>a <code>git.Repo</code> handle for the scheduler's repository root,</li> <li>the experiment-scoped <code>MapElitesManager</code>,</li> <li>the current <code>experiment</code> and its <code>repository</code>.</li> </ul>"},{"location":"loreley/scheduler/ingestion/#ingesting-succeeded-jobs","title":"Ingesting succeeded jobs","text":"<ul> <li><code>ingest_completed_jobs() -&gt; int</code>:</li> <li>Scans for <code>SUCCEEDED</code> <code>EvolutionJob</code> rows up to     <code>SCHEDULER_INGEST_BATCH_SIZE</code>.</li> <li>Filters out jobs whose payload already records a terminal ingestion status     (<code>\"succeeded\"</code> or <code>\"skipped\"</code>).</li> <li>Builds a <code>JobSnapshot</code> for each remaining job and forwards it to     <code>_ingest_snapshot(...)</code>.</li> <li>Returns the number of jobs whose commits actually updated the MAP-Elites     archive.</li> </ul> <p>Internally, <code>_ingest_snapshot(...)</code>:</p> <ol> <li>Extracts <code>result.commit_hash</code> and optional <code>result.metrics</code> from the job    payload.</li> <li>Ensures the corresponding git commit is present locally, fetching from    remotes as necessary.</li> <li>Calls <code>MapElitesManager.ingest(...)</code> with:</li> <li><code>commit_hash</code>,</li> <li><code>metrics</code>,</li> <li><code>island_id</code>,</li> <li><code>repo_root</code> and <code>treeish</code>,</li> <li>a structured metadata block that links the job, context, and evaluation      summaries.</li> <li>Writes a compact ingestion state back under    <code>payload[\"ingestion\"][\"map_elites\"]</code>, including:</li> <li><code>status</code> (<code>\"succeeded\"</code> or <code>\"skipped\"</code>),</li> <li><code>delta</code>, <code>status_code</code>, and <code>message</code> from the ingest result,</li> <li>a serialised view of the archive record (if any),</li> <li>retry bookkeeping (<code>attempts</code>, <code>last_attempt_at</code>, <code>reason</code>).</li> </ol> <p>This state allows ingestion to be retried safely and audited later without re-running the full evaluation.</p>"},{"location":"loreley/scheduler/ingestion/#root-commit-initialisation","title":"Root commit initialisation","text":"<p>When <code>MAPELITES_EXPERIMENT_ROOT_COMMIT</code> is set, <code>EvolutionScheduler</code> asks <code>MapElitesIngestion</code> to initialise that commit via <code>initialise_root_commit(commit_hash)</code>:</p> <ol> <li><code>_ensure_commit_available(...)</code> guarantees the commit exists locally,    fetching from remotes as needed.</li> <li><code>_ensure_root_commit_metadata(...)</code> creates or updates a <code>CommitMetadata</code>    row with:</li> <li>the commit's parent, author, and message,</li> <li>the current <code>experiment_id</code>,</li> <li>a default island id (from <code>MAPELITES_DEFAULT_ISLAND_ID</code> or <code>\"main\"</code>),</li> <li>a rich <code>extra_context</code> block that links back to the experiment and      repository.</li> <li><code>_ensure_root_commit_evaluated(...)</code> runs a one-off evaluation for the root    commit when no <code>Metric</code> rows already exist, writing baseline metrics into    the <code>metrics</code> table and a compact <code>root_evaluation</code> block into    <code>CommitMetadata.extra_context</code>. These metrics act as an experiment-wide    baseline but do not insert the root commit into any MAP-Elites archive.</li> </ol> <p>Failures during root-commit initialisation are logged but do not prevent the scheduler from running; they simply mean the experiment may effectively start from the first successfully ingested commit instead.</p>"},{"location":"loreley/scheduler/ingestion/#interaction-with-evolutionscheduler","title":"Interaction with EvolutionScheduler","text":"<p><code>EvolutionScheduler.tick()</code> uses <code>MapElitesIngestion</code> as the first stage in the pipeline:</p> <ol> <li><code>ingest_completed_jobs()</code> ingests any newly succeeded jobs and annotates    them with ingestion state.</li> <li>Later, when all jobs have finished and the global job limit has been    reached, <code>EvolutionScheduler</code> uses MAP-Elites metrics and commit metadata    to create a dedicated git branch for the current best-fitness commit.</li> </ol> <p>Separating this logic into <code>MapElitesIngestion</code> keeps the main scheduler loop small and clarifies the boundary between job lifecycle and archive maintenance.</p>"},{"location":"loreley/scheduler/job_scheduler/","title":"loreley.scheduler.job_scheduler","text":"<p>Job production and dispatch logic extracted from the central evolution scheduler.</p> <p>The <code>JobScheduler</code> class keeps all concerns related to how many jobs can be scheduled, which jobs should be dispatched next, and when they are submitted to the Dramatiq worker queue.</p>"},{"location":"loreley/scheduler/job_scheduler/#jobscheduler","title":"JobScheduler","text":"<pre><code>from loreley.scheduler.job_scheduler import JobScheduler\n</code></pre> <ul> <li>Purpose: encapsulate database interaction and Dramatiq calls for   scheduling and dispatching evolution jobs, so that the main   <code>EvolutionScheduler</code> can focus on orchestration.</li> <li>Construction: created by <code>EvolutionScheduler</code> with:</li> <li>a shared <code>Settings</code> instance,</li> <li>the interactive <code>rich</code> console,</li> <li>the experiment-scoped <code>MapElitesSampler</code>,</li> <li>the current <code>experiment_id</code>.</li> </ul>"},{"location":"loreley/scheduler/job_scheduler/#measuring-unfinished-work","title":"Measuring unfinished work","text":"<ul> <li><code>count_unfinished_jobs()</code>:</li> <li>Counts all jobs whose status is one of <code>PENDING</code>, <code>QUEUED</code>, or <code>RUNNING</code>.</li> <li>Used by the main scheduler loop to decide how much new work (if any) can     safely be created this tick.</li> </ul>"},{"location":"loreley/scheduler/job_scheduler/#scheduling-new-jobs","title":"Scheduling new jobs","text":"<ul> <li><code>schedule_jobs(unfinished_jobs: int, *, total_scheduled_jobs: int) -&gt; int</code>:</li> <li>Enforces <code>SCHEDULER_MAX_UNFINISHED_JOBS</code> as an upper bound across     <code>PENDING</code>/<code>QUEUED</code>/<code>RUNNING</code> jobs.</li> <li>Respects the optional <code>SCHEDULER_MAX_TOTAL_JOBS</code> global cap using the     <code>total_scheduled_jobs</code> counter maintained by <code>EvolutionScheduler</code>.</li> <li>Requests new work from MAP-Elites via     <code>MapElitesSampler.schedule_job(experiment_id=experiment_id)</code>.</li> <li>Immediately transitions any newly created jobs to <code>QUEUED</code> and pushes them     to Dramatiq using the private <code>_enqueue_jobs(...)</code> helper.</li> <li>Returns the number of jobs scheduled during this tick.</li> </ul> <p>If the sampler indicates that no archive cell currently wants new work, the console logs a short <code>[yellow]Sampler returned no job[/]</code> message and no rows are touched in the database.</p>"},{"location":"loreley/scheduler/job_scheduler/#dispatching-pending-jobs","title":"Dispatching pending jobs","text":"<ul> <li><code>dispatch_pending_jobs() -&gt; int</code>:</li> <li>Selects up to <code>SCHEDULER_DISPATCH_BATCH_SIZE</code> jobs with status <code>PENDING</code>,     ordered by:<ol> <li><code>priority</code> (descending),</li> <li><code>scheduled_at</code> (ascending),</li> <li><code>created_at</code> (ascending), so that higher-priority and older jobs drain first.</li> </ol> </li> <li>Uses a <code>SELECT ... FOR UPDATE</code> window to safely promote eligible jobs to     <code>QUEUED</code> and stamp their <code>scheduled_at</code> time.</li> <li>Sends each queued job id to the Dramatiq <code>run_evolution_job</code> actor.</li> <li>Returns the number of jobs successfully dispatched this tick.</li> </ul> <p>Any failures while enqueuing individual jobs are logged with Loguru and surfaced on the Rich console, but do not prevent other jobs from being dispatched.</p>"},{"location":"loreley/scheduler/job_scheduler/#interaction-with-evolutionscheduler","title":"Interaction with EvolutionScheduler","text":"<p><code>EvolutionScheduler.tick()</code> calls into <code>JobScheduler</code> as follows:</p> <ol> <li><code>count_unfinished_jobs()</code> to measure current load.</li> <li><code>schedule_jobs(...)</code> to request new work from MAP-Elites, honouring both    capacity and global job limits.</li> <li><code>dispatch_pending_jobs()</code> to move ready jobs into the worker queue.</li> </ol> <p>This separation keeps the scheduler loop simple and makes it easier to test and evolve the job pipeline independently of the rest of the orchestration logic.</p>"},{"location":"loreley/scheduler/main/","title":"loreley.scheduler.main","text":"<p>Central orchestration loop that keeps the Loreley evolution pipeline moving by coupling the MAP-Elites archive, the PostgreSQL job store, and the Dramatiq worker queue.</p>"},{"location":"loreley/scheduler/main/#evolutionscheduler","title":"EvolutionScheduler","text":"<ul> <li>Purpose: continuously monitors unfinished jobs (<code>pending</code>, <code>queued</code>, <code>running</code>), schedules new work from the MAP-Elites archive when capacity allows, dispatches pending jobs to the Dramatiq <code>run_evolution_job</code> actor, and backfills the archive with freshly evaluated commits.</li> <li>Construction: <code>EvolutionScheduler(settings=None)</code> loads <code>loreley.config.Settings</code>, resolves the target repository root (preferring <code>SCHEDULER_REPO_ROOT</code> and falling back to <code>WORKER_REPO_WORKTREE</code>), initialises a <code>git</code> repository handle, derives a <code>Repository</code>/<code>Experiment</code> pair via <code>loreley.core.experiments.get_or_create_experiment()</code>, wires <code>MapElitesManager</code> (scoped to that <code>experiment_id</code>) plus <code>MapElitesSampler</code> with the same settings, and, when <code>MAPELITES_EXPERIMENT_ROOT_COMMIT</code> is set, delegates root-commit registration and baseline evaluation to <code>loreley.scheduler.ingestion.MapElitesIngestion</code>.</li> <li>Lifecycle:</li> <li><code>tick()</code> runs the ingest \u2192 dispatch \u2192 measure \u2192 seed \u2192 schedule pipeline and logs a concise summary for observability. Each stage is isolated so failures are logged and do not crash the loop.</li> <li><code>run_forever()</code> installs <code>SIGINT</code>/<code>SIGTERM</code> handlers, runs <code>tick()</code> at the configured poll interval, and keeps looping until interrupted.</li> <li><code>--once</code> CLI flag runs a single tick and exits, useful for cron jobs or tests.</li> <li>Job scheduling &amp; dispatching: the scheduler delegates all capacity calculations, MAP-Elites sampling, and Dramatiq job submission to <code>loreley.scheduler.job_scheduler.JobScheduler</code>, which:</li> <li>counts unfinished jobs in the database,</li> <li>enforces <code>SCHEDULER_MAX_UNFINISHED_JOBS</code> and the optional <code>SCHEDULER_MAX_TOTAL_JOBS</code> cap,</li> <li>calls <code>MapElitesSampler.schedule_job(experiment_id=experiment.id)</code> to produce new work, and</li> <li>marks rows as <code>QUEUED</code> and sends them to the <code>run_evolution_job</code> actor in priority order.</li> <li>MAP-Elites maintenance: ingestion of succeeded jobs is handled by <code>loreley.scheduler.ingestion.MapElitesIngestion</code>, which:</li> <li>scans for <code>SUCCEEDED</code> jobs that have not yet been fully ingested,</li> <li>extracts their <code>result.commit_hash</code>, fetches the corresponding git commit, and computes per-file change counts,</li> <li>calls <code>MapElitesManager.ingest(...)</code> with metrics and contextual metadata, and</li> <li>writes a detailed ingestion status block back under <code>payload[\"ingestion\"][\"map_elites\"]</code> (including attempts, delta, placement, and any error messages).</li> </ul>"},{"location":"loreley/scheduler/main/#configuration","title":"Configuration","text":"<p>The scheduler consumes the following <code>Settings</code> fields (all exposed as environment variables):</p> <ul> <li><code>SCHEDULER_REPO_ROOT</code>: optional path to a read-only clone of the evolved repository; defaults to <code>WORKER_REPO_WORKTREE</code>.</li> <li><code>SCHEDULER_POLL_INTERVAL_SECONDS</code>: delay between scheduler ticks (default: <code>30</code> seconds).</li> <li><code>SCHEDULER_MAX_UNFINISHED_JOBS</code>: hard cap on the number of jobs that are not yet finished (<code>pending</code>, <code>queued</code>, <code>running</code>).</li> <li><code>SCHEDULER_SCHEDULE_BATCH_SIZE</code>: maximum number of new jobs sampled from MAP-Elites per tick (bounded by the unused capacity).</li> <li><code>SCHEDULER_DISPATCH_BATCH_SIZE</code>: number of pending jobs promoted to <code>QUEUED</code> and sent to Dramatiq per tick.</li> <li><code>SCHEDULER_INGEST_BATCH_SIZE</code>: number of newly succeeded jobs ingested into MAP-Elites per tick.</li> <li><code>MAPELITES_EXPERIMENT_ROOT_COMMIT</code>: optional git commit hash used as the logical root for the current experiment. When set, the scheduler ensures a <code>CommitMetadata</code> row exists for that commit and runs a one-off baseline evaluation to populate <code>Metric</code> rows and a compact <code>root_evaluation</code> block on the metadata, treating it as an experiment-wide baseline rather than inserting it into any MAP-Elites archive. During cold-start, when the archive is empty and no jobs exist yet, the scheduler first generates up to <code>MAPELITES_SEED_POPULATION_SIZE</code> seed evolution jobs from this root commit to form the initial population before switching to regular MAP-Elites sampling.</li> </ul>"},{"location":"loreley/scheduler/main/#cli-usage","title":"CLI usage","text":"<pre><code>uv run python -m loreley.scheduler.main        # continuous loop\nuv run python -m loreley.scheduler.main --once # single tick (cron / smoke tests)\n</code></pre> <p>For details about the dedicated CLI wrapper script (including logging setup and recommended usage), see <code>docs/script/run_scheduler.md</code>.</p> <p>Running the module imports <code>loreley.tasks.workers</code>, so the Dramatiq broker is configured before the first dispatch. Rich console output summarises each tick, while Loguru records detailed diagnostics for ingestion, scheduling, and dispatching via the dedicated <code>job_scheduler</code> and <code>ingestion</code> helper classes. This makes the scheduler easy to supervise either interactively or under a process manager. </p> <p>For more detailed information about these helper modules, see:</p> <ul> <li><code>loreley.scheduler.job_scheduler.JobScheduler</code> \u2014 job production and dispatch pipeline.</li> <li><code>loreley.scheduler.ingestion.MapElitesIngestion</code> \u2014 result ingestion, root-commit initialisation, and MAP-Elites maintenance.</li> </ul>"},{"location":"loreley/tasks/broker/","title":"loreley.tasks.broker","text":"<p>Helpers for configuring the Dramatiq Redis broker used by Loreley workers.</p>"},{"location":"loreley/tasks/broker/#public-api","title":"Public API","text":"<ul> <li> <p><code>build_redis_broker(settings: Settings | None = None) -&gt; RedisBroker</code>   Constructs a <code>RedisBroker</code> instance from the <code>Settings</code> object. It prefers <code>TASKS_REDIS_URL</code> when set and otherwise falls back to the individual <code>TASKS_REDIS_HOST</code>, <code>TASKS_REDIS_PORT</code>, <code>TASKS_REDIS_DB</code>, and <code>TASKS_REDIS_PASSWORD</code> fields, always attaching <code>TASKS_REDIS_NAMESPACE</code> as the Dramatiq namespace.</p> </li> <li> <p><code>setup_broker(settings: Settings | None = None) -&gt; RedisBroker</code>   Wraps <code>build_redis_broker()</code> and calls <code>dramatiq.set_broker(...)</code> so that Dramatiq actors use the configured Redis broker. It logs a sanitised representation of the Redis connection (scheme, host, port, and DB index) along with the logical namespace, explicitly avoiding logging any credentials from <code>TASKS_REDIS_URL</code> or <code>TASKS_REDIS_PASSWORD</code>.</p> </li> <li> <p><code>broker</code>   A module-level <code>RedisBroker</code> instance created eagerly by calling <code>setup_broker()</code> at import time. Importing <code>loreley.tasks.broker</code> is therefore sufficient to configure the global Dramatiq broker; this side effect is relied on by <code>loreley.tasks.workers</code> when running worker processes.</p> </li> </ul>"},{"location":"loreley/tasks/workers/","title":"loreley.tasks.workers","text":"<p>Dramatiq task actors that drive the Loreley evolution worker.</p>"},{"location":"loreley/tasks/workers/#evolution-worker","title":"Evolution worker","text":"<ul> <li><code>run_evolution_job(job_id: str) -&gt; None</code>   A Dramatiq actor that runs a single evolution job via <code>loreley.core.worker.evolution.EvolutionWorker</code>. The queue name, retry policy, and time limit are derived from the task-related settings in <code>loreley.config.Settings</code> (<code>TASKS_QUEUE_NAME</code>, <code>TASKS_WORKER_MAX_RETRIES</code>, and <code>TASKS_WORKER_TIME_LIMIT_SECONDS</code>). The time limit is configured in milliseconds at the actor level: positive <code>TASKS_WORKER_TIME_LIMIT_SECONDS</code> values set a hard wall-clock limit, while values <code>&lt;= 0</code> disable the time limit (no hard cap).</li> </ul> <p>On execution, the actor:</p> <ul> <li>Validates and normalises the <code>job_id</code> argument.</li> <li>Logs a \u201cjob started\u201d event to both the rich console and <code>loguru</code>.</li> <li>Delegates execution to <code>EvolutionWorker.run(...)</code>.</li> <li>Handles worker-specific exceptions with distinct behaviours:<ul> <li><code>JobLockConflict</code>: logs that the job was skipped due to a lock conflict and returns without raising.</li> <li><code>JobPreconditionError</code>: logs a warning and skips the job without raising (treating it as a non-retriable business error).</li> <li><code>EvolutionWorkerError</code>: logs an error and re-raises so Dramatiq can apply its retry policy.</li> <li>Any other unexpected exception: logs with a full stack trace and re-raises as a defensive fallback.</li> </ul> </li> <li>Logs a \u201cjob complete\u201d event including the resulting candidate commit hash on success.</li> </ul> <p>Importing <code>loreley.tasks.workers</code> also imports <code>loreley.tasks.broker</code>, which configures the global Dramatiq broker using the Redis settings in <code>loreley.config.Settings</code>.</p> <p>For details about the dedicated worker CLI wrapper script (including how it starts a single-process, single-threaded Dramatiq worker), see <code>docs/script/run_worker.md</code>.</p>"},{"location":"script/run_api/","title":"Running the UI API","text":""},{"location":"script/run_api/#running-the-ui-api","title":"Running the UI API","text":"<p>This script starts the read-only UI API based on FastAPI.</p>"},{"location":"script/run_api/#install-ui-dependencies","title":"Install UI dependencies","text":"<pre><code>uv sync --extra ui\n</code></pre>"},{"location":"script/run_api/#start","title":"Start","text":"<pre><code>uv run python script/run_api.py\n</code></pre>"},{"location":"script/run_api/#options","title":"Options","text":"<ul> <li><code>--host</code>: bind host (default: <code>127.0.0.1</code>)</li> <li><code>--port</code>: bind port (default: <code>8000</code>)</li> <li><code>--log-level</code>: override <code>LOG_LEVEL</code></li> <li><code>--reload</code>: enable auto-reload (development only)</li> </ul>"},{"location":"script/run_api/#logs","title":"Logs","text":"<p>Logs are written to:</p> <ul> <li><code>logs/ui_api/ui-api-&lt;timestamp&gt;.log</code></li> </ul>"},{"location":"script/run_scheduler/","title":"script/run_scheduler.py","text":"<p>Thin CLI wrapper for running the Loreley evolution scheduler.</p>"},{"location":"script/run_scheduler/#purpose","title":"Purpose","text":"<ul> <li>Expose a minimal CLI (<code>--help</code>, <code>--log-level</code>) that works even when required   environment variables are unset.</li> <li>Configure global Loguru logging based on <code>loreley.config.Settings.log_level</code>   with optional per-invocation overrides.</li> <li>Delegate scheduler-specific CLI parsing and control flow to   <code>loreley.scheduler.main.main</code>.</li> <li>Provide a convenient entrypoint for process managers and local development   without having to remember the module path.</li> </ul> <p>The underlying scheduling logic, MAP-Elites integration, and database interaction are all implemented in <code>loreley.scheduler.main.EvolutionScheduler</code>.</p>"},{"location":"script/run_scheduler/#behaviour","title":"Behaviour","text":"<ul> <li>Parses wrapper CLI arguments (notably <code>--help</code> and <code>--log-level</code>) before   loading configuration, so help output works without a valid environment.</li> <li>Calls <code>get_settings()</code> to load <code>Settings</code>; validation errors are printed to   the console and the process exits with code <code>1</code>.</li> <li>Resets Loguru sinks and installs a stderr sink at the configured level,   disabling backtraces and diagnosis in production-style runs.</li> <li>Resolves a log directory under <code>&lt;BASE&gt;/logs/scheduler</code> where <code>&lt;BASE&gt;</code> is:</li> <li><code>LOGS_BASE_DIR</code> (expanded as a path) when set.</li> <li>the current working directory when <code>LOGS_BASE_DIR</code> is unset.</li> <li>Adds a rotating file sink at <code>scheduler-YYYYMMDD.log</code> inside that directory   with <code>rotation=\"10 MB\"</code> and <code>retention=\"14 days\"</code>, so scheduler output is   always persisted for later debugging.</li> <li>Imports <code>loreley.scheduler.main.main</code> lazily after logging is configured; any   import failure is reported via console/log and exits with code <code>1</code>.</li> <li>Forwards any remaining CLI arguments to <code>loreley.scheduler.main.main(argv)</code>,   which supports the <code>--once</code> flag to run a single scheduler tick.</li> </ul> <p>Exit codes are determined by <code>loreley.scheduler.main.main</code>: on success it returns <code>0</code>, while unexpected exceptions bubble up and cause a non-zero exit.</p>"},{"location":"script/run_scheduler/#cli-usage","title":"CLI usage","text":"<p>Recommended usage with <code>uv</code>:</p> <pre><code>uv run python script/run_scheduler.py        # continuous loop\nuv run python script/run_scheduler.py --once # single tick (cron / smoke tests)\nuv run python script/run_scheduler.py --log-level DEBUG\n</code></pre> <p>The wrapper is equivalent to invoking the module directly:</p> <pre><code>uv run python -m loreley.scheduler.main        # continuous loop\nuv run python -m loreley.scheduler.main --once # single tick\n</code></pre>"},{"location":"script/run_scheduler/#configuration","title":"Configuration","text":"<p>The script relies on <code>loreley.config.Settings</code>:</p> <ul> <li><code>LOG_LEVEL</code> controls the global Loguru log level for the scheduler process.</li> <li><code>LOGS_BASE_DIR</code> (optional) overrides the base directory used for scheduler   log files; when unset, logs are written under <code>./logs/scheduler</code> relative to   the current working directory.</li> <li>All scheduler-related fields (<code>SCHEDULER_*</code>) and database/Redis settings are   consumed by <code>loreley.scheduler.main</code> and <code>loreley.tasks.broker</code>. See   <code>docs/loreley/config.md</code> and <code>docs/loreley/scheduler/main.md</code> for details.</li> </ul> <p>The <code>examples/evol_circle_packing.py</code> helper simply delegates to this script when running the scheduler, so its runs use the same logging configuration and log file locations.</p>"},{"location":"script/run_scheduler/#failure-handling","title":"Failure handling","text":"<ul> <li>Invalid or missing environment variables produce a short console message and   exit code <code>1</code> instead of an unhandled exception.</li> <li>Errors while importing or launching the scheduler are logged and surfaced   before the process exits, so misconfigured dependencies do not produce long   stack traces.</li> </ul>"},{"location":"script/run_ui/","title":"Running the UI","text":""},{"location":"script/run_ui/#running-the-streamlit-ui","title":"Running the Streamlit UI","text":"<p>The Streamlit UI is a read-only dashboard that calls the UI API.</p>"},{"location":"script/run_ui/#install-ui-dependencies","title":"Install UI dependencies","text":"<pre><code>uv sync --extra ui\n</code></pre>"},{"location":"script/run_ui/#start","title":"Start","text":"<p>Start the API first:</p> <pre><code>uv run python script/run_api.py\n</code></pre> <p>Then start Streamlit:</p> <pre><code>uv run python script/run_ui.py --api-base-url http://127.0.0.1:8000\n</code></pre>"},{"location":"script/run_ui/#options","title":"Options","text":"<ul> <li><code>--api-base-url</code>: base URL of the UI API (also available via <code>LORELEY_UI_API_BASE_URL</code>)</li> <li><code>--host</code>: Streamlit bind host (default: <code>127.0.0.1</code>)</li> <li><code>--port</code>: Streamlit bind port (default: <code>8501</code>)</li> <li><code>--headless</code>: run without opening a browser</li> </ul>"},{"location":"script/run_worker/","title":"script/run_worker.py","text":"<p>CLI wrapper that runs the Loreley evolution worker as a single Dramatiq process.</p>"},{"location":"script/run_worker/#purpose","title":"Purpose","text":"<ul> <li>Expose a minimal CLI (<code>--help</code>, <code>--log-level</code>) that works even when required   environment variables are unset.</li> <li>Configure global Loguru logging based on <code>loreley.config.Settings.log_level</code>   with optional per-invocation overrides.</li> <li>Lazily initialise the Dramatiq Redis broker (<code>loreley.tasks.broker.broker</code>)   and import <code>loreley.tasks.workers</code> so that the <code>run_evolution_job</code> actor is   registered.</li> <li>Start a single-threaded Dramatiq <code>Worker</code> bound to the configured queue.</li> </ul>"},{"location":"script/run_worker/#behaviour","title":"Behaviour","text":"<p>On startup the script:</p> <ol> <li>Parses CLI args (primarily <code>--help</code> and <code>--log-level</code>) before loading    configuration, so help output works without a valid environment.</li> <li>Calls <code>get_settings()</code> to load <code>Settings</code>; any validation error is printed to    the console and the process exits with code <code>1</code> instead of crashing.</li> <li>Configures Loguru to log to stderr using <code>LOG_LEVEL</code> (or <code>--log-level</code>) as    the threshold.</li> <li>Resolves a log directory under <code>&lt;BASE&gt;/logs/worker</code> where <code>&lt;BASE&gt;</code> is:</li> <li><code>LOGS_BASE_DIR</code> (expanded as a path) when set.</li> <li>the current working directory when <code>LOGS_BASE_DIR</code> is unset.</li> <li>Adds a rotating file sink at <code>worker-YYYYMMDD.log</code> inside that directory    with <code>rotation=\"10 MB\"</code> and <code>retention=\"14 days\"</code>, so worker output is    always persisted for later debugging.</li> <li>Imports <code>loreley.tasks.broker</code> (which constructs and registers the Redis    broker) and <code>loreley.tasks.workers</code> (which defines the <code>run_evolution_job</code>    actor and its queue settings) after logging is configured; any import/startup    failure is reported with a concise console message and exit code <code>1</code>.</li> <li>Logs a short \u201cworker online\u201d message including <code>TASKS_QUEUE_NAME</code> and    <code>WORKER_REPO_WORKTREE</code>.</li> <li>Creates a <code>dramatiq.Worker</code> with:</li> <li><code>broker</code> set to the global Redis broker instance.</li> <li><code>worker_threads=1</code> to ensure a single-threaded execution model.</li> <li>Installs <code>SIGINT</code>/<code>SIGTERM</code> handlers that call <code>worker.stop()</code> for a    graceful shutdown.</li> <li>Starts the worker and blocks with <code>worker.join()</code> until the process is    stopped.</li> </ol> <p>Keyboard interrupts (<code>Ctrl+C</code>) are handled explicitly with a friendly shutdown message.</p>"},{"location":"script/run_worker/#cli-usage","title":"CLI usage","text":"<p>Typical usage with <code>uv</code>:</p> <pre><code>uv run python script/run_worker.py\nuv run python script/run_worker.py --log-level DEBUG\n</code></pre> <p>The worker will begin consuming messages for the queue specified by <code>TASKS_QUEUE_NAME</code> (default: <code>loreley.evolution</code>) in a single process with a single worker thread. Jobs are expected to be created and dispatched by the scheduler (<code>loreley.scheduler.main</code>).</p> <p><code>--help</code> works without configured environment variables; other configuration is still read from <code>loreley.config.Settings</code>.</p>"},{"location":"script/run_worker/#configuration","title":"Configuration","text":"<p>The script uses <code>loreley.config.Settings</code> for:</p> <ul> <li>Logging</li> <li><code>LOG_LEVEL</code>: global Loguru level for worker logs.</li> <li><code>LOGS_BASE_DIR</code> (optional): overrides the base directory used for worker     log files; when unset, logs are written under <code>./logs/worker</code> relative to     the current working directory.</li> <li>Task queue / broker</li> <li><code>TASKS_REDIS_URL</code> or (<code>TASKS_REDIS_HOST</code>, <code>TASKS_REDIS_PORT</code>,     <code>TASKS_REDIS_DB</code>, <code>TASKS_REDIS_PASSWORD</code>, <code>TASKS_REDIS_NAMESPACE</code>).</li> <li><code>TASKS_QUEUE_NAME</code>: queue name for the <code>run_evolution_job</code> actor.</li> <li><code>TASKS_WORKER_MAX_RETRIES</code>, <code>TASKS_WORKER_TIME_LIMIT_SECONDS</code>: consumed     by <code>loreley.tasks.workers</code> when configuring the actor.</li> <li>Worker repository</li> <li><code>WORKER_REPO_REMOTE_URL</code>, <code>WORKER_REPO_BRANCH</code>, <code>WORKER_REPO_WORKTREE</code>,     <code>WORKER_REPO_WORKTREE_RANDOMIZE</code>, <code>WORKER_REPO_WORKTREE_RANDOM_SUFFIX_LEN</code>,     and related <code>WORKER_REPO_*</code> options used by     <code>loreley.core.worker.repository.WorkerRepository</code>.</li> </ul> <p>For a full description of these settings, see <code>docs/loreley/config.md</code> and the worker module documentation in <code>docs/loreley/tasks/workers.md</code>.</p> <p>The <code>examples/evol_circle_packing.py</code> helper simply delegates to this script when running the worker, so its runs use the same logging configuration and log file locations.</p>"},{"location":"script/run_worker/#failure-handling","title":"Failure handling","text":"<ul> <li>Invalid or missing environment variables produce a short console message and   exit code <code>1</code> instead of an unhandled exception.</li> <li>Errors while initialising the broker/worker modules are logged and surfaced   before the process exits, so misconfigured Redis/DB credentials do not   produce long stack traces.</li> </ul>"}]}