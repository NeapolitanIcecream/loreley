{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Loreley \u00b6 Automated MAP-Elites for entire git repositories. Loreley is an automated MAP-Elites system that evolves whole git repositories instead of single files or scripts. It continuously samples promising commits, asks external agents to implement and evaluate them, and archives the best-performing variants so they can be reused later. If you are new to the project, start with the high\u2011level ideas below and then follow the links into the focused module guides under docs/loreley and docs/script . Why Loreley? \u00b6 Whole-repo evolution \u2013 each individual is a real git commit, so results remain debuggable and compatible with normal tooling. Learned behaviour space \u2013 behaviour descriptors come from embeddings (optionally reduced with PCA) instead of hand\u2011picked heuristics. Production-ready loop \u2013 a Dramatiq/Redis worker fleet, PostgreSQL archive, and central scheduler keep the system running indefinitely. Architecture at a Glance \u00b6 Loreley is split into a few key areas: Configuration \u2013 loreley.config.Settings centralises environment\u2011driven configuration for logging, database, Redis/Dramatiq, scheduler, worker repositories, and MAP\u2011Elites knobs. See: Configuration Database \u2013 loreley.db defines the SQLAlchemy engine/session helpers and ORM models for repositories, experiments, jobs, metrics, and archive state. See: DB base , DB models MAP-Elites core \u2013 loreley.core.map_elites handles preprocessing, chunking, embeddings, dimensionality reduction, archive management, sampling, and snapshots. See: MAP\u2011Elites overview and the linked sub\u2011pages below. Worker pipeline \u2013 loreley.core.worker manages worktrees, planning, coding, evaluation, evolution commits, and commit summaries. See: worker module pages under loreley/core/worker/ . Scheduler \u2013 loreley.scheduler.main.EvolutionScheduler runs the ingest \u2192 dispatch \u2192 measure \u2192 schedule loop that keeps the archive and job store in sync. See: Scheduler Tasks & scripts \u2013 loreley.tasks exposes the Dramatiq broker and actors, while script/run_scheduler.py and script/run_worker.py are CLI wrappers. See: Tasks broker , Tasks workers , Running the scheduler , Running the worker Getting Started \u00b6 1. Requirements \u00b6 Python 3.11+ uv for dependency management PostgreSQL and Redis Git (including worktrees; LFS optional) 2. Install dependencies \u00b6 git clone <YOUR_FORK_OR_ORIGIN_URL> loreley cd loreley uv sync # install dependencies from pyproject.toml / uv.lock If you already have an environment, you can pin dependencies without creating a workspace: uv sync --no-workspace 3. Configure Loreley \u00b6 All runtime configuration is provided via environment variables and loaded by loreley.config.Settings . Common examples: APP_NAME , ENVIRONMENT , LOG_LEVEL DATABASE_URL (or individual DB_* fields) TASKS_REDIS_URL / TASKS_REDIS_HOST / TASKS_REDIS_PORT / TASKS_REDIS_DB / TASKS_REDIS_PASSWORD / TASKS_REDIS_NAMESPACE TASKS_QUEUE_NAME , TASKS_WORKER_MAX_RETRIES , TASKS_WORKER_TIME_LIMIT_SECONDS SCHEDULER_* and WORKER_REPO_* settings for the scheduler and worker repositories MAPELITES_* settings for preprocessing, embeddings, dimensionality reduction, bounds, archive, fitness, and sampling See Configuration for the exhaustive list. 4. Run the scheduler \u00b6 The scheduler drives ingestion, scheduling, dispatch, and archive maintenance: uv run python script/run_scheduler.py # continuous loop uv run python script/run_scheduler.py --once # single tick See Running the scheduler and Scheduler internals for more detail. 5. Run the worker \u00b6 A worker process consumes jobs from Dramatiq, applies planning/coding/evaluation, and pushes results back into the database: uv run python script/run_worker.py See Running the worker and the worker module pages under loreley/core/worker/ for operational and implementation details. Documentation Map \u00b6 Use this index as a quick map of the rest of the documentation: Configuration Global settings Database Engine and sessions ORM models MAP-Elites core Overview & archive Preprocessing Chunking Code embeddings Dimensionality reduction Sampler Snapshots Summary embeddings Worker pipeline Planning agent Coding agent Evaluator Evolution loop Commit summaries Job store Worker repository Scheduler & tasks Scheduler Tasks broker Tasks workers Operational scripts Run scheduler script Run worker script Next Steps \u00b6 Start by configuring a small test repository and running the scheduler/worker pair locally. Once the basic loop works, plug in a custom evaluator (for example, based on the circle packing example) and tune MAPELITES_* settings. When you are ready for production, point the scheduler at a long\u2011lived repository clone and supervise both processes with your preferred process manager.","title":"Home"},{"location":"#loreley","text":"Automated MAP-Elites for entire git repositories. Loreley is an automated MAP-Elites system that evolves whole git repositories instead of single files or scripts. It continuously samples promising commits, asks external agents to implement and evaluate them, and archives the best-performing variants so they can be reused later. If you are new to the project, start with the high\u2011level ideas below and then follow the links into the focused module guides under docs/loreley and docs/script .","title":"Loreley"},{"location":"#why-loreley","text":"Whole-repo evolution \u2013 each individual is a real git commit, so results remain debuggable and compatible with normal tooling. Learned behaviour space \u2013 behaviour descriptors come from embeddings (optionally reduced with PCA) instead of hand\u2011picked heuristics. Production-ready loop \u2013 a Dramatiq/Redis worker fleet, PostgreSQL archive, and central scheduler keep the system running indefinitely.","title":"Why Loreley?"},{"location":"#architecture-at-a-glance","text":"Loreley is split into a few key areas: Configuration \u2013 loreley.config.Settings centralises environment\u2011driven configuration for logging, database, Redis/Dramatiq, scheduler, worker repositories, and MAP\u2011Elites knobs. See: Configuration Database \u2013 loreley.db defines the SQLAlchemy engine/session helpers and ORM models for repositories, experiments, jobs, metrics, and archive state. See: DB base , DB models MAP-Elites core \u2013 loreley.core.map_elites handles preprocessing, chunking, embeddings, dimensionality reduction, archive management, sampling, and snapshots. See: MAP\u2011Elites overview and the linked sub\u2011pages below. Worker pipeline \u2013 loreley.core.worker manages worktrees, planning, coding, evaluation, evolution commits, and commit summaries. See: worker module pages under loreley/core/worker/ . Scheduler \u2013 loreley.scheduler.main.EvolutionScheduler runs the ingest \u2192 dispatch \u2192 measure \u2192 schedule loop that keeps the archive and job store in sync. See: Scheduler Tasks & scripts \u2013 loreley.tasks exposes the Dramatiq broker and actors, while script/run_scheduler.py and script/run_worker.py are CLI wrappers. See: Tasks broker , Tasks workers , Running the scheduler , Running the worker","title":"Architecture at a Glance"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#1-requirements","text":"Python 3.11+ uv for dependency management PostgreSQL and Redis Git (including worktrees; LFS optional)","title":"1. Requirements"},{"location":"#2-install-dependencies","text":"git clone <YOUR_FORK_OR_ORIGIN_URL> loreley cd loreley uv sync # install dependencies from pyproject.toml / uv.lock If you already have an environment, you can pin dependencies without creating a workspace: uv sync --no-workspace","title":"2. Install dependencies"},{"location":"#3-configure-loreley","text":"All runtime configuration is provided via environment variables and loaded by loreley.config.Settings . Common examples: APP_NAME , ENVIRONMENT , LOG_LEVEL DATABASE_URL (or individual DB_* fields) TASKS_REDIS_URL / TASKS_REDIS_HOST / TASKS_REDIS_PORT / TASKS_REDIS_DB / TASKS_REDIS_PASSWORD / TASKS_REDIS_NAMESPACE TASKS_QUEUE_NAME , TASKS_WORKER_MAX_RETRIES , TASKS_WORKER_TIME_LIMIT_SECONDS SCHEDULER_* and WORKER_REPO_* settings for the scheduler and worker repositories MAPELITES_* settings for preprocessing, embeddings, dimensionality reduction, bounds, archive, fitness, and sampling See Configuration for the exhaustive list.","title":"3. Configure Loreley"},{"location":"#4-run-the-scheduler","text":"The scheduler drives ingestion, scheduling, dispatch, and archive maintenance: uv run python script/run_scheduler.py # continuous loop uv run python script/run_scheduler.py --once # single tick See Running the scheduler and Scheduler internals for more detail.","title":"4. Run the scheduler"},{"location":"#5-run-the-worker","text":"A worker process consumes jobs from Dramatiq, applies planning/coding/evaluation, and pushes results back into the database: uv run python script/run_worker.py See Running the worker and the worker module pages under loreley/core/worker/ for operational and implementation details.","title":"5. Run the worker"},{"location":"#documentation-map","text":"Use this index as a quick map of the rest of the documentation: Configuration Global settings Database Engine and sessions ORM models MAP-Elites core Overview & archive Preprocessing Chunking Code embeddings Dimensionality reduction Sampler Snapshots Summary embeddings Worker pipeline Planning agent Coding agent Evaluator Evolution loop Commit summaries Job store Worker repository Scheduler & tasks Scheduler Tasks broker Tasks workers Operational scripts Run scheduler script Run worker script","title":"Documentation Map"},{"location":"#next-steps","text":"Start by configuring a small test repository and running the scheduler/worker pair locally. Once the basic loop works, plug in a custom evaluator (for example, based on the circle packing example) and tune MAPELITES_* settings. When you are ready for production, point the scheduler at a long\u2011lived repository clone and supervise both processes with your preferred process manager.","title":"Next Steps"},{"location":"loreley/config/","text":"loreley.config \u00b6 Centralised configuration for the Loreley application, backed by pydantic-settings and environment variables. Settings \u00b6 Settings : BaseSettings subclass that loads core application configuration. Environment : app_name , environment , log_level , logs_base_dir . log_level controls the global Loguru log level used across long-running processes (including the scheduler, workers, and their CLI wrappers); logs_base_dir (via LOGS_BASE_DIR ) optionally overrides the base directory where long-running process logs are written, defaulting to a logs/ directory under the current working directory. See the scripts documentation under docs/script for concrete examples. OpenAI-compatible API : OPENAI_API_KEY , OPENAI_BASE_URL , OPENAI_API_SPEC configure the API key, base URL, and API surface for all OpenAI-compatible LLM and embedding calls, used by loreley.core.map-elites.code_embedding.CodeEmbedder , loreley.core.map-elites.summarization_embedding.SummaryEmbedder , and loreley.core.worker.commit_summary.CommitSummarizer . When unset, OPENAI_API_KEY / OPENAI_BASE_URL fall back to the OpenAI Python client's own environment variable defaults. OPENAI_API_SPEC accepts: \"responses\" (default): use the unified responses API ( client.responses.create ) for text generation. \"chat_completions\" : use the classic Chat Completions API ( client.chat.completions.create ) while preserving the same high-level behaviour. Database : either a raw DATABASE_URL or individual DB_* fields (scheme, host, port, username, password, database name, pool options, echo flag). Metrics : metrics_retention_days controls how long metrics are retained. Task queue : TASKS_REDIS_URL , TASKS_REDIS_HOST , TASKS_REDIS_PORT , TASKS_REDIS_DB , TASKS_REDIS_PASSWORD , TASKS_REDIS_NAMESPACE , TASKS_QUEUE_NAME , TASKS_WORKER_MAX_RETRIES , and TASKS_WORKER_TIME_LIMIT_SECONDS configure the Dramatiq Redis broker connection details, logical namespace, queue routing, retry policy, and actor time limits used by loreley.tasks.broker and loreley.tasks.workers . When TASKS_REDIS_URL is set and includes credentials, only a sanitised scheme://host:port/db form is logged, never the raw URL or password. TASKS_WORKER_TIME_LIMIT_SECONDS is interpreted in seconds and converted to Dramatiq's millisecond time_limit : values <= 0 disable the time limit (no hard cap on actor runtime), while positive values enforce a per-job wall-clock limit. Scheduler : SCHEDULER_REPO_ROOT , SCHEDULER_POLL_INTERVAL_SECONDS , SCHEDULER_MAX_UNFINISHED_JOBS , SCHEDULER_SCHEDULE_BATCH_SIZE , SCHEDULER_DISPATCH_BATCH_SIZE , and SCHEDULER_INGEST_BATCH_SIZE drive loreley.scheduler.main . These options determine which git worktree the scheduler inspects, how often it runs a reconciliation tick, how many unfinished jobs are allowed at once, how aggressively it samples new MAP-Elites jobs per tick, how many pending jobs are dispatched to Dramatiq each cycle, and how many completed jobs are ingested back into the archive per tick. Worker repository : WORKER_REPO_REMOTE_URL , WORKER_REPO_BRANCH , WORKER_REPO_WORKTREE , WORKER_REPO_GIT_BIN , WORKER_REPO_FETCH_DEPTH , WORKER_REPO_CLEAN_EXCLUDES , WORKER_REPO_JOB_BRANCH_PREFIX , WORKER_REPO_ENABLE_LFS , and WORKER_REPO_JOB_BRANCH_TTL_HOURS configure the git worktree used by worker processes (upstream remote and branch, local checkout path, git binary, shallow clone depth, clean exclusions, job branch naming, optional Git LFS support, and how long remote job branches are retained before pruning), used by loreley.core.worker.repository.WorkerRepository . Worker planning : WORKER_PLANNING_* options configuring how the external Codex CLI planner is invoked (binary path, optional profile, maximum attempts, timeout, extra environment variables, and an optional JSON schema override), used by loreley.core.worker.planning.PlanningAgent . Worker coding : WORKER_CODING_* options configuring how the external Codex-based coding agent is invoked (binary path, optional profile, maximum attempts, timeout, extra environment variables, and an optional JSON schema override), used by loreley.core.worker.coding.CodingAgent . Worker evaluator : WORKER_EVALUATOR_PLUGIN , WORKER_EVALUATOR_PYTHON_PATHS , WORKER_EVALUATOR_TIMEOUT_SECONDS , and WORKER_EVALUATOR_MAX_METRICS configure the evaluation plugin entry point, additional Python paths, subprocess timeout, and the maximum number of metrics to keep, used by loreley.core.worker.evaluator.Evaluator . Worker evolution commits : WORKER_EVOLUTION_COMMIT_MODEL , WORKER_EVOLUTION_COMMIT_TEMPERATURE , WORKER_EVOLUTION_COMMIT_MAX_OUTPUT_TOKENS , WORKER_EVOLUTION_COMMIT_MAX_RETRIES , WORKER_EVOLUTION_COMMIT_RETRY_BACKOFF_SECONDS , WORKER_EVOLUTION_COMMIT_AUTHOR , WORKER_EVOLUTION_COMMIT_EMAIL , and WORKER_EVOLUTION_COMMIT_SUBJECT_MAX_CHARS configure how the evolution worker generates and records commit subjects (LLM model, sampling behaviour, retry policy, and subject length) and which author identity is used when creating commits, used by loreley.core.worker.commit_summary.CommitSummarizer and loreley.core.worker.repository.WorkerRepository . Worker evolution global goal : WORKER_EVOLUTION_GLOBAL_GOAL provides a single, plain\u2011language evolution objective that is shared across all jobs. When no per\u2011job goal / objective / description is present in the job payload, this value is used as the Global objective in both the planning and coding prompts so that the worker consistently optimises towards a user\u2011defined high\u2011level target. Map-Elites preprocessing : MAPELITES_PREPROCESS_* options controlling which changed code files are considered for feature extraction (limits on file count/size, allowed extensions/filenames, excluded path globs, whitespace handling, and comment stripping), used by loreley.core.map-elites.preprocess.CodePreprocessor . Map-Elites chunking : MAPELITES_CHUNK_* options controlling how preprocessed files are split into chunks (target and minimum lines per chunk, overlap, maximum chunks per file, and boundary keywords used by the chunker), used by loreley.core.map-elites.chunk.CodeChunker . Map-Elites code embedding : MAPELITES_CODE_EMBEDDING_* options configuring the embedding model, optional output dimensions, batch size, per-commit chunk budget, retry count, and exponential backoff for embedding requests, used by loreley.core.map-elites.code_embedding.CodeEmbedder . Map-Elites summary embedding : MAPELITES_SUMMARY_* and MAPELITES_SUMMARY_EMBEDDING_* options configuring the LLM summary model (name, temperature, max output tokens, source excerpt character limit, retries, and backoff) and the summary embedding model (name, dimensions, and batch size), used by loreley.core.map-elites.summarization_embedding.SummaryEmbedder . Map-Elites dimensionality reduction : MAPELITES_DIMENSION_REDUCTION_* options controlling how penultimate code/summary embeddings are normalised, the target feature dimensions, minimum sample count for fitting PCA, rolling history size, and refit cadence, used by loreley.core.map-elites.dimension_reduction.DimensionReducer . Map-Elites feature bounds and archive : MAPELITES_FEATURE_* and MAPELITES_ARCHIVE_* options defining the search space for behaviour features (lower/upper bounds) and the grid resolution and learning parameters for the underlying MAP-Elites archive, used by loreley.core.map-elites.map-elites.MapElitesManager . Map-Elites fitness and sampling : MAPELITES_FITNESS_* and MAPELITES_SAMPLER_* options that configure which metric to optimise, how to treat fitness direction/floor, and how new jobs are drawn from the archive (inspiration count, neighbour radius, fallback sampling, default priority, and whether to include metadata), used by loreley.core.map-elites.map-elites.MapElitesManager and loreley.core.map-elites.sampler.MapElitesSampler . Map-Elites experiment root commit : MAPELITES_EXPERIMENT_ROOT_COMMIT optionally pins a specific git commit hash as the logical root for a given experiment. When set, the scheduler ensures this commit is recorded in the commits table and ingested into each relevant MAP-Elites island archive before scheduling any evolution jobs, so all subsequent jobs branch from a well-defined starting point. database_dsn : computed property that returns a SQLAlchemy-compatible DSN, preferring DATABASE_URL when set and otherwise building one from the individual DB fields (with credentials URL-encoded). export_safe() : helper that returns a dict of non-sensitive configuration values suitable for logging. Access helpers \u00b6 get_settings() : cached factory that instantiates Settings , logs a concise summary of the environment and DB host using rich / loguru , and returns a singleton instance for reuse across the loreley.","title":"Configuration"},{"location":"loreley/config/#loreleyconfig","text":"Centralised configuration for the Loreley application, backed by pydantic-settings and environment variables.","title":"loreley.config"},{"location":"loreley/config/#settings","text":"Settings : BaseSettings subclass that loads core application configuration. Environment : app_name , environment , log_level , logs_base_dir . log_level controls the global Loguru log level used across long-running processes (including the scheduler, workers, and their CLI wrappers); logs_base_dir (via LOGS_BASE_DIR ) optionally overrides the base directory where long-running process logs are written, defaulting to a logs/ directory under the current working directory. See the scripts documentation under docs/script for concrete examples. OpenAI-compatible API : OPENAI_API_KEY , OPENAI_BASE_URL , OPENAI_API_SPEC configure the API key, base URL, and API surface for all OpenAI-compatible LLM and embedding calls, used by loreley.core.map-elites.code_embedding.CodeEmbedder , loreley.core.map-elites.summarization_embedding.SummaryEmbedder , and loreley.core.worker.commit_summary.CommitSummarizer . When unset, OPENAI_API_KEY / OPENAI_BASE_URL fall back to the OpenAI Python client's own environment variable defaults. OPENAI_API_SPEC accepts: \"responses\" (default): use the unified responses API ( client.responses.create ) for text generation. \"chat_completions\" : use the classic Chat Completions API ( client.chat.completions.create ) while preserving the same high-level behaviour. Database : either a raw DATABASE_URL or individual DB_* fields (scheme, host, port, username, password, database name, pool options, echo flag). Metrics : metrics_retention_days controls how long metrics are retained. Task queue : TASKS_REDIS_URL , TASKS_REDIS_HOST , TASKS_REDIS_PORT , TASKS_REDIS_DB , TASKS_REDIS_PASSWORD , TASKS_REDIS_NAMESPACE , TASKS_QUEUE_NAME , TASKS_WORKER_MAX_RETRIES , and TASKS_WORKER_TIME_LIMIT_SECONDS configure the Dramatiq Redis broker connection details, logical namespace, queue routing, retry policy, and actor time limits used by loreley.tasks.broker and loreley.tasks.workers . When TASKS_REDIS_URL is set and includes credentials, only a sanitised scheme://host:port/db form is logged, never the raw URL or password. TASKS_WORKER_TIME_LIMIT_SECONDS is interpreted in seconds and converted to Dramatiq's millisecond time_limit : values <= 0 disable the time limit (no hard cap on actor runtime), while positive values enforce a per-job wall-clock limit. Scheduler : SCHEDULER_REPO_ROOT , SCHEDULER_POLL_INTERVAL_SECONDS , SCHEDULER_MAX_UNFINISHED_JOBS , SCHEDULER_SCHEDULE_BATCH_SIZE , SCHEDULER_DISPATCH_BATCH_SIZE , and SCHEDULER_INGEST_BATCH_SIZE drive loreley.scheduler.main . These options determine which git worktree the scheduler inspects, how often it runs a reconciliation tick, how many unfinished jobs are allowed at once, how aggressively it samples new MAP-Elites jobs per tick, how many pending jobs are dispatched to Dramatiq each cycle, and how many completed jobs are ingested back into the archive per tick. Worker repository : WORKER_REPO_REMOTE_URL , WORKER_REPO_BRANCH , WORKER_REPO_WORKTREE , WORKER_REPO_GIT_BIN , WORKER_REPO_FETCH_DEPTH , WORKER_REPO_CLEAN_EXCLUDES , WORKER_REPO_JOB_BRANCH_PREFIX , WORKER_REPO_ENABLE_LFS , and WORKER_REPO_JOB_BRANCH_TTL_HOURS configure the git worktree used by worker processes (upstream remote and branch, local checkout path, git binary, shallow clone depth, clean exclusions, job branch naming, optional Git LFS support, and how long remote job branches are retained before pruning), used by loreley.core.worker.repository.WorkerRepository . Worker planning : WORKER_PLANNING_* options configuring how the external Codex CLI planner is invoked (binary path, optional profile, maximum attempts, timeout, extra environment variables, and an optional JSON schema override), used by loreley.core.worker.planning.PlanningAgent . Worker coding : WORKER_CODING_* options configuring how the external Codex-based coding agent is invoked (binary path, optional profile, maximum attempts, timeout, extra environment variables, and an optional JSON schema override), used by loreley.core.worker.coding.CodingAgent . Worker evaluator : WORKER_EVALUATOR_PLUGIN , WORKER_EVALUATOR_PYTHON_PATHS , WORKER_EVALUATOR_TIMEOUT_SECONDS , and WORKER_EVALUATOR_MAX_METRICS configure the evaluation plugin entry point, additional Python paths, subprocess timeout, and the maximum number of metrics to keep, used by loreley.core.worker.evaluator.Evaluator . Worker evolution commits : WORKER_EVOLUTION_COMMIT_MODEL , WORKER_EVOLUTION_COMMIT_TEMPERATURE , WORKER_EVOLUTION_COMMIT_MAX_OUTPUT_TOKENS , WORKER_EVOLUTION_COMMIT_MAX_RETRIES , WORKER_EVOLUTION_COMMIT_RETRY_BACKOFF_SECONDS , WORKER_EVOLUTION_COMMIT_AUTHOR , WORKER_EVOLUTION_COMMIT_EMAIL , and WORKER_EVOLUTION_COMMIT_SUBJECT_MAX_CHARS configure how the evolution worker generates and records commit subjects (LLM model, sampling behaviour, retry policy, and subject length) and which author identity is used when creating commits, used by loreley.core.worker.commit_summary.CommitSummarizer and loreley.core.worker.repository.WorkerRepository . Worker evolution global goal : WORKER_EVOLUTION_GLOBAL_GOAL provides a single, plain\u2011language evolution objective that is shared across all jobs. When no per\u2011job goal / objective / description is present in the job payload, this value is used as the Global objective in both the planning and coding prompts so that the worker consistently optimises towards a user\u2011defined high\u2011level target. Map-Elites preprocessing : MAPELITES_PREPROCESS_* options controlling which changed code files are considered for feature extraction (limits on file count/size, allowed extensions/filenames, excluded path globs, whitespace handling, and comment stripping), used by loreley.core.map-elites.preprocess.CodePreprocessor . Map-Elites chunking : MAPELITES_CHUNK_* options controlling how preprocessed files are split into chunks (target and minimum lines per chunk, overlap, maximum chunks per file, and boundary keywords used by the chunker), used by loreley.core.map-elites.chunk.CodeChunker . Map-Elites code embedding : MAPELITES_CODE_EMBEDDING_* options configuring the embedding model, optional output dimensions, batch size, per-commit chunk budget, retry count, and exponential backoff for embedding requests, used by loreley.core.map-elites.code_embedding.CodeEmbedder . Map-Elites summary embedding : MAPELITES_SUMMARY_* and MAPELITES_SUMMARY_EMBEDDING_* options configuring the LLM summary model (name, temperature, max output tokens, source excerpt character limit, retries, and backoff) and the summary embedding model (name, dimensions, and batch size), used by loreley.core.map-elites.summarization_embedding.SummaryEmbedder . Map-Elites dimensionality reduction : MAPELITES_DIMENSION_REDUCTION_* options controlling how penultimate code/summary embeddings are normalised, the target feature dimensions, minimum sample count for fitting PCA, rolling history size, and refit cadence, used by loreley.core.map-elites.dimension_reduction.DimensionReducer . Map-Elites feature bounds and archive : MAPELITES_FEATURE_* and MAPELITES_ARCHIVE_* options defining the search space for behaviour features (lower/upper bounds) and the grid resolution and learning parameters for the underlying MAP-Elites archive, used by loreley.core.map-elites.map-elites.MapElitesManager . Map-Elites fitness and sampling : MAPELITES_FITNESS_* and MAPELITES_SAMPLER_* options that configure which metric to optimise, how to treat fitness direction/floor, and how new jobs are drawn from the archive (inspiration count, neighbour radius, fallback sampling, default priority, and whether to include metadata), used by loreley.core.map-elites.map-elites.MapElitesManager and loreley.core.map-elites.sampler.MapElitesSampler . Map-Elites experiment root commit : MAPELITES_EXPERIMENT_ROOT_COMMIT optionally pins a specific git commit hash as the logical root for a given experiment. When set, the scheduler ensures this commit is recorded in the commits table and ingested into each relevant MAP-Elites island archive before scheduling any evolution jobs, so all subsequent jobs branch from a well-defined starting point. database_dsn : computed property that returns a SQLAlchemy-compatible DSN, preferring DATABASE_URL when set and otherwise building one from the individual DB fields (with credentials URL-encoded). export_safe() : helper that returns a dict of non-sensitive configuration values suitable for logging.","title":"Settings"},{"location":"loreley/config/#access-helpers","text":"get_settings() : cached factory that instantiates Settings , logs a concise summary of the environment and DB host using rich / loguru , and returns a singleton instance for reuse across the loreley.","title":"Access helpers"},{"location":"loreley/core/map-elites/chunk/","text":"loreley.core.map-elites.chunk \u00b6 Chunking utilities for turning preprocessed code into semantically meaningful segments that can be embedded and explored by map-elites. Data structures \u00b6 FileChunk : represents one chunked segment of a single file, including its path, stable chunk_id , positional range ( start_line / end_line ), text content , line_count , and aggregated change_count . ChunkedFile : groups all FileChunk instances produced from a single file, tracking the file path , overall change_count , and total_lines . PreprocessedArtifact : lightweight protocol describing the preprocessed inputs consumed by the chunker ( path , change_count , content ). Chunker \u00b6 CodeChunker : splits preprocessed files into windows tuned for downstream embedding. Uses Settings map-elites chunk configuration ( MAPELITES_CHUNK_* ) to control target and minimum lines per chunk, overlap between windows, maximum chunks per file, and keywords that hint at logical boundaries (e.g. def , class ). Iterates over each file, selecting break points on blank lines or boundary-looking lines where possible, and falls back to simple windowing when no better break point exists. Produces ChunkedFile records while displaying a rich progress spinner and logging summary statistics via loguru . Convenience API \u00b6 chunk_preprocessed_files(files, settings=None) : helper that instantiates a CodeChunker and returns the list of ChunkedFile results for a sequence of preprocessed artifacts.","title":"Chunking"},{"location":"loreley/core/map-elites/chunk/#loreleycoremap-eliteschunk","text":"Chunking utilities for turning preprocessed code into semantically meaningful segments that can be embedded and explored by map-elites.","title":"loreley.core.map-elites.chunk"},{"location":"loreley/core/map-elites/chunk/#data-structures","text":"FileChunk : represents one chunked segment of a single file, including its path, stable chunk_id , positional range ( start_line / end_line ), text content , line_count , and aggregated change_count . ChunkedFile : groups all FileChunk instances produced from a single file, tracking the file path , overall change_count , and total_lines . PreprocessedArtifact : lightweight protocol describing the preprocessed inputs consumed by the chunker ( path , change_count , content ).","title":"Data structures"},{"location":"loreley/core/map-elites/chunk/#chunker","text":"CodeChunker : splits preprocessed files into windows tuned for downstream embedding. Uses Settings map-elites chunk configuration ( MAPELITES_CHUNK_* ) to control target and minimum lines per chunk, overlap between windows, maximum chunks per file, and keywords that hint at logical boundaries (e.g. def , class ). Iterates over each file, selecting break points on blank lines or boundary-looking lines where possible, and falls back to simple windowing when no better break point exists. Produces ChunkedFile records while displaying a rich progress spinner and logging summary statistics via loguru .","title":"Chunker"},{"location":"loreley/core/map-elites/chunk/#convenience-api","text":"chunk_preprocessed_files(files, settings=None) : helper that instantiates a CodeChunker and returns the list of ChunkedFile results for a sequence of preprocessed artifacts.","title":"Convenience API"},{"location":"loreley/core/map-elites/code_embedding/","text":"loreley.core.map-elites.code_embedding \u00b6 Commit-level code embedding utilities that consume chunked code artifacts and talk to the OpenAI embeddings API as part of the Map-Elites pipeline. Data structures \u00b6 ChunkEmbedding : embedding vector derived from a single FileChunk , storing the original chunk, its numeric embedding vector , and a scalar weight used during aggregation. FileEmbedding : aggregated embedding for one ChunkedFile , including the source file, the tuple of ChunkEmbedding instances, a file-level vector , and an overall weight . CommitCodeEmbedding : commit-level representation that bundles all FileEmbedding instances, the final aggregated vector , the embedding model name, and dimensions , plus a chunk_count convenience property. Embedder \u00b6 CodeEmbedder : orchestrates calls to the OpenAI embeddings API and aggregation logic. Configured via Settings map-elites code embedding options ( MAPELITES_CODE_EMBEDDING_* ) controlling model name, optional output dimensions, batch size, maximum chunks per commit, retry count, and retry backoff. run(chunked_files) filters out empty inputs, flattens chunks into a payload, embeds them in batches with a rich progress spinner, and turns raw vectors into ChunkEmbedding , FileEmbedding , and CommitCodeEmbedding objects using weighted averaging. Logs detailed progress and warnings with loguru , including mismatched response sizes, missing owners for chunks, and empty aggregation results. Convenience API \u00b6 embed_chunked_files(chunked_files, settings=None, client=None) : helper that constructs a CodeEmbedder and returns a CommitCodeEmbedding for the supplied chunked files, or None if there is nothing worth embedding.","title":"Code embeddings"},{"location":"loreley/core/map-elites/code_embedding/#loreleycoremap-elitescode_embedding","text":"Commit-level code embedding utilities that consume chunked code artifacts and talk to the OpenAI embeddings API as part of the Map-Elites pipeline.","title":"loreley.core.map-elites.code_embedding"},{"location":"loreley/core/map-elites/code_embedding/#data-structures","text":"ChunkEmbedding : embedding vector derived from a single FileChunk , storing the original chunk, its numeric embedding vector , and a scalar weight used during aggregation. FileEmbedding : aggregated embedding for one ChunkedFile , including the source file, the tuple of ChunkEmbedding instances, a file-level vector , and an overall weight . CommitCodeEmbedding : commit-level representation that bundles all FileEmbedding instances, the final aggregated vector , the embedding model name, and dimensions , plus a chunk_count convenience property.","title":"Data structures"},{"location":"loreley/core/map-elites/code_embedding/#embedder","text":"CodeEmbedder : orchestrates calls to the OpenAI embeddings API and aggregation logic. Configured via Settings map-elites code embedding options ( MAPELITES_CODE_EMBEDDING_* ) controlling model name, optional output dimensions, batch size, maximum chunks per commit, retry count, and retry backoff. run(chunked_files) filters out empty inputs, flattens chunks into a payload, embeds them in batches with a rich progress spinner, and turns raw vectors into ChunkEmbedding , FileEmbedding , and CommitCodeEmbedding objects using weighted averaging. Logs detailed progress and warnings with loguru , including mismatched response sizes, missing owners for chunks, and empty aggregation results.","title":"Embedder"},{"location":"loreley/core/map-elites/code_embedding/#convenience-api","text":"embed_chunked_files(chunked_files, settings=None, client=None) : helper that constructs a CodeEmbedder and returns a CommitCodeEmbedding for the supplied chunked files, or None if there is nothing worth embedding.","title":"Convenience API"},{"location":"loreley/core/map-elites/dimension_reduction/","text":"loreley.core.map-elites.dimension_reduction \u00b6 PCA-based dimensionality reduction of combined code and summary embeddings before they are fed into the MAP-Elites archive. Data structures \u00b6 PenultimateEmbedding : concatenated code and summary embedding for a single commit, tracking the original models and dimension counts as well as the combined vector (optionally L2-normalised). PCAProjection : serialisable wrapper around a fitted sklearn.decomposition.PCA model, capturing the mean vector, components, explained variance ratio, and sample metadata, plus a transform() helper that projects new vectors. FinalEmbedding : low-dimensional vector that sits on the MAP-Elites grid for a commit, along with the originating PenultimateEmbedding and optional PCAProjection used. Reducer \u00b6 DimensionReducer : maintains rolling history of penultimate embeddings and an optional PCA projection to keep the behaviour space stable. Configured via Settings map-elites dimensionality options ( MAPELITES_DIMENSION_REDUCTION_* ) controlling target dimensions, minimum sample count, history size, refit interval, and whether to normalise penultimate vectors. build_penultimate(...) concatenates code and summary embeddings, normalises them when enabled, and returns a PenultimateEmbedding or None when no embeddings are available. reduce(penultimate, refit=None) records the embedding in history, (re)fits PCA when needed, and projects into the target space, returning a FinalEmbedding and logging issues via loguru when projection cannot be computed. Convenience API \u00b6 reduce_commit_embeddings(...) : one-shot helper that constructs a DimensionReducer , builds the penultimate embedding from a commit's code and summary embeddings, and returns the FinalEmbedding together with the updated history and projection so callers can persist state.","title":"Dimensionality reduction"},{"location":"loreley/core/map-elites/dimension_reduction/#loreleycoremap-elitesdimension_reduction","text":"PCA-based dimensionality reduction of combined code and summary embeddings before they are fed into the MAP-Elites archive.","title":"loreley.core.map-elites.dimension_reduction"},{"location":"loreley/core/map-elites/dimension_reduction/#data-structures","text":"PenultimateEmbedding : concatenated code and summary embedding for a single commit, tracking the original models and dimension counts as well as the combined vector (optionally L2-normalised). PCAProjection : serialisable wrapper around a fitted sklearn.decomposition.PCA model, capturing the mean vector, components, explained variance ratio, and sample metadata, plus a transform() helper that projects new vectors. FinalEmbedding : low-dimensional vector that sits on the MAP-Elites grid for a commit, along with the originating PenultimateEmbedding and optional PCAProjection used.","title":"Data structures"},{"location":"loreley/core/map-elites/dimension_reduction/#reducer","text":"DimensionReducer : maintains rolling history of penultimate embeddings and an optional PCA projection to keep the behaviour space stable. Configured via Settings map-elites dimensionality options ( MAPELITES_DIMENSION_REDUCTION_* ) controlling target dimensions, minimum sample count, history size, refit interval, and whether to normalise penultimate vectors. build_penultimate(...) concatenates code and summary embeddings, normalises them when enabled, and returns a PenultimateEmbedding or None when no embeddings are available. reduce(penultimate, refit=None) records the embedding in history, (re)fits PCA when needed, and projects into the target space, returning a FinalEmbedding and logging issues via loguru when projection cannot be computed.","title":"Reducer"},{"location":"loreley/core/map-elites/dimension_reduction/#convenience-api","text":"reduce_commit_embeddings(...) : one-shot helper that constructs a DimensionReducer , builds the penultimate embedding from a commit's code and summary embeddings, and returns the FinalEmbedding together with the updated history and projection so callers can persist state.","title":"Convenience API"},{"location":"loreley/core/map-elites/map-elites/","text":"loreley.core.map-elites.map-elites \u00b6 High-level manager that runs the MAP-Elites pipeline on git commits and maintains per-island archives backed by the database. Data structures \u00b6 CommitEmbeddingArtifacts : immutable container bundling preprocessed files, chunked files, and the commit-level code and summary embeddings plus the final low-dimensional embedding produced for a commit. MapElitesRecord : snapshot of a single elite stored in the archive, including commit hash, island, cell index, fitness, behaviour measures, solution vector, metadata, and timestamp. MapElitesInsertionResult : describes the outcome of attempting to insert a commit into the archive, exposing a status flag, fitness delta, optional MapElitesRecord , any intermediate artifacts, and an optional human-readable message. IslandState : internal mutable state attached to each island, holding the GridArchive , behaviour bounds, PCA history/projection, and mappings between commits and archive cell indices. Manager \u00b6 MapElitesManager : orchestrates preprocessing, chunking, embedding, dimensionality reduction, archive updates, and snapshot persistence. Configured via Settings map-elites options: preprocessing, chunking, code/summary embeddings, dimensionality reduction, feature bounds, archive grid, fitness metric, and default island identifiers. Accepts an optional experiment_id at construction time; when provided, all persisted MapElitesState rows are scoped by (experiment_id, island_id) , allowing multiple experiments to maintain independent archives even when they share island identifiers. When omitted, archive state is kept purely in-memory and snapshots are not written. ingest(commit_hash, changed_files, metrics, island_id, repo_root, treeish, metadata, fitness_override) runs the full pipeline for a commit: loads and preprocesses changed files, chunks them, derives code and summary embeddings, reduces them to the behaviour space, resolves a scalar fitness from metrics or overrides, and attempts to insert the result into the island's GridArchive . Tracks per-island PCA history and projection so that new embeddings are consistent with previous ones, logging detailed progress and warnings with loguru . Delegates snapshot serialisation and persistence to loreley.core.map_elites.snapshot , which exposes pure helpers for encoding/decoding archive state plus pluggable storage backends (database or no-op). Query helpers \u00b6 get_records(island_id=None) : returns all current MapElitesRecord entries for an island, rebuilding them from the underlying archive. sample_records(island_id=None, count=1) : randomly samples up to count elites from an island's archive for downstream planning or analysis. clear_island(island_id=None) : clears an island's archive and associated PCA history/projection state, removing all stored elites and mappings for that island. describe_island(island_id=None) : returns a small dict of observability stats for an island (ID, occupied cell count, total cells, QD score, and best fitness).","title":"Overview"},{"location":"loreley/core/map-elites/map-elites/#loreleycoremap-elitesmap-elites","text":"High-level manager that runs the MAP-Elites pipeline on git commits and maintains per-island archives backed by the database.","title":"loreley.core.map-elites.map-elites"},{"location":"loreley/core/map-elites/map-elites/#data-structures","text":"CommitEmbeddingArtifacts : immutable container bundling preprocessed files, chunked files, and the commit-level code and summary embeddings plus the final low-dimensional embedding produced for a commit. MapElitesRecord : snapshot of a single elite stored in the archive, including commit hash, island, cell index, fitness, behaviour measures, solution vector, metadata, and timestamp. MapElitesInsertionResult : describes the outcome of attempting to insert a commit into the archive, exposing a status flag, fitness delta, optional MapElitesRecord , any intermediate artifacts, and an optional human-readable message. IslandState : internal mutable state attached to each island, holding the GridArchive , behaviour bounds, PCA history/projection, and mappings between commits and archive cell indices.","title":"Data structures"},{"location":"loreley/core/map-elites/map-elites/#manager","text":"MapElitesManager : orchestrates preprocessing, chunking, embedding, dimensionality reduction, archive updates, and snapshot persistence. Configured via Settings map-elites options: preprocessing, chunking, code/summary embeddings, dimensionality reduction, feature bounds, archive grid, fitness metric, and default island identifiers. Accepts an optional experiment_id at construction time; when provided, all persisted MapElitesState rows are scoped by (experiment_id, island_id) , allowing multiple experiments to maintain independent archives even when they share island identifiers. When omitted, archive state is kept purely in-memory and snapshots are not written. ingest(commit_hash, changed_files, metrics, island_id, repo_root, treeish, metadata, fitness_override) runs the full pipeline for a commit: loads and preprocesses changed files, chunks them, derives code and summary embeddings, reduces them to the behaviour space, resolves a scalar fitness from metrics or overrides, and attempts to insert the result into the island's GridArchive . Tracks per-island PCA history and projection so that new embeddings are consistent with previous ones, logging detailed progress and warnings with loguru . Delegates snapshot serialisation and persistence to loreley.core.map_elites.snapshot , which exposes pure helpers for encoding/decoding archive state plus pluggable storage backends (database or no-op).","title":"Manager"},{"location":"loreley/core/map-elites/map-elites/#query-helpers","text":"get_records(island_id=None) : returns all current MapElitesRecord entries for an island, rebuilding them from the underlying archive. sample_records(island_id=None, count=1) : randomly samples up to count elites from an island's archive for downstream planning or analysis. clear_island(island_id=None) : clears an island's archive and associated PCA history/projection state, removing all stored elites and mappings for that island. describe_island(island_id=None) : returns a small dict of observability stats for an island (ID, occupied cell count, total cells, QD score, and best fitness).","title":"Query helpers"},{"location":"loreley/core/map-elites/preprocess/","text":"loreley.core.map-elites.preprocess \u00b6 Preprocessing utilities for turning raw commit diffs into cleaned code snippets suitable for embedding and feature extraction. Data structures \u00b6 ChangedFile : lightweight description of a file touched by a commit (path, approximate change_count , optional inline content override). PreprocessedFile : output record capturing the repository-relative path , accumulated change_count , and cleaned textual content after preprocessing. Preprocessor \u00b6 CodePreprocessor : filters and normalises changed files before embedding. Uses Settings map-elites preprocessing options to enforce maximum file count/size, allowed extensions/filenames, and excluded glob patterns. Loads file contents either from the working tree or a specific treeish via GitPython, applies comment stripping, tab-to-spaces conversion, blank-line collapse, and basic normalisation. Exposes run(changed_files) which returns a list of PreprocessedFile objects ordered by change_count . Reports progress via a rich Progress spinner and logs structured messages through loguru . Convenience API \u00b6 preprocess_changed_files(changed_files, repo_root=None, settings=None, treeish=None, repo=None) : functional helper that instantiates a CodePreprocessor and runs it over the provided list of changed files.","title":"Preprocessing"},{"location":"loreley/core/map-elites/preprocess/#loreleycoremap-elitespreprocess","text":"Preprocessing utilities for turning raw commit diffs into cleaned code snippets suitable for embedding and feature extraction.","title":"loreley.core.map-elites.preprocess"},{"location":"loreley/core/map-elites/preprocess/#data-structures","text":"ChangedFile : lightweight description of a file touched by a commit (path, approximate change_count , optional inline content override). PreprocessedFile : output record capturing the repository-relative path , accumulated change_count , and cleaned textual content after preprocessing.","title":"Data structures"},{"location":"loreley/core/map-elites/preprocess/#preprocessor","text":"CodePreprocessor : filters and normalises changed files before embedding. Uses Settings map-elites preprocessing options to enforce maximum file count/size, allowed extensions/filenames, and excluded glob patterns. Loads file contents either from the working tree or a specific treeish via GitPython, applies comment stripping, tab-to-spaces conversion, blank-line collapse, and basic normalisation. Exposes run(changed_files) which returns a list of PreprocessedFile objects ordered by change_count . Reports progress via a rich Progress spinner and logs structured messages through loguru .","title":"Preprocessor"},{"location":"loreley/core/map-elites/preprocess/#convenience-api","text":"preprocess_changed_files(changed_files, repo_root=None, settings=None, treeish=None, repo=None) : functional helper that instantiates a CodePreprocessor and runs it over the provided list of changed files.","title":"Convenience API"},{"location":"loreley/core/map-elites/sampler/","text":"loreley.core.map-elites.sampler \u00b6 Sampler that turns MAP-Elites archive records into concrete EvolutionJob rows for further evolution. Protocols \u00b6 SupportsMapElitesRecord : protocol describing the record interface consumed by the sampler (commit hash, cell index, fitness, measures, solution, metadata, timestamp). SupportsMapElitesManager : protocol that exposes a get_records(island_id) method, allowing the sampler to be used against MapElitesManager or any compatible implementation. Sampling \u00b6 ScheduledSamplerJob : immutable descriptor for a newly scheduled job, exposing the EvolutionJob ID, island, base record, inspiration records, and the payload sent to the scheduler. MapElitesSampler : coordinates archive sampling and job persistence. Configured via Settings map-elites options for dimensionality, feature bounds, and sampler behaviour ( MAPELITES_DIMENSION_REDUCTION_* , MAPELITES_FEATURE_* , MAPELITES_ARCHIVE_* , and MAPELITES_SAMPLER_* ). schedule_job(island_id=None, payload_overrides=None, priority=None, experiment_id=None) pulls records from the manager, chooses a base record, selects neighbours as inspirations using a configurable neighbourhood radius with optional fallback sampling, builds a rich JSON payload (including grid shape and feature bounds, sampling statistics, and normalised base/inspiration records), and persists a new EvolutionJob via session_scope . When experiment_id is provided, it is coerced to a UUID and stored on the EvolutionJob row so that downstream components (scheduler ingestion, worker, MAP-Elites manager) can reliably group jobs and commits by experiment. When omitted, jobs are still scheduled and experiment_id remains NULL , which is useful for legacy or single-experiment deployments. Uses loguru for structured logging and rich to print a concise confirmation when a job is enqueued. Neighbourhood selection \u00b6 _select_inspirations(...) : internal helper that walks outward from the base cell over the discretised behaviour grid, gathering nearby elites up to the requested inspiration count and recording selection statistics. _neighbor_indices(center_index, radius) : converts a flat cell index and radius into neighbouring cell indices using numpy's unravel_index / ravel_multi_index , respecting grid bounds.","title":"Sampler"},{"location":"loreley/core/map-elites/sampler/#loreleycoremap-elitessampler","text":"Sampler that turns MAP-Elites archive records into concrete EvolutionJob rows for further evolution.","title":"loreley.core.map-elites.sampler"},{"location":"loreley/core/map-elites/sampler/#protocols","text":"SupportsMapElitesRecord : protocol describing the record interface consumed by the sampler (commit hash, cell index, fitness, measures, solution, metadata, timestamp). SupportsMapElitesManager : protocol that exposes a get_records(island_id) method, allowing the sampler to be used against MapElitesManager or any compatible implementation.","title":"Protocols"},{"location":"loreley/core/map-elites/sampler/#sampling","text":"ScheduledSamplerJob : immutable descriptor for a newly scheduled job, exposing the EvolutionJob ID, island, base record, inspiration records, and the payload sent to the scheduler. MapElitesSampler : coordinates archive sampling and job persistence. Configured via Settings map-elites options for dimensionality, feature bounds, and sampler behaviour ( MAPELITES_DIMENSION_REDUCTION_* , MAPELITES_FEATURE_* , MAPELITES_ARCHIVE_* , and MAPELITES_SAMPLER_* ). schedule_job(island_id=None, payload_overrides=None, priority=None, experiment_id=None) pulls records from the manager, chooses a base record, selects neighbours as inspirations using a configurable neighbourhood radius with optional fallback sampling, builds a rich JSON payload (including grid shape and feature bounds, sampling statistics, and normalised base/inspiration records), and persists a new EvolutionJob via session_scope . When experiment_id is provided, it is coerced to a UUID and stored on the EvolutionJob row so that downstream components (scheduler ingestion, worker, MAP-Elites manager) can reliably group jobs and commits by experiment. When omitted, jobs are still scheduled and experiment_id remains NULL , which is useful for legacy or single-experiment deployments. Uses loguru for structured logging and rich to print a concise confirmation when a job is enqueued.","title":"Sampling"},{"location":"loreley/core/map-elites/sampler/#neighbourhood-selection","text":"_select_inspirations(...) : internal helper that walks outward from the base cell over the discretised behaviour grid, gathering nearby elites up to the requested inspiration count and recording selection statistics. _neighbor_indices(center_index, radius) : converts a flat cell index and radius into neighbouring cell indices using numpy's unravel_index / ravel_multi_index , respecting grid bounds.","title":"Neighbourhood selection"},{"location":"loreley/core/map-elites/snapshot/","text":"loreley.core.map-elites.snapshot \u00b6 Helpers and backends for serialising and persisting MAP-Elites archive snapshots. Responsibilities \u00b6 Serialisation helpers : Convert per-island PCA history ( PenultimateEmbedding ), PCAProjection , and GridArchive contents into JSON-compatible snapshot payloads. Restore bounds, history, projection, archive entries, and commit-to-cell mappings from previously stored snapshots. Backends : Define a small SnapshotBackend interface with load(island_id) and save(island_id, snapshot) methods. Provide a NullSnapshotBackend that disables persistence and simply returns None on load . Provide a DatabaseSnapshotBackend that stores snapshots in the map_elites_states table via the MapElitesState ORM model. Integration with MapElitesManager \u00b6 MapElitesManager constructs a backend through build_snapshot_backend(experiment_id) : When experiment_id is None , a NullSnapshotBackend is returned and all snapshot operations become in-memory only. When experiment_id is set, a DatabaseSnapshotBackend is used and snapshots are scoped by (experiment_id, island_id) . The manager decides when to persist: On island initialisation it calls backend.load(island_id) and, if a payload exists, applies it with apply_snapshot(...) . After ingestion and clear_island() , it calls build_snapshot(island_id, state) and backend.save(island_id, snapshot) to keep durable state up to date.","title":"Snapshots"},{"location":"loreley/core/map-elites/snapshot/#loreleycoremap-elitessnapshot","text":"Helpers and backends for serialising and persisting MAP-Elites archive snapshots.","title":"loreley.core.map-elites.snapshot"},{"location":"loreley/core/map-elites/snapshot/#responsibilities","text":"Serialisation helpers : Convert per-island PCA history ( PenultimateEmbedding ), PCAProjection , and GridArchive contents into JSON-compatible snapshot payloads. Restore bounds, history, projection, archive entries, and commit-to-cell mappings from previously stored snapshots. Backends : Define a small SnapshotBackend interface with load(island_id) and save(island_id, snapshot) methods. Provide a NullSnapshotBackend that disables persistence and simply returns None on load . Provide a DatabaseSnapshotBackend that stores snapshots in the map_elites_states table via the MapElitesState ORM model.","title":"Responsibilities"},{"location":"loreley/core/map-elites/snapshot/#integration-with-mapelitesmanager","text":"MapElitesManager constructs a backend through build_snapshot_backend(experiment_id) : When experiment_id is None , a NullSnapshotBackend is returned and all snapshot operations become in-memory only. When experiment_id is set, a DatabaseSnapshotBackend is used and snapshots are scoped by (experiment_id, island_id) . The manager decides when to persist: On island initialisation it calls backend.load(island_id) and, if a payload exists, applies it with apply_snapshot(...) . After ingestion and clear_island() , it calls build_snapshot(island_id, state) and backend.save(island_id, snapshot) to keep durable state up to date.","title":"Integration with MapElitesManager"},{"location":"loreley/core/map-elites/summarization_embedding/","text":"loreley.core.map-elites.summarization_embedding \u00b6 Summary-level embedding utilities that turn preprocessed code into structured natural-language summaries and a commit-level embedding vector for the Map-Elites pipeline. Data structures \u00b6 FileSummary : immutable record describing one file-level summary (repository-relative path , approximate change_count , and the generated markdown summary text). The path is normalised to a pathlib.Path . SummaryEmbedding : embedding derived from a FileSummary , containing the original file_summary , its numeric vector , and a scalar weight used during aggregation. CommitSummaryEmbedding : commit-level representation bundling all SummaryEmbedding instances, the final aggregated vector , the summary_model and embedding_model names, dimensions , and a file_count convenience property. Embedder \u00b6 SummaryEmbedder : orchestrates summarisation of preprocessed files and embedding of the resulting summaries. Configured via Settings Map-Elites summary options ( MAPELITES_SUMMARY_* ) controlling the LLM model, temperature, maximum output tokens, source excerpt character limit, and retry/backoff behaviour, and summary embedding options ( MAPELITES_SUMMARY_EMBEDDING_* ) controlling the embedding model, optional output dimensions, and batch size. Uses the global OpenAI API configuration from loreley.config.Settings ( OPENAI_API_KEY , OPENAI_BASE_URL , OPENAI_API_SPEC ) when constructing the shared OpenAI client. OPENAI_API_SPEC selects whether summarisation uses: the unified Responses API ( client.responses.create , default), passing _SUMMARY_INSTRUCTIONS as instructions and the file prompt as input ; or the classic Chat Completions API ( client.chat.completions.create ), mapping _SUMMARY_INSTRUCTIONS to a system message and the file prompt to a user message while keeping temperature and token limits aligned. run(files) skips empty input, calls _summarize_files to build FileSummary objects with a rich progress spinner and loguru debug/warning logs (using either Responses or Chat Completions under the hood), then calls _embed_summaries to batch summaries through the OpenAI embeddings API, weighting them by change count or summary length. Aggregates per-file vectors into a single commit-level vector using _weighted_average , after sorting entries by path for stable output; returns a CommitSummaryEmbedding or None if no usable summaries or embeddings were produced. Convenience API \u00b6 summarize_preprocessed_files(files, settings=None, client=None) : helper that constructs a SummaryEmbedder (optionally injecting custom settings or OpenAI client) and returns a CommitSummaryEmbedding for the supplied PreprocessedFile sequence, or None when there is nothing to summarise or embed.","title":"Summary embeddings"},{"location":"loreley/core/map-elites/summarization_embedding/#loreleycoremap-elitessummarization_embedding","text":"Summary-level embedding utilities that turn preprocessed code into structured natural-language summaries and a commit-level embedding vector for the Map-Elites pipeline.","title":"loreley.core.map-elites.summarization_embedding"},{"location":"loreley/core/map-elites/summarization_embedding/#data-structures","text":"FileSummary : immutable record describing one file-level summary (repository-relative path , approximate change_count , and the generated markdown summary text). The path is normalised to a pathlib.Path . SummaryEmbedding : embedding derived from a FileSummary , containing the original file_summary , its numeric vector , and a scalar weight used during aggregation. CommitSummaryEmbedding : commit-level representation bundling all SummaryEmbedding instances, the final aggregated vector , the summary_model and embedding_model names, dimensions , and a file_count convenience property.","title":"Data structures"},{"location":"loreley/core/map-elites/summarization_embedding/#embedder","text":"SummaryEmbedder : orchestrates summarisation of preprocessed files and embedding of the resulting summaries. Configured via Settings Map-Elites summary options ( MAPELITES_SUMMARY_* ) controlling the LLM model, temperature, maximum output tokens, source excerpt character limit, and retry/backoff behaviour, and summary embedding options ( MAPELITES_SUMMARY_EMBEDDING_* ) controlling the embedding model, optional output dimensions, and batch size. Uses the global OpenAI API configuration from loreley.config.Settings ( OPENAI_API_KEY , OPENAI_BASE_URL , OPENAI_API_SPEC ) when constructing the shared OpenAI client. OPENAI_API_SPEC selects whether summarisation uses: the unified Responses API ( client.responses.create , default), passing _SUMMARY_INSTRUCTIONS as instructions and the file prompt as input ; or the classic Chat Completions API ( client.chat.completions.create ), mapping _SUMMARY_INSTRUCTIONS to a system message and the file prompt to a user message while keeping temperature and token limits aligned. run(files) skips empty input, calls _summarize_files to build FileSummary objects with a rich progress spinner and loguru debug/warning logs (using either Responses or Chat Completions under the hood), then calls _embed_summaries to batch summaries through the OpenAI embeddings API, weighting them by change count or summary length. Aggregates per-file vectors into a single commit-level vector using _weighted_average , after sorting entries by path for stable output; returns a CommitSummaryEmbedding or None if no usable summaries or embeddings were produced.","title":"Embedder"},{"location":"loreley/core/map-elites/summarization_embedding/#convenience-api","text":"summarize_preprocessed_files(files, settings=None, client=None) : helper that constructs a SummaryEmbedder (optionally injecting custom settings or OpenAI client) and returns a CommitSummaryEmbedding for the supplied PreprocessedFile sequence, or None when there is nothing to summarise or embed.","title":"Convenience API"},{"location":"loreley/core/worker/coding/","text":"loreley.core.worker.coding \u00b6 Execution engine for Loreley's autonomous worker, responsible for driving the Codex-based coding agent that applies a planning agent's plan to a real git worktree. Domain types \u00b6 CodingError : custom runtime error raised when the coding agent cannot successfully execute the plan (invalid schema, Codex failures, bad working directory, timeouts, etc.). StepExecutionStatus : string Enum describing how each plan step was handled ( COMPLETED , PARTIAL , or SKIPPED ). CodingStepReport : dataclass capturing the outcome of a single step ( step_id , status , human-readable summary , and optional files / commands touched by that step). CodingPlanExecution : aggregate result for the whole run, including the overall implementation_summary , optional commit_message , tuple of step_results , tests_executed , tests_recommended , follow_up_items , and notes . CodingAgentRequest : input payload given to the coding agent ( goal , plan from PlanningPlan , base_commit , optional constraints , acceptance_criteria , iteration_hint , and additional_notes ); the goal is the same global evolution objective that the planning agent sees, resolved by the evolution worker from either explicit job payload fields or Settings.worker_evolution_global_goal . All sequence fields are normalised to tuples in __post_init__ . CodingAgentResponse : envelope returned from the agent combining the structured execution , raw backend raw_output , rendered prompt , executed backend command , captured stderr , number of attempts , and total duration_seconds . JSON schema and validation \u00b6 CODING_OUTPUT_SCHEMA : JSON schema describing the expected coding agent output (top-level implementation_summary , optional commit_message , array of step_results , plus optional tests_executed , tests_recommended , follow_up_items , and notes ), used to validate the backend response. _StepResultModel / _CodingOutputModel : internal frozen pydantic models that validate the JSON payload against CODING_OUTPUT_SCHEMA and provide a typed bridge into the domain dataclasses. loreley.core.worker.agent_backend : shared backend abstractions ( AgentBackend , StructuredAgentTask , AgentInvocation ) plus the default CodexCliBackend implementation used by the coding agent. Coding agent \u00b6 CodingAgent : high-level orchestrator that turns a CodingAgentRequest and PlanningPlan into a sequence of edits via a configurable backend. Instantiated with a Settings object and an optional AgentBackend implementation. When no backend is provided, it uses CodexCliBackend configured via WORKER_CODING_CODEX_BIN , WORKER_CODING_CODEX_PROFILE , WORKER_CODING_MAX_ATTEMPTS , WORKER_CODING_TIMEOUT_SECONDS , WORKER_CODING_EXTRA_ENV , and WORKER_CODING_SCHEMA_PATH . You can override the default by setting WORKER_CODING_BACKEND to a dotted Python path ( module:attr or module.attr ) that resolves to either an AgentBackend instance, a class implementing the AgentBackend protocol (constructed with no arguments), or a factory callable that returns such an instance. implement(request, *, working_dir) : resolves the git worktree path, renders a detailed natural-language prompt describing the goal, constraints, acceptance criteria, plan steps, focus metrics, guardrails, validation bullets, risks, additional notes, handoff notes, and any fallback plan, builds a StructuredAgentTask that references CODING_OUTPUT_SCHEMA , and asks the backend to execute it (for CodexCliBackend this means codex exec --full-auto ). Retries the backend invocation up to max_attempts times when the process fails, the JSON is invalid, or schema validation errors occur, logging warnings via loguru and showing concise progress output with rich . On success, parses the JSON into _CodingOutputModel , converts it into a CodingPlanExecution , and returns a CodingAgentResponse ; on repeated failure or timeout, raises CodingError with a descriptive message. Merges any configured extra environment variables into the backend subprocess environment and enforces bounded prompt and log sizes via _truncate . Exceptions and helpers \u00b6 _parse_output() , _log_invalid_output() , _to_domain() , _format_plan_step() , _format_bullets() , and _truncate() : utilities that format human-readable prompt sections, enforce length limits, convert the raw JSON model into domain types, and provide rich logging when backend output cannot be validated.","title":"Coding"},{"location":"loreley/core/worker/coding/#loreleycoreworkercoding","text":"Execution engine for Loreley's autonomous worker, responsible for driving the Codex-based coding agent that applies a planning agent's plan to a real git worktree.","title":"loreley.core.worker.coding"},{"location":"loreley/core/worker/coding/#domain-types","text":"CodingError : custom runtime error raised when the coding agent cannot successfully execute the plan (invalid schema, Codex failures, bad working directory, timeouts, etc.). StepExecutionStatus : string Enum describing how each plan step was handled ( COMPLETED , PARTIAL , or SKIPPED ). CodingStepReport : dataclass capturing the outcome of a single step ( step_id , status , human-readable summary , and optional files / commands touched by that step). CodingPlanExecution : aggregate result for the whole run, including the overall implementation_summary , optional commit_message , tuple of step_results , tests_executed , tests_recommended , follow_up_items , and notes . CodingAgentRequest : input payload given to the coding agent ( goal , plan from PlanningPlan , base_commit , optional constraints , acceptance_criteria , iteration_hint , and additional_notes ); the goal is the same global evolution objective that the planning agent sees, resolved by the evolution worker from either explicit job payload fields or Settings.worker_evolution_global_goal . All sequence fields are normalised to tuples in __post_init__ . CodingAgentResponse : envelope returned from the agent combining the structured execution , raw backend raw_output , rendered prompt , executed backend command , captured stderr , number of attempts , and total duration_seconds .","title":"Domain types"},{"location":"loreley/core/worker/coding/#json-schema-and-validation","text":"CODING_OUTPUT_SCHEMA : JSON schema describing the expected coding agent output (top-level implementation_summary , optional commit_message , array of step_results , plus optional tests_executed , tests_recommended , follow_up_items , and notes ), used to validate the backend response. _StepResultModel / _CodingOutputModel : internal frozen pydantic models that validate the JSON payload against CODING_OUTPUT_SCHEMA and provide a typed bridge into the domain dataclasses. loreley.core.worker.agent_backend : shared backend abstractions ( AgentBackend , StructuredAgentTask , AgentInvocation ) plus the default CodexCliBackend implementation used by the coding agent.","title":"JSON schema and validation"},{"location":"loreley/core/worker/coding/#coding-agent","text":"CodingAgent : high-level orchestrator that turns a CodingAgentRequest and PlanningPlan into a sequence of edits via a configurable backend. Instantiated with a Settings object and an optional AgentBackend implementation. When no backend is provided, it uses CodexCliBackend configured via WORKER_CODING_CODEX_BIN , WORKER_CODING_CODEX_PROFILE , WORKER_CODING_MAX_ATTEMPTS , WORKER_CODING_TIMEOUT_SECONDS , WORKER_CODING_EXTRA_ENV , and WORKER_CODING_SCHEMA_PATH . You can override the default by setting WORKER_CODING_BACKEND to a dotted Python path ( module:attr or module.attr ) that resolves to either an AgentBackend instance, a class implementing the AgentBackend protocol (constructed with no arguments), or a factory callable that returns such an instance. implement(request, *, working_dir) : resolves the git worktree path, renders a detailed natural-language prompt describing the goal, constraints, acceptance criteria, plan steps, focus metrics, guardrails, validation bullets, risks, additional notes, handoff notes, and any fallback plan, builds a StructuredAgentTask that references CODING_OUTPUT_SCHEMA , and asks the backend to execute it (for CodexCliBackend this means codex exec --full-auto ). Retries the backend invocation up to max_attempts times when the process fails, the JSON is invalid, or schema validation errors occur, logging warnings via loguru and showing concise progress output with rich . On success, parses the JSON into _CodingOutputModel , converts it into a CodingPlanExecution , and returns a CodingAgentResponse ; on repeated failure or timeout, raises CodingError with a descriptive message. Merges any configured extra environment variables into the backend subprocess environment and enforces bounded prompt and log sizes via _truncate .","title":"Coding agent"},{"location":"loreley/core/worker/coding/#exceptions-and-helpers","text":"_parse_output() , _log_invalid_output() , _to_domain() , _format_plan_step() , _format_bullets() , and _truncate() : utilities that format human-readable prompt sections, enforce length limits, convert the raw JSON model into domain types, and provide rich logging when backend output cannot be validated.","title":"Exceptions and helpers"},{"location":"loreley/core/worker/commit_summary/","text":"loreley.core.worker.commit_summary \u00b6 Commit summarisation utilities used by the evolution worker to derive concise git commit subjects from planning and coding context. Domain types and errors \u00b6 CommitSummaryError : runtime error raised when the summariser cannot produce a subject line (for example, due to API errors, empty model output, or repeated failures across retries). CommitSummarizer \u00b6 CommitSummarizer : LLM-backed helper responsible for generating short, imperative git subjects. Configured via loreley.config.Settings worker evolution commit options: WORKER_EVOLUTION_COMMIT_MODEL : model identifier used with the OpenAI client. WORKER_EVOLUTION_COMMIT_TEMPERATURE : sampling temperature. WORKER_EVOLUTION_COMMIT_MAX_OUTPUT_TOKENS : upper bound on model output tokens (clamped to at least 32). WORKER_EVOLUTION_COMMIT_MAX_RETRIES : maximum number of retry attempts on failure (minimum 1). WORKER_EVOLUTION_COMMIT_RETRY_BACKOFF_SECONDS : linear backoff applied between retries. WORKER_EVOLUTION_COMMIT_SUBJECT_MAX_CHARS : hard character limit for subject lines (minimum 32). Lazily initialises an OpenAI client and applies a local _truncate_limit when building prompts to keep context sizes reasonable. Respects the global OpenAI API surface setting OPENAI_API_SPEC : \"responses\" (default) uses the unified Responses API ( client.responses.create ) with an instructions string and full prompt as input . \"chat_completions\" uses the Chat Completions API ( client.chat.completions.create ), mapping the same instruction text to a system message and the prompt to a user message, extracting the assistant's reply from the first choice. Subject generation \u00b6 generate(job, plan, coding) : Constructs a detailed prompt that includes: The global job goal . Plan summary , rationale , focus metrics, guardrails, constraints, acceptance criteria, and notes. Coding execution summary, per-step outcomes (step IDs, statuses, summaries), and the list of tests executed. The coding agent's own suggested commit_message as a fallback hint. Calls the OpenAI responses API with the configured model, temperature, and token limit, plus an instructions string that asks for a single imperative git subject no longer than 72 characters. Retries up to _max_retries times on OpenAIError or CommitSummaryError , waiting for retry_backoff * attempt seconds between attempts, regardless of whether Responses or Chat Completions is selected. On success, strips and normalises whitespace, then enforces the subject character limit via _normalise_subject() , logging the attempt count via loguru . On exhausting retries, raises CommitSummaryError with a descriptive message including the number of attempts. Normalisation helpers \u00b6 coerce_subject(text, *, default) : Used as a safer fallback when LLM-based summarisation fails or when an existing subject must be clamped to a valid git style. Collapses whitespace, falls back to a provided default when text is empty, and applies _normalise_subject() to respect the configured character limit. _normalise_subject(text) : Collapses consecutive whitespace to single spaces and trims leading/trailing spaces. If the cleaned subject exceeds _subject_limit , truncates it and appends an ellipsis to signal truncation. _build_prompt(job, plan, coding) / _truncate(text, limit=None) : Internal helpers which format the rich multi-section prompt while bounding long summaries and step descriptions, ensuring that the most relevant context is preserved for the LLM.","title":"Commit summaries"},{"location":"loreley/core/worker/commit_summary/#loreleycoreworkercommit_summary","text":"Commit summarisation utilities used by the evolution worker to derive concise git commit subjects from planning and coding context.","title":"loreley.core.worker.commit_summary"},{"location":"loreley/core/worker/commit_summary/#domain-types-and-errors","text":"CommitSummaryError : runtime error raised when the summariser cannot produce a subject line (for example, due to API errors, empty model output, or repeated failures across retries).","title":"Domain types and errors"},{"location":"loreley/core/worker/commit_summary/#commitsummarizer","text":"CommitSummarizer : LLM-backed helper responsible for generating short, imperative git subjects. Configured via loreley.config.Settings worker evolution commit options: WORKER_EVOLUTION_COMMIT_MODEL : model identifier used with the OpenAI client. WORKER_EVOLUTION_COMMIT_TEMPERATURE : sampling temperature. WORKER_EVOLUTION_COMMIT_MAX_OUTPUT_TOKENS : upper bound on model output tokens (clamped to at least 32). WORKER_EVOLUTION_COMMIT_MAX_RETRIES : maximum number of retry attempts on failure (minimum 1). WORKER_EVOLUTION_COMMIT_RETRY_BACKOFF_SECONDS : linear backoff applied between retries. WORKER_EVOLUTION_COMMIT_SUBJECT_MAX_CHARS : hard character limit for subject lines (minimum 32). Lazily initialises an OpenAI client and applies a local _truncate_limit when building prompts to keep context sizes reasonable. Respects the global OpenAI API surface setting OPENAI_API_SPEC : \"responses\" (default) uses the unified Responses API ( client.responses.create ) with an instructions string and full prompt as input . \"chat_completions\" uses the Chat Completions API ( client.chat.completions.create ), mapping the same instruction text to a system message and the prompt to a user message, extracting the assistant's reply from the first choice.","title":"CommitSummarizer"},{"location":"loreley/core/worker/commit_summary/#subject-generation","text":"generate(job, plan, coding) : Constructs a detailed prompt that includes: The global job goal . Plan summary , rationale , focus metrics, guardrails, constraints, acceptance criteria, and notes. Coding execution summary, per-step outcomes (step IDs, statuses, summaries), and the list of tests executed. The coding agent's own suggested commit_message as a fallback hint. Calls the OpenAI responses API with the configured model, temperature, and token limit, plus an instructions string that asks for a single imperative git subject no longer than 72 characters. Retries up to _max_retries times on OpenAIError or CommitSummaryError , waiting for retry_backoff * attempt seconds between attempts, regardless of whether Responses or Chat Completions is selected. On success, strips and normalises whitespace, then enforces the subject character limit via _normalise_subject() , logging the attempt count via loguru . On exhausting retries, raises CommitSummaryError with a descriptive message including the number of attempts.","title":"Subject generation"},{"location":"loreley/core/worker/commit_summary/#normalisation-helpers","text":"coerce_subject(text, *, default) : Used as a safer fallback when LLM-based summarisation fails or when an existing subject must be clamped to a valid git style. Collapses whitespace, falls back to a provided default when text is empty, and applies _normalise_subject() to respect the configured character limit. _normalise_subject(text) : Collapses consecutive whitespace to single spaces and trims leading/trailing spaces. If the cleaned subject exceeds _subject_limit , truncates it and appends an ellipsis to signal truncation. _build_prompt(job, plan, coding) / _truncate(text, limit=None) : Internal helpers which format the rich multi-section prompt while bounding long summaries and step descriptions, ensuring that the most relevant context is preserved for the LLM.","title":"Normalisation helpers"},{"location":"loreley/core/worker/evaluator/","text":"loreley.core.worker.evaluator \u00b6 Evaluation utilities for Loreley's autonomous worker, responsible for running user-defined evaluation plugins in an isolated subprocess and turning their outputs into structured metrics. Domain types \u00b6 EvaluationMetric : single metric reported by the evaluation plugin ( name , numeric value , optional unit , higher_is_better flag, and optional structured details mapping). Provides as_dict() to produce a JSON-serialisable representation. EvaluationContext : immutable-ish context object passed into plugins, including the git worktree path, optional base_commit_hash and candidate_commit_hash , optional job_id and high-level goal , an arbitrary payload dict (typically containing job and plan information), an optional plan_summary , and a free-form metadata dict. Paths and mappings are normalised and resolved in __post_init__ . EvaluationResult : structured result returned from evaluation, containing a mandatory summary , a tuple of metrics , a tuple of tests_executed , a tuple of textual logs , and an extra dict for arbitrary details; its __post_init__ enforces a non-empty summary and normalises all collections. Exceptions and protocols \u00b6 EvaluationError : custom runtime error raised when the evaluator cannot run the plugin successfully (import failures, bad configuration, timeouts, invalid payloads, etc.). EvaluationPlugin : protocol type describing callables that accept an EvaluationContext and return either an EvaluationResult or a plain mapping compatible with EvaluationResult fields. EvaluationCallable : internal alias for the concrete callable signature used by the evaluator. Evaluator \u00b6 Evaluator : adapter around user-defined evaluation plugins that handles import, isolation, timeouts, and coercion into EvaluationResult . Configured via loreley.config.Settings worker evaluator options ( WORKER_EVALUATOR_PLUGIN , WORKER_EVALUATOR_PYTHON_PATHS , WORKER_EVALUATOR_TIMEOUT_SECONDS , WORKER_EVALUATOR_MAX_METRICS ). evaluate(context) : validates that the worktree exists and is a directory, resolves or imports the plugin callable, logs the run via loguru and rich , executes the plugin in a separate process with a strict timeout, and converts the returned payload into an EvaluationResult , truncating the number of metrics to max_metrics when necessary. Supports two configuration modes: A dotted string reference such as package.module:plugin or package.module.plugin via WORKER_EVALUATOR_PLUGIN . An inline callable passed at construction time (useful for tests or in-process usage), in which case no import is performed in the subprocess. Extends sys.path using WORKER_EVALUATOR_PYTHON_PATHS before importing plugins, allowing evaluation logic to live outside the main application package. Plugin execution model \u00b6 The evaluator always runs plugins in a dedicated subprocess created via multiprocessing.get_context(\"spawn\") : _plugin_subprocess_entry() prepares the Python path, imports or reuses the plugin callable, executes it with the provided EvaluationContext , and sends either an (\"ok\", payload) or (\"error\", {message, traceback}) tuple back through a multiprocessing.Queue . The parent process waits up to timeout seconds for the subprocess to finish, and a small additional grace period to read from the queue. If the subprocess is still alive after the timeout, the evaluator terminates it and raises EvaluationError with a clear timeout message. Payload coercion helpers \u00b6 _coerce_result(payload) : converts whatever the plugin returned into an EvaluationResult . Accepts an existing EvaluationResult instance as-is. When given a mapping, expects at least a non-empty summary , plus optional metrics , tests_executed , logs , and extra entries. Raises EvaluationError when the payload is missing a summary or is of an unsupported type. _coerce_metrics(metrics_payload) : accepts a single EvaluationMetric , a mapping, or an iterable of these, and always returns a tuple of EvaluationMetric instances. _metric_from_mapping(payload) : turns a mapping into an EvaluationMetric , enforcing presence and validity of name and value fields, and validating the shape of unit , higher_is_better , and details . _normalise_sequence(values, label) : utility used to normalise tests_executed and logs into tuples of non-empty strings, accepting either a single string or an arbitrary iterable. _coerce_extra(payload) : normalises the extra field into a plain dict, rejecting non-mapping inputs with EvaluationError . _validate_context(context) : ensures that the worktree exists and is a directory before any plugin is run, failing fast with EvaluationError otherwise.","title":"Evaluation"},{"location":"loreley/core/worker/evaluator/#loreleycoreworkerevaluator","text":"Evaluation utilities for Loreley's autonomous worker, responsible for running user-defined evaluation plugins in an isolated subprocess and turning their outputs into structured metrics.","title":"loreley.core.worker.evaluator"},{"location":"loreley/core/worker/evaluator/#domain-types","text":"EvaluationMetric : single metric reported by the evaluation plugin ( name , numeric value , optional unit , higher_is_better flag, and optional structured details mapping). Provides as_dict() to produce a JSON-serialisable representation. EvaluationContext : immutable-ish context object passed into plugins, including the git worktree path, optional base_commit_hash and candidate_commit_hash , optional job_id and high-level goal , an arbitrary payload dict (typically containing job and plan information), an optional plan_summary , and a free-form metadata dict. Paths and mappings are normalised and resolved in __post_init__ . EvaluationResult : structured result returned from evaluation, containing a mandatory summary , a tuple of metrics , a tuple of tests_executed , a tuple of textual logs , and an extra dict for arbitrary details; its __post_init__ enforces a non-empty summary and normalises all collections.","title":"Domain types"},{"location":"loreley/core/worker/evaluator/#exceptions-and-protocols","text":"EvaluationError : custom runtime error raised when the evaluator cannot run the plugin successfully (import failures, bad configuration, timeouts, invalid payloads, etc.). EvaluationPlugin : protocol type describing callables that accept an EvaluationContext and return either an EvaluationResult or a plain mapping compatible with EvaluationResult fields. EvaluationCallable : internal alias for the concrete callable signature used by the evaluator.","title":"Exceptions and protocols"},{"location":"loreley/core/worker/evaluator/#evaluator","text":"Evaluator : adapter around user-defined evaluation plugins that handles import, isolation, timeouts, and coercion into EvaluationResult . Configured via loreley.config.Settings worker evaluator options ( WORKER_EVALUATOR_PLUGIN , WORKER_EVALUATOR_PYTHON_PATHS , WORKER_EVALUATOR_TIMEOUT_SECONDS , WORKER_EVALUATOR_MAX_METRICS ). evaluate(context) : validates that the worktree exists and is a directory, resolves or imports the plugin callable, logs the run via loguru and rich , executes the plugin in a separate process with a strict timeout, and converts the returned payload into an EvaluationResult , truncating the number of metrics to max_metrics when necessary. Supports two configuration modes: A dotted string reference such as package.module:plugin or package.module.plugin via WORKER_EVALUATOR_PLUGIN . An inline callable passed at construction time (useful for tests or in-process usage), in which case no import is performed in the subprocess. Extends sys.path using WORKER_EVALUATOR_PYTHON_PATHS before importing plugins, allowing evaluation logic to live outside the main application package.","title":"Evaluator"},{"location":"loreley/core/worker/evaluator/#plugin-execution-model","text":"The evaluator always runs plugins in a dedicated subprocess created via multiprocessing.get_context(\"spawn\") : _plugin_subprocess_entry() prepares the Python path, imports or reuses the plugin callable, executes it with the provided EvaluationContext , and sends either an (\"ok\", payload) or (\"error\", {message, traceback}) tuple back through a multiprocessing.Queue . The parent process waits up to timeout seconds for the subprocess to finish, and a small additional grace period to read from the queue. If the subprocess is still alive after the timeout, the evaluator terminates it and raises EvaluationError with a clear timeout message.","title":"Plugin execution model"},{"location":"loreley/core/worker/evaluator/#payload-coercion-helpers","text":"_coerce_result(payload) : converts whatever the plugin returned into an EvaluationResult . Accepts an existing EvaluationResult instance as-is. When given a mapping, expects at least a non-empty summary , plus optional metrics , tests_executed , logs , and extra entries. Raises EvaluationError when the payload is missing a summary or is of an unsupported type. _coerce_metrics(metrics_payload) : accepts a single EvaluationMetric , a mapping, or an iterable of these, and always returns a tuple of EvaluationMetric instances. _metric_from_mapping(payload) : turns a mapping into an EvaluationMetric , enforcing presence and validity of name and value fields, and validating the shape of unit , higher_is_better , and details . _normalise_sequence(values, label) : utility used to normalise tests_executed and logs into tuples of non-empty strings, accepting either a single string or an arbitrary iterable. _coerce_extra(payload) : normalises the extra field into a plain dict, rejecting non-mapping inputs with EvaluationError . _validate_context(context) : ensures that the worktree exists and is a directory before any plugin is run, failing fast with EvaluationError otherwise.","title":"Payload coercion helpers"},{"location":"loreley/core/worker/evolution/","text":"loreley.core.worker.evolution \u00b6 Autonomous evolution worker that orchestrates planning, coding, evaluation, repository management, and persistence for a single evolution job. Domain types \u00b6 CommitSnapshot : immutable snapshot of commit-related data used to build planning context ( commit_hash , derived summary , optional evaluation_summary , a tuple of text highlights , a tuple of CommitMetric instances, and an extra_context dict that may include both DB and MAP-Elites metadata). Exposes to_planning_context() to convert into a CommitPlanningContext . JobContext : in-memory representation of a locked evolution job containing: job_id , base_commit_hash , optional island_id , optional experiment_id and repository_id , and the raw job payload . base_snapshot and inspiration_snapshots that wrap DB records and/or MAP-Elites payloads. user-facing goal , constraints , acceptance_criteria , optional iteration_hint , free-form notes , and tags , all normalised to tuples of strings. a boolean is_seed_job flag indicating whether the job is a cold-start seed job (root base commit, no inspirations, and/or an explicit seed_job marker in the payload). EvolutionWorkerResult : structured success payload returned from EvolutionWorker.run() , combining the job_id , base_commit_hash , resulting candidate_commit_hash , the full PlanningAgentResponse , CodingAgentResponse , EvaluationResult , CheckoutContext , and the final commit_message used for the worker commit. Public worker API \u00b6 EvolutionWorker : service-layer entry point for running an evolution job synchronously end-to-end. Constructor wires together dependencies, all of which may be overridden for tests or custom orchestration: WorkerRepository for git operations. PlanningAgent / CodingAgent for Codex-powered planning and coding. Evaluator for running evaluation plugins. CommitSummarizer for generating concise commit messages. EvolutionJobStore for DB persistence of job status and results. run(job_id) : Coerces the job_id into a UUID . Calls _start_job() to lock and validate the job row, building a JobContext . Checks out the base commit via WorkerRepository.checkout_for_job() . Runs planning ( _run_planning() ), coding ( _run_coding() ), and evaluation ( _run_evaluation() ) in sequence. Prepares a commit message via _prepare_commit_message() , then creates and pushes a new commit via _create_commit() . Persists success artifacts and metrics through EvolutionJobStore.persist_success() and prunes stale job branches. Returns an EvolutionWorkerResult when everything succeeds. On failure, records the error via _mark_job_failed() and re-raises, or directly propagates job lock/precondition errors. Orchestration helpers \u00b6 _start_job(job_id) : uses EvolutionJobStore.start_job() to lock the job row, validates its status, and constructs a JobContext by: Loading commit metadata and metrics from the DB via _load_commit_snapshot() . Merging optional MAP-Elites record payloads from the job payload into extra_context . Deriving the job goal , constraints , acceptance_criteria , iteration_hint , notes , and tags from the payload and extra context using a set of coercion helpers; when no explicit goal is provided in the payload or extra_context , the worker falls back to the configured Settings.worker_evolution_global_goal . For cold-start seed jobs (root base commit with no inspirations and/or an explicit seed_job flag in extra_context ), _start_job sets is_seed_job=True and appends a short seed hint to iteration_hint to make the cold-start semantics visible to downstream agents. _run_planning(job_ctx, checkout) : builds a PlanningAgentRequest from commit snapshots and job fields, invokes PlanningAgent.plan() , and wraps PlanningError into EvolutionWorkerError . For seed jobs, _run_planning clears metrics, highlights, and evaluation details from the base planning context, drops all inspirations, and passes cold_start=True so that the planning agent treats the request as a cold-start seed population design run. _run_coding(job_ctx, plan, checkout) : builds a CodingAgentRequest from the plan and job context, runs CodingAgent.implement() , and wraps CodingError into EvolutionWorkerError . _prepare_commit_message(job_ctx, plan, coding) : delegates to CommitSummarizer.generate() to generate an LLM-backed git subject line; if summarisation fails, falls back to the coding agent's suggested commit_message , plan summary , or a generic \"Evolution job <id>\" string. _create_commit(checkout, commit_message) : ensures the checkout is on a branch and that the repository contains changes, stages everything, creates a commit, and pushes the per-job branch using force-with-lease . _run_evaluation(job_ctx, checkout, plan, candidate_commit) : constructs an EvaluationContext payload that includes job metadata and a normalised plan payload (via build_plan_payload() ), then calls Evaluator.evaluate() and wraps EvaluationError into EvolutionWorkerError . _prune_job_branches() : calls WorkerRepository.prune_stale_job_branches() and logs the number of branches removed, swallowing repository errors into warnings. _mark_job_failed(job_id, exc) : logs a red failure message and forwards the concise error text to EvolutionJobStore.mark_job_failed() , ensuring job rows still capture failures even when other parts of the worker raise. Data extraction and normalisation \u00b6 _load_commit_snapshot(commit_hash, fallback) : pulls CommitMetadata and Metric rows for a given commit hash via session_scope() , merges DB and fallback MAP-Elites data into a CommitSnapshot , and derives: A human-readable summary built from commit message, fallback metadata, or a plain \"Commit <hash>\" string. A set of highlights assembled from various highlights / snippets / notes fields in DB and payload metadata. A list of CommitMetric values taken either from DB rows or fallback payload metrics. Additional helpers such as _extract_goal() , _extract_iteration_hint() , _map_inspiration_payloads() , _extract_mapping() , _extract_highlights() , _first_non_empty() , _coerce_str_sequence() , and _coerce_uuid() encapsulate common logic for turning loosely-structured job payloads into the strongly-typed structures that the planning and coding agents expect. In particular, _extract_goal() derives the goal from explicit job fields and the global configuration and does not use commit messages as a fallback.","title":"Evolution loop"},{"location":"loreley/core/worker/evolution/#loreleycoreworkerevolution","text":"Autonomous evolution worker that orchestrates planning, coding, evaluation, repository management, and persistence for a single evolution job.","title":"loreley.core.worker.evolution"},{"location":"loreley/core/worker/evolution/#domain-types","text":"CommitSnapshot : immutable snapshot of commit-related data used to build planning context ( commit_hash , derived summary , optional evaluation_summary , a tuple of text highlights , a tuple of CommitMetric instances, and an extra_context dict that may include both DB and MAP-Elites metadata). Exposes to_planning_context() to convert into a CommitPlanningContext . JobContext : in-memory representation of a locked evolution job containing: job_id , base_commit_hash , optional island_id , optional experiment_id and repository_id , and the raw job payload . base_snapshot and inspiration_snapshots that wrap DB records and/or MAP-Elites payloads. user-facing goal , constraints , acceptance_criteria , optional iteration_hint , free-form notes , and tags , all normalised to tuples of strings. a boolean is_seed_job flag indicating whether the job is a cold-start seed job (root base commit, no inspirations, and/or an explicit seed_job marker in the payload). EvolutionWorkerResult : structured success payload returned from EvolutionWorker.run() , combining the job_id , base_commit_hash , resulting candidate_commit_hash , the full PlanningAgentResponse , CodingAgentResponse , EvaluationResult , CheckoutContext , and the final commit_message used for the worker commit.","title":"Domain types"},{"location":"loreley/core/worker/evolution/#public-worker-api","text":"EvolutionWorker : service-layer entry point for running an evolution job synchronously end-to-end. Constructor wires together dependencies, all of which may be overridden for tests or custom orchestration: WorkerRepository for git operations. PlanningAgent / CodingAgent for Codex-powered planning and coding. Evaluator for running evaluation plugins. CommitSummarizer for generating concise commit messages. EvolutionJobStore for DB persistence of job status and results. run(job_id) : Coerces the job_id into a UUID . Calls _start_job() to lock and validate the job row, building a JobContext . Checks out the base commit via WorkerRepository.checkout_for_job() . Runs planning ( _run_planning() ), coding ( _run_coding() ), and evaluation ( _run_evaluation() ) in sequence. Prepares a commit message via _prepare_commit_message() , then creates and pushes a new commit via _create_commit() . Persists success artifacts and metrics through EvolutionJobStore.persist_success() and prunes stale job branches. Returns an EvolutionWorkerResult when everything succeeds. On failure, records the error via _mark_job_failed() and re-raises, or directly propagates job lock/precondition errors.","title":"Public worker API"},{"location":"loreley/core/worker/evolution/#orchestration-helpers","text":"_start_job(job_id) : uses EvolutionJobStore.start_job() to lock the job row, validates its status, and constructs a JobContext by: Loading commit metadata and metrics from the DB via _load_commit_snapshot() . Merging optional MAP-Elites record payloads from the job payload into extra_context . Deriving the job goal , constraints , acceptance_criteria , iteration_hint , notes , and tags from the payload and extra context using a set of coercion helpers; when no explicit goal is provided in the payload or extra_context , the worker falls back to the configured Settings.worker_evolution_global_goal . For cold-start seed jobs (root base commit with no inspirations and/or an explicit seed_job flag in extra_context ), _start_job sets is_seed_job=True and appends a short seed hint to iteration_hint to make the cold-start semantics visible to downstream agents. _run_planning(job_ctx, checkout) : builds a PlanningAgentRequest from commit snapshots and job fields, invokes PlanningAgent.plan() , and wraps PlanningError into EvolutionWorkerError . For seed jobs, _run_planning clears metrics, highlights, and evaluation details from the base planning context, drops all inspirations, and passes cold_start=True so that the planning agent treats the request as a cold-start seed population design run. _run_coding(job_ctx, plan, checkout) : builds a CodingAgentRequest from the plan and job context, runs CodingAgent.implement() , and wraps CodingError into EvolutionWorkerError . _prepare_commit_message(job_ctx, plan, coding) : delegates to CommitSummarizer.generate() to generate an LLM-backed git subject line; if summarisation fails, falls back to the coding agent's suggested commit_message , plan summary , or a generic \"Evolution job <id>\" string. _create_commit(checkout, commit_message) : ensures the checkout is on a branch and that the repository contains changes, stages everything, creates a commit, and pushes the per-job branch using force-with-lease . _run_evaluation(job_ctx, checkout, plan, candidate_commit) : constructs an EvaluationContext payload that includes job metadata and a normalised plan payload (via build_plan_payload() ), then calls Evaluator.evaluate() and wraps EvaluationError into EvolutionWorkerError . _prune_job_branches() : calls WorkerRepository.prune_stale_job_branches() and logs the number of branches removed, swallowing repository errors into warnings. _mark_job_failed(job_id, exc) : logs a red failure message and forwards the concise error text to EvolutionJobStore.mark_job_failed() , ensuring job rows still capture failures even when other parts of the worker raise.","title":"Orchestration helpers"},{"location":"loreley/core/worker/evolution/#data-extraction-and-normalisation","text":"_load_commit_snapshot(commit_hash, fallback) : pulls CommitMetadata and Metric rows for a given commit hash via session_scope() , merges DB and fallback MAP-Elites data into a CommitSnapshot , and derives: A human-readable summary built from commit message, fallback metadata, or a plain \"Commit <hash>\" string. A set of highlights assembled from various highlights / snippets / notes fields in DB and payload metadata. A list of CommitMetric values taken either from DB rows or fallback payload metrics. Additional helpers such as _extract_goal() , _extract_iteration_hint() , _map_inspiration_payloads() , _extract_mapping() , _extract_highlights() , _first_non_empty() , _coerce_str_sequence() , and _coerce_uuid() encapsulate common logic for turning loosely-structured job payloads into the strongly-typed structures that the planning and coding agents expect. In particular, _extract_goal() derives the goal from explicit job fields and the global configuration and does not use commit messages as a fallback.","title":"Data extraction and normalisation"},{"location":"loreley/core/worker/job_store/","text":"loreley.core.worker.job_store \u00b6 Persistence adapter for the evolution worker, responsible for locking jobs, storing results, and recording job failures in the database. Domain types and errors \u00b6 EvolutionWorkerError : base runtime error used when the worker cannot complete or persist a job due to configuration, database, or repository issues. JobLockConflict : raised when start_job() fails to obtain a NOWAIT lock on a job row, indicating that another worker is already processing the same job. JobPreconditionError : raised when a job cannot start because preconditions are not satisfied (missing row, unsupported status, missing base_commit_hash , etc.). LockedJob : dataclass snapshot of the locked EvolutionJob row containing the job_id , base_commit_hash , optional island_id , optional experiment_id and repository_id , the deserialised JSON payload , and the tuple of inspiration_commit_hashes . This is used by EvolutionWorker to build its JobContext . Serialization helpers \u00b6 build_plan_payload(response) : converts a PlanningAgentResponse into a JSON-serialisable dict. Serialises the underlying PlanningPlan via as_dict() . Adds the planner prompt , raw Codex raw_output , CLI command , stderr , number of attempts , and total duration_seconds so downstream systems can introspect planner behaviour. build_coding_payload(response) : converts a CodingAgentResponse into a dict that flattens both the CodingPlanExecution and transport metadata. Includes implementation_summary , final commit_message , detailed per-step outcomes, executed tests, recommended tests, follow-up items, notes, raw Codex output, prompt, command, stderr, attempts, and duration. build_evaluation_payload(result) : converts an EvaluationResult into a dict containing its textual summary , a list of metric dicts via metric.as_dict() , tests_executed , logs , and an extra mapping. EvolutionJobStore \u00b6 EvolutionJobStore : database-facing adapter that encapsulates the lifecycle of an evolution job. Constructed with Settings to attach worker/application metadata when persisting results. Uses session_scope() and the ORM models from loreley.db.models ( EvolutionJob , CommitMetadata , Metric , JobStatus ) to modify rows transactionally. Job lifecycle methods \u00b6 start_job(job_id) : Acquires a row-level lock on the EvolutionJob using SELECT ... FOR UPDATE NOWAIT . Validates that the job exists, that base_commit_hash is present, and that the current status is in {PENDING, QUEUED} . Marks the job as RUNNING , records started_at , clears any last_error , and returns a LockedJob snapshot. Wraps SQL errors into JobLockConflict when they indicate a lock-not-available condition, or EvolutionWorkerError otherwise. persist_success(job_ctx, plan, coding, evaluation, commit_hash, commit_message) : Updates the EvolutionJob row to SUCCEEDED , sets completed_at , stores the plan summary, updated job payload (including a compact result section with commit/metric/test summaries), and clears last_error . Inserts a new CommitMetadata row representing the produced commit, with parent commit hash, island ID, author/email from settings, commit message, evaluation summary, tags, and a rich extra_context payload that includes: Job context (goal, constraints, acceptance criteria, notes, tags, raw payload) plus the resolved experiment_id and repository_id for the job when available. An experiment block containing stable experiment metadata such as id , repository_id , name , config_hash , and repository_slug when the experiment can be resolved from the database. Base and inspiration commit hashes. Detailed plan, coding, and evaluation payloads via the helper functions above. Worker metadata such as app_name , environment, and completion timestamp. Inserts one Metric row per evaluation metric for the new commit, copying numeric value , unit , higher_is_better , and any structured details . Wraps SQLAlchemy errors into EvolutionWorkerError so the caller can surface persistence failures cleanly. mark_job_failed(job_id, message) : Best-effort helper that records a failure reason on an EvolutionJob row. If the job no longer exists or has already reached SUCCEEDED or CANCELLED , the call becomes a no-op. Otherwise sets status to FAILED , stamps completed_at , and stores the latest last_error message. Swallows and logs any SQL errors rather than propagating them, to avoid masking the original worker exception. Lock conflict detection \u00b6 _is_lock_conflict(exc) : inspects the original DB error to determine whether it represents a NOWAIT lock conflict. For PostgreSQL, checks for error code \"55P03\" (lock_not_available). Falls back to substring checks on the exception message for phrases like \"could not obtain lock\" or \"database is locked\" , covering other backends. Time helpers \u00b6 _utc_now() : returns the current UTC datetime and is used consistently when stamping started_at , completed_at , and worker metadata timestamps.","title":"Job store"},{"location":"loreley/core/worker/job_store/#loreleycoreworkerjob_store","text":"Persistence adapter for the evolution worker, responsible for locking jobs, storing results, and recording job failures in the database.","title":"loreley.core.worker.job_store"},{"location":"loreley/core/worker/job_store/#domain-types-and-errors","text":"EvolutionWorkerError : base runtime error used when the worker cannot complete or persist a job due to configuration, database, or repository issues. JobLockConflict : raised when start_job() fails to obtain a NOWAIT lock on a job row, indicating that another worker is already processing the same job. JobPreconditionError : raised when a job cannot start because preconditions are not satisfied (missing row, unsupported status, missing base_commit_hash , etc.). LockedJob : dataclass snapshot of the locked EvolutionJob row containing the job_id , base_commit_hash , optional island_id , optional experiment_id and repository_id , the deserialised JSON payload , and the tuple of inspiration_commit_hashes . This is used by EvolutionWorker to build its JobContext .","title":"Domain types and errors"},{"location":"loreley/core/worker/job_store/#serialization-helpers","text":"build_plan_payload(response) : converts a PlanningAgentResponse into a JSON-serialisable dict. Serialises the underlying PlanningPlan via as_dict() . Adds the planner prompt , raw Codex raw_output , CLI command , stderr , number of attempts , and total duration_seconds so downstream systems can introspect planner behaviour. build_coding_payload(response) : converts a CodingAgentResponse into a dict that flattens both the CodingPlanExecution and transport metadata. Includes implementation_summary , final commit_message , detailed per-step outcomes, executed tests, recommended tests, follow-up items, notes, raw Codex output, prompt, command, stderr, attempts, and duration. build_evaluation_payload(result) : converts an EvaluationResult into a dict containing its textual summary , a list of metric dicts via metric.as_dict() , tests_executed , logs , and an extra mapping.","title":"Serialization helpers"},{"location":"loreley/core/worker/job_store/#evolutionjobstore","text":"EvolutionJobStore : database-facing adapter that encapsulates the lifecycle of an evolution job. Constructed with Settings to attach worker/application metadata when persisting results. Uses session_scope() and the ORM models from loreley.db.models ( EvolutionJob , CommitMetadata , Metric , JobStatus ) to modify rows transactionally.","title":"EvolutionJobStore"},{"location":"loreley/core/worker/job_store/#job-lifecycle-methods","text":"start_job(job_id) : Acquires a row-level lock on the EvolutionJob using SELECT ... FOR UPDATE NOWAIT . Validates that the job exists, that base_commit_hash is present, and that the current status is in {PENDING, QUEUED} . Marks the job as RUNNING , records started_at , clears any last_error , and returns a LockedJob snapshot. Wraps SQL errors into JobLockConflict when they indicate a lock-not-available condition, or EvolutionWorkerError otherwise. persist_success(job_ctx, plan, coding, evaluation, commit_hash, commit_message) : Updates the EvolutionJob row to SUCCEEDED , sets completed_at , stores the plan summary, updated job payload (including a compact result section with commit/metric/test summaries), and clears last_error . Inserts a new CommitMetadata row representing the produced commit, with parent commit hash, island ID, author/email from settings, commit message, evaluation summary, tags, and a rich extra_context payload that includes: Job context (goal, constraints, acceptance criteria, notes, tags, raw payload) plus the resolved experiment_id and repository_id for the job when available. An experiment block containing stable experiment metadata such as id , repository_id , name , config_hash , and repository_slug when the experiment can be resolved from the database. Base and inspiration commit hashes. Detailed plan, coding, and evaluation payloads via the helper functions above. Worker metadata such as app_name , environment, and completion timestamp. Inserts one Metric row per evaluation metric for the new commit, copying numeric value , unit , higher_is_better , and any structured details . Wraps SQLAlchemy errors into EvolutionWorkerError so the caller can surface persistence failures cleanly. mark_job_failed(job_id, message) : Best-effort helper that records a failure reason on an EvolutionJob row. If the job no longer exists or has already reached SUCCEEDED or CANCELLED , the call becomes a no-op. Otherwise sets status to FAILED , stamps completed_at , and stores the latest last_error message. Swallows and logs any SQL errors rather than propagating them, to avoid masking the original worker exception.","title":"Job lifecycle methods"},{"location":"loreley/core/worker/job_store/#lock-conflict-detection","text":"_is_lock_conflict(exc) : inspects the original DB error to determine whether it represents a NOWAIT lock conflict. For PostgreSQL, checks for error code \"55P03\" (lock_not_available). Falls back to substring checks on the exception message for phrases like \"could not obtain lock\" or \"database is locked\" , covering other backends.","title":"Lock conflict detection"},{"location":"loreley/core/worker/job_store/#time-helpers","text":"_utc_now() : returns the current UTC datetime and is used consistently when stamping started_at , completed_at , and worker metadata timestamps.","title":"Time helpers"},{"location":"loreley/core/worker/planning/","text":"loreley.core.worker.planning \u00b6 Planning utilities for Loreley's autonomous worker, responsible for turning commit history and evaluation results into a structured, multi-step plan that a coding agent can execute. Domain types \u00b6 CommitMetric : lightweight value object describing a single evaluation metric ( name , numeric value , optional unit , higher_is_better flag, and human-readable summary ). CommitPlanningContext : shared context for one commit, including the commit_hash , high-level summary , optional textual highlights , an optional evaluation_summary , a sequence of CommitMetric instances, and an extra_context dict for arbitrary structured details; normalises all collections to tuples/dicts on initialisation. PlanningAgentRequest : input payload for the planning agent containing the base commit context, a sequence of inspirations , the plain-language global evolution goal (resolved by the evolution worker from either per-job payload fields or the Settings.worker_evolution_global_goal configuration), optional constraints and acceptance_criteria bullet lists, an optional iteration_hint , and a boolean cold_start flag; when cold_start=True , the planning agent treats the request as a cold-start seed population design run and adjusts the prompt accordingly. All list-like fields are normalised to tuples. PlanStep : single actionable step in the generated plan ( step_id , title , intent , actions , files , dependencies , validation , risks , references ) with an as_dict() helper that converts all tuples back to plain lists for serialisation. PlanningPlan : structured planning output that aggregates the global summary , rationale , focus_metrics , guardrails , risks , overall validation bullets, the ordered steps , optional handoff_notes , and an optional free-form fallback_plan , again with as_dict() for JSON-friendly output. PlanningAgentResponse : envelope returned from the planner containing the domain plan , raw backend JSON raw_output , the rendered prompt , executed backend command , captured stderr , number of attempts , and total duration_seconds . JSON schema and validation \u00b6 PLANNING_OUTPUT_SCHEMA : JSON schema describing the expected shape of the planning output (top-level fields like plan_summary , rationale , focus_metrics , guardrails , risks , validation , steps , handoff_notes , and fallback_plan , plus constraints on each step's fields), used when invoking the external Codex CLI. _PlanStepModel / _PlanModel : internal pydantic models that validate the Codex JSON payload against the schema and provide a typed bridge from raw JSON into the PlanStep / PlanningPlan domain objects. loreley.core.worker.agent_backend : shared backend abstractions ( AgentBackend , StructuredAgentTask , AgentInvocation ) plus the default CodexCliBackend implementation that actually talks to the codex CLI. Planning agent \u00b6 PlanningAgent : high-level orchestration layer that prepares a structured planning request and delegates execution to a configurable backend. Instantiated with a Settings object and an optional AgentBackend implementation. When no backend is provided, it uses CodexCliBackend configured via WORKER_PLANNING_CODEX_BIN , WORKER_PLANNING_CODEX_PROFILE , WORKER_PLANNING_MAX_ATTEMPTS , WORKER_PLANNING_TIMEOUT_SECONDS , WORKER_PLANNING_EXTRA_ENV , and WORKER_PLANNING_SCHEMA_PATH . You can override the default by setting WORKER_PLANNING_BACKEND to a dotted Python path ( module:attr or module.attr ) that resolves to either an AgentBackend instance, a class implementing the AgentBackend protocol (constructed with no arguments), or a factory callable that returns such an instance. plan(request, *, working_dir) : resolves the git worktree path, renders a rich natural-language prompt from the request (including base commit, inspiration commits, constraints, and acceptance criteria), builds a StructuredAgentTask that references PLANNING_OUTPUT_SCHEMA , and asks the backend to execute it. Retries the backend invocation up to max_attempts times when JSON decoding, schema validation, or other PlanningError / ValidationError issues occur, logging warnings via loguru and printing concise progress messages with rich . On success, parses the JSON into the _PlanModel , converts it into a PlanningPlan , and returns a PlanningAgentResponse ; on repeated failure or timeout, raises PlanningError with a descriptive message. Performs basic truncation of long text fields to keep prompts and summaries bounded and writes detailed debug artifacts under logs/worker/planning . Exceptions and helpers \u00b6 PlanningError : custom runtime error raised when validation fails, the backend returns an error or empty response, the planning schema path is invalid, or the working directory is not a git repository. _truncate() , _format_commit_block() , and _format_metrics() : internal utilities that format commit context and metrics into human-readable sections for the prompt while enforcing length limits and providing clear fallbacks when no metrics or highlights are available.","title":"Planning"},{"location":"loreley/core/worker/planning/#loreleycoreworkerplanning","text":"Planning utilities for Loreley's autonomous worker, responsible for turning commit history and evaluation results into a structured, multi-step plan that a coding agent can execute.","title":"loreley.core.worker.planning"},{"location":"loreley/core/worker/planning/#domain-types","text":"CommitMetric : lightweight value object describing a single evaluation metric ( name , numeric value , optional unit , higher_is_better flag, and human-readable summary ). CommitPlanningContext : shared context for one commit, including the commit_hash , high-level summary , optional textual highlights , an optional evaluation_summary , a sequence of CommitMetric instances, and an extra_context dict for arbitrary structured details; normalises all collections to tuples/dicts on initialisation. PlanningAgentRequest : input payload for the planning agent containing the base commit context, a sequence of inspirations , the plain-language global evolution goal (resolved by the evolution worker from either per-job payload fields or the Settings.worker_evolution_global_goal configuration), optional constraints and acceptance_criteria bullet lists, an optional iteration_hint , and a boolean cold_start flag; when cold_start=True , the planning agent treats the request as a cold-start seed population design run and adjusts the prompt accordingly. All list-like fields are normalised to tuples. PlanStep : single actionable step in the generated plan ( step_id , title , intent , actions , files , dependencies , validation , risks , references ) with an as_dict() helper that converts all tuples back to plain lists for serialisation. PlanningPlan : structured planning output that aggregates the global summary , rationale , focus_metrics , guardrails , risks , overall validation bullets, the ordered steps , optional handoff_notes , and an optional free-form fallback_plan , again with as_dict() for JSON-friendly output. PlanningAgentResponse : envelope returned from the planner containing the domain plan , raw backend JSON raw_output , the rendered prompt , executed backend command , captured stderr , number of attempts , and total duration_seconds .","title":"Domain types"},{"location":"loreley/core/worker/planning/#json-schema-and-validation","text":"PLANNING_OUTPUT_SCHEMA : JSON schema describing the expected shape of the planning output (top-level fields like plan_summary , rationale , focus_metrics , guardrails , risks , validation , steps , handoff_notes , and fallback_plan , plus constraints on each step's fields), used when invoking the external Codex CLI. _PlanStepModel / _PlanModel : internal pydantic models that validate the Codex JSON payload against the schema and provide a typed bridge from raw JSON into the PlanStep / PlanningPlan domain objects. loreley.core.worker.agent_backend : shared backend abstractions ( AgentBackend , StructuredAgentTask , AgentInvocation ) plus the default CodexCliBackend implementation that actually talks to the codex CLI.","title":"JSON schema and validation"},{"location":"loreley/core/worker/planning/#planning-agent","text":"PlanningAgent : high-level orchestration layer that prepares a structured planning request and delegates execution to a configurable backend. Instantiated with a Settings object and an optional AgentBackend implementation. When no backend is provided, it uses CodexCliBackend configured via WORKER_PLANNING_CODEX_BIN , WORKER_PLANNING_CODEX_PROFILE , WORKER_PLANNING_MAX_ATTEMPTS , WORKER_PLANNING_TIMEOUT_SECONDS , WORKER_PLANNING_EXTRA_ENV , and WORKER_PLANNING_SCHEMA_PATH . You can override the default by setting WORKER_PLANNING_BACKEND to a dotted Python path ( module:attr or module.attr ) that resolves to either an AgentBackend instance, a class implementing the AgentBackend protocol (constructed with no arguments), or a factory callable that returns such an instance. plan(request, *, working_dir) : resolves the git worktree path, renders a rich natural-language prompt from the request (including base commit, inspiration commits, constraints, and acceptance criteria), builds a StructuredAgentTask that references PLANNING_OUTPUT_SCHEMA , and asks the backend to execute it. Retries the backend invocation up to max_attempts times when JSON decoding, schema validation, or other PlanningError / ValidationError issues occur, logging warnings via loguru and printing concise progress messages with rich . On success, parses the JSON into the _PlanModel , converts it into a PlanningPlan , and returns a PlanningAgentResponse ; on repeated failure or timeout, raises PlanningError with a descriptive message. Performs basic truncation of long text fields to keep prompts and summaries bounded and writes detailed debug artifacts under logs/worker/planning .","title":"Planning agent"},{"location":"loreley/core/worker/planning/#exceptions-and-helpers","text":"PlanningError : custom runtime error raised when validation fails, the backend returns an error or empty response, the planning schema path is invalid, or the working directory is not a git repository. _truncate() , _format_commit_block() , and _format_metrics() : internal utilities that format commit context and metrics into human-readable sections for the prompt while enforcing length limits and providing clear fallbacks when no metrics or highlights are available.","title":"Exceptions and helpers"},{"location":"loreley/core/worker/repository/","text":"loreley.core.worker.repository \u00b6 Git worktree management for Loreley worker processes, responsible for cloning, syncing, cleaning, and publishing the upstream repository used for evolutionary jobs. Types \u00b6 RepositoryError : custom runtime error raised when a git operation fails, capturing the command, return code, stdout, and stderr for easier debugging. CheckoutContext : frozen dataclass describing the result of preparing a job checkout ( job_id , derived branch_name , selected base_commit , and local worktree path). Repository \u00b6 WorkerRepository : high-level manager for the worker git worktree built on top of git.Repo . Configured via loreley.config.Settings worker repository options ( WORKER_REPO_REMOTE_URL , WORKER_REPO_BRANCH , WORKER_REPO_WORKTREE , WORKER_REPO_GIT_BIN , WORKER_REPO_FETCH_DEPTH , WORKER_REPO_CLEAN_EXCLUDES , WORKER_REPO_JOB_BRANCH_PREFIX , WORKER_REPO_ENABLE_LFS , WORKER_REPO_JOB_BRANCH_TTL_HOURS ) and honours commit author settings for worker-produced commits. prepare() ensures the worktree directory exists, clones the remote with the configured depth/branch if necessary, aligns the local tracking branch with the configured upstream, and refreshes tags/LFS where enabled, logging progress via rich and loguru . checkout_for_job(job_id, base_commit, create_branch=True) cleans the worktree, ensures the base_commit object is available locally (unshallows the repository when needed), then either checks out that commit in detached mode or creates a per-job branch under the configured job-branch prefix, returning a CheckoutContext . clean_worktree() hard-resets tracked files and runs git clean -xdf , preserving any paths configured in WORKER_REPO_CLEAN_EXCLUDES . current_commit() returns the current HEAD commit hash for observability and scheduling. has_changes() reports whether the worktree is dirty (including untracked files), which the evolution worker uses to decide if there is anything to commit after coding. stage_all() stages all tracked and untracked changes, and commit(message) creates a commit and returns its hash, using GitPython under the hood. push_branch(branch_name, remote=\\\"origin\\\", force_with_lease=False) pushes the current branch to the configured remote (optionally with --force-with-lease ), and delete_remote_branch(branch_name, remote=\\\"origin\\\") removes remote job branches without affecting local history. prune_stale_job_branches() enumerates remote job branches under the configured job-branch prefix and deletes those whose last commit is older than WORKER_REPO_JOB_BRANCH_TTL_HOURS , logging a concise summary of how many branches were pruned. Internal helpers such as _ensure_worktree_ready() , _sync_upstream() , _ensure_remote_origin() , _fetch() , _sync_lfs() , _ensure_commit_available() , and _wrap_git_error() encapsulate the GitPython integration, remote configuration, LFS sync, shallow/unshallow behaviour, and consistent error wrapping with sanitised git commands.","title":"Repository"},{"location":"loreley/core/worker/repository/#loreleycoreworkerrepository","text":"Git worktree management for Loreley worker processes, responsible for cloning, syncing, cleaning, and publishing the upstream repository used for evolutionary jobs.","title":"loreley.core.worker.repository"},{"location":"loreley/core/worker/repository/#types","text":"RepositoryError : custom runtime error raised when a git operation fails, capturing the command, return code, stdout, and stderr for easier debugging. CheckoutContext : frozen dataclass describing the result of preparing a job checkout ( job_id , derived branch_name , selected base_commit , and local worktree path).","title":"Types"},{"location":"loreley/core/worker/repository/#repository","text":"WorkerRepository : high-level manager for the worker git worktree built on top of git.Repo . Configured via loreley.config.Settings worker repository options ( WORKER_REPO_REMOTE_URL , WORKER_REPO_BRANCH , WORKER_REPO_WORKTREE , WORKER_REPO_GIT_BIN , WORKER_REPO_FETCH_DEPTH , WORKER_REPO_CLEAN_EXCLUDES , WORKER_REPO_JOB_BRANCH_PREFIX , WORKER_REPO_ENABLE_LFS , WORKER_REPO_JOB_BRANCH_TTL_HOURS ) and honours commit author settings for worker-produced commits. prepare() ensures the worktree directory exists, clones the remote with the configured depth/branch if necessary, aligns the local tracking branch with the configured upstream, and refreshes tags/LFS where enabled, logging progress via rich and loguru . checkout_for_job(job_id, base_commit, create_branch=True) cleans the worktree, ensures the base_commit object is available locally (unshallows the repository when needed), then either checks out that commit in detached mode or creates a per-job branch under the configured job-branch prefix, returning a CheckoutContext . clean_worktree() hard-resets tracked files and runs git clean -xdf , preserving any paths configured in WORKER_REPO_CLEAN_EXCLUDES . current_commit() returns the current HEAD commit hash for observability and scheduling. has_changes() reports whether the worktree is dirty (including untracked files), which the evolution worker uses to decide if there is anything to commit after coding. stage_all() stages all tracked and untracked changes, and commit(message) creates a commit and returns its hash, using GitPython under the hood. push_branch(branch_name, remote=\\\"origin\\\", force_with_lease=False) pushes the current branch to the configured remote (optionally with --force-with-lease ), and delete_remote_branch(branch_name, remote=\\\"origin\\\") removes remote job branches without affecting local history. prune_stale_job_branches() enumerates remote job branches under the configured job-branch prefix and deletes those whose last commit is older than WORKER_REPO_JOB_BRANCH_TTL_HOURS , logging a concise summary of how many branches were pruned. Internal helpers such as _ensure_worktree_ready() , _sync_upstream() , _ensure_remote_origin() , _fetch() , _sync_lfs() , _ensure_commit_available() , and _wrap_git_error() encapsulate the GitPython integration, remote configuration, LFS sync, shallow/unshallow behaviour, and consistent error wrapping with sanitised git commands.","title":"Repository"},{"location":"loreley/db/base/","text":"loreley.db.base \u00b6 Database engine and session management for Loreley. Engine and session factory \u00b6 _sanitize_dsn(raw_dsn) : masks the password portion of a database DSN so it can be safely logged. engine : global SQLAlchemy engine created from Settings.database_dsn , configured with pool_pre_ping , connection pool sizing, timeouts, and optional SQL echoing. SessionLocal : scoped session factory bound to engine , with autocommit=False , autoflush=False , and expire_on_commit=False to make ORM usage predictable in long-running workers. Declarative base and context manager \u00b6 Base : shared declarative base class used by all ORM models in loreley.db.models . session_scope() : context manager that yields a Session , commits on success, rolls back on exception, logs failures with loguru , and always disposes of the session via SessionLocal.remove() .","title":"Engine & sessions"},{"location":"loreley/db/base/#loreleydbbase","text":"Database engine and session management for Loreley.","title":"loreley.db.base"},{"location":"loreley/db/base/#engine-and-session-factory","text":"_sanitize_dsn(raw_dsn) : masks the password portion of a database DSN so it can be safely logged. engine : global SQLAlchemy engine created from Settings.database_dsn , configured with pool_pre_ping , connection pool sizing, timeouts, and optional SQL echoing. SessionLocal : scoped session factory bound to engine , with autocommit=False , autoflush=False , and expire_on_commit=False to make ORM usage predictable in long-running workers.","title":"Engine and session factory"},{"location":"loreley/db/base/#declarative-base-and-context-manager","text":"Base : shared declarative base class used by all ORM models in loreley.db.models . session_scope() : context manager that yields a Session , commits on success, rolls back on exception, logs failures with loguru , and always disposes of the session via SessionLocal.remove() .","title":"Declarative base and context manager"},{"location":"loreley/db/models/","text":"loreley.db.models \u00b6 ORM models and enums for tracking evolutionary jobs, commits, and associated metrics. Shared mixins and enums \u00b6 TimestampMixin : adds created_at and updated_at columns that default to now() and automatically update on modification. JobStatus : string-based Enum capturing the lifecycle of an evolution job ( PENDING , QUEUED , RUNNING , SUCCEEDED , FAILED , CANCELLED ). Core models \u00b6 Repository ( repositories table): normalised view of a source code repository. Stores a stable slug derived from either the canonical remote URL or local worktree path, the current remote_url , optional root_path , and an extra JSONB payload with additional metadata (canonical origin, remotes, etc.). Owns a collection of Experiment rows and is treated as the top-level key when reasoning about experiments in a multi-repository deployment. Experiment ( experiments table): captures a single experiment configuration within a repository. References a repository_id , a stable config_hash computed from a subset of Settings , an optional human-readable name , a JSONB config_snapshot of the relevant settings, and a free-form status . Relates to EvolutionJob , CommitMetadata , and MapElitesState so that jobs, commits, and archive state can all be grouped by experiment. CommitMetadata ( commits table): stores git commit metadata and evolution context. Tracks commit hash, parent hash, optional island identifier, optional experiment_id , author, message, evaluation summary, free-form tags, and arbitrary JSONB extra_context . Defines relationships to associated Metric records and jobs that use this commit as their base, and back to the owning Experiment when one exists. Metric ( metrics table): records individual evaluation metrics for a commit. Stores metric name , numeric value , optional unit , whether higher values are better, and a JSONB details payload. Links back to CommitMetadata via commit_hash and maintains uniqueness per (commit_hash, name) . EvolutionJob ( evolution_jobs table): represents a single evolution iteration scheduled by the system. Tracks current status , base commit, island ID, optional experiment_id , inspiration commit hashes, request payload , human-readable plan_summary , priority, scheduling/processing timestamps, and last error if any. Relates back to CommitMetadata via base_commit_hash and to Experiment via experiment_id , enabling efficient queries per base commit or experiment. MapElitesState ( map_elites_states table): persists per-experiment, per-island snapshots of the MAP-Elites archive. Uses a composite primary key (experiment_id, island_id) so that multiple experiments can maintain independent archives even when they share island identifiers. Stores a JSONB snapshot payload containing feature bounds, PCA history/projection metadata, and the current archive entries so that loreley.core.map_elites.snapshot and MapElitesManager can restore state across process restarts for a given experiment.","title":"Models"},{"location":"loreley/db/models/#loreleydbmodels","text":"ORM models and enums for tracking evolutionary jobs, commits, and associated metrics.","title":"loreley.db.models"},{"location":"loreley/db/models/#shared-mixins-and-enums","text":"TimestampMixin : adds created_at and updated_at columns that default to now() and automatically update on modification. JobStatus : string-based Enum capturing the lifecycle of an evolution job ( PENDING , QUEUED , RUNNING , SUCCEEDED , FAILED , CANCELLED ).","title":"Shared mixins and enums"},{"location":"loreley/db/models/#core-models","text":"Repository ( repositories table): normalised view of a source code repository. Stores a stable slug derived from either the canonical remote URL or local worktree path, the current remote_url , optional root_path , and an extra JSONB payload with additional metadata (canonical origin, remotes, etc.). Owns a collection of Experiment rows and is treated as the top-level key when reasoning about experiments in a multi-repository deployment. Experiment ( experiments table): captures a single experiment configuration within a repository. References a repository_id , a stable config_hash computed from a subset of Settings , an optional human-readable name , a JSONB config_snapshot of the relevant settings, and a free-form status . Relates to EvolutionJob , CommitMetadata , and MapElitesState so that jobs, commits, and archive state can all be grouped by experiment. CommitMetadata ( commits table): stores git commit metadata and evolution context. Tracks commit hash, parent hash, optional island identifier, optional experiment_id , author, message, evaluation summary, free-form tags, and arbitrary JSONB extra_context . Defines relationships to associated Metric records and jobs that use this commit as their base, and back to the owning Experiment when one exists. Metric ( metrics table): records individual evaluation metrics for a commit. Stores metric name , numeric value , optional unit , whether higher values are better, and a JSONB details payload. Links back to CommitMetadata via commit_hash and maintains uniqueness per (commit_hash, name) . EvolutionJob ( evolution_jobs table): represents a single evolution iteration scheduled by the system. Tracks current status , base commit, island ID, optional experiment_id , inspiration commit hashes, request payload , human-readable plan_summary , priority, scheduling/processing timestamps, and last error if any. Relates back to CommitMetadata via base_commit_hash and to Experiment via experiment_id , enabling efficient queries per base commit or experiment. MapElitesState ( map_elites_states table): persists per-experiment, per-island snapshots of the MAP-Elites archive. Uses a composite primary key (experiment_id, island_id) so that multiple experiments can maintain independent archives even when they share island identifiers. Stores a JSONB snapshot payload containing feature bounds, PCA history/projection metadata, and the current archive entries so that loreley.core.map_elites.snapshot and MapElitesManager can restore state across process restarts for a given experiment.","title":"Core models"},{"location":"loreley/scheduler/ingestion/","text":"loreley.scheduler.ingestion \u00b6 Result ingestion and MAP-Elites maintenance logic extracted from the central evolution scheduler. The MapElitesIngestion class owns how succeeded jobs are discovered , mapped to git commits, and folded into the MAP-Elites archives, as well as how the configured experiment root commit is initialised. MapElitesIngestion \u00b6 from loreley.scheduler.ingestion import MapElitesIngestion Purpose : ingest completed evolution jobs into MAP-Elites, record rich ingestion state back onto the job payload, and ensure the experiment's root commit is registered and evaluated as a baseline in the database. Construction : created by EvolutionScheduler with: the shared Settings instance, the interactive rich console, a git.Repo handle for the scheduler's repository root, the experiment-scoped MapElitesManager , the current experiment and its repository . Ingesting succeeded jobs \u00b6 ingest_completed_jobs() -> int : Scans for SUCCEEDED EvolutionJob rows up to SCHEDULER_INGEST_BATCH_SIZE . Filters out jobs whose payload already records a terminal ingestion status ( \"succeeded\" or \"skipped\" ). Builds a JobSnapshot for each remaining job and forwards it to _ingest_snapshot(...) . Returns the number of jobs whose commits actually updated the MAP-Elites archive. Internally, _ingest_snapshot(...) : Extracts result.commit_hash and optional result.metrics from the job payload. Ensures the corresponding git commit is present locally, fetching from remotes as necessary. Computes a list of ChangedFile entries from the commit stats to use as a proxy for behavioural diversity. Calls MapElitesManager.ingest(...) with: commit_hash , per-file changed_files , metrics , island_id , repo_root and treeish , a structured metadata block that links the job, context, and evaluation summaries. Writes a compact ingestion state back under payload[\"ingestion\"][\"map_elites\"] , including: status ( \"succeeded\" or \"skipped\" ), delta , status_code , and message from the ingest result, a serialised view of the archive record (if any), retry bookkeeping ( attempts , last_attempt_at , reason ). This state allows ingestion to be retried safely and audited later without re-running the full evaluation. Root commit initialisation \u00b6 When MAPELITES_EXPERIMENT_ROOT_COMMIT is set, EvolutionScheduler asks MapElitesIngestion to initialise that commit via initialise_root_commit(commit_hash) : _ensure_commit_available(...) guarantees the commit exists locally, fetching from remotes as needed. _ensure_root_commit_metadata(...) creates or updates a CommitMetadata row with: the commit's parent, author, and message, the current experiment_id , a default island id (from MAPELITES_DEFAULT_ISLAND_ID or \"main\" ), a rich extra_context block that links back to the experiment and repository. _ensure_root_commit_evaluated(...) runs a one-off evaluation for the root commit when no Metric rows already exist, writing baseline metrics into the metrics table and a compact root_evaluation block into CommitMetadata.extra_context . These metrics act as an experiment-wide baseline but do not insert the root commit into any MAP-Elites archive. Failures during root-commit initialisation are logged but do not prevent the scheduler from running; they simply mean the experiment may effectively start from the first successfully ingested commit instead. Interaction with EvolutionScheduler \u00b6 EvolutionScheduler.tick() uses MapElitesIngestion as the first stage in the pipeline: ingest_completed_jobs() ingests any newly succeeded jobs and annotates them with ingestion state. Later, when all jobs have finished and the global job limit has been reached, EvolutionScheduler uses MAP-Elites metrics and commit metadata to create a dedicated git branch for the current best-fitness commit. Separating this logic into MapElitesIngestion keeps the main scheduler loop small and clarifies the boundary between job lifecycle and archive maintenance .","title":"Ingestion"},{"location":"loreley/scheduler/ingestion/#loreleyscheduleringestion","text":"Result ingestion and MAP-Elites maintenance logic extracted from the central evolution scheduler. The MapElitesIngestion class owns how succeeded jobs are discovered , mapped to git commits, and folded into the MAP-Elites archives, as well as how the configured experiment root commit is initialised.","title":"loreley.scheduler.ingestion"},{"location":"loreley/scheduler/ingestion/#mapelitesingestion","text":"from loreley.scheduler.ingestion import MapElitesIngestion Purpose : ingest completed evolution jobs into MAP-Elites, record rich ingestion state back onto the job payload, and ensure the experiment's root commit is registered and evaluated as a baseline in the database. Construction : created by EvolutionScheduler with: the shared Settings instance, the interactive rich console, a git.Repo handle for the scheduler's repository root, the experiment-scoped MapElitesManager , the current experiment and its repository .","title":"MapElitesIngestion"},{"location":"loreley/scheduler/ingestion/#ingesting-succeeded-jobs","text":"ingest_completed_jobs() -> int : Scans for SUCCEEDED EvolutionJob rows up to SCHEDULER_INGEST_BATCH_SIZE . Filters out jobs whose payload already records a terminal ingestion status ( \"succeeded\" or \"skipped\" ). Builds a JobSnapshot for each remaining job and forwards it to _ingest_snapshot(...) . Returns the number of jobs whose commits actually updated the MAP-Elites archive. Internally, _ingest_snapshot(...) : Extracts result.commit_hash and optional result.metrics from the job payload. Ensures the corresponding git commit is present locally, fetching from remotes as necessary. Computes a list of ChangedFile entries from the commit stats to use as a proxy for behavioural diversity. Calls MapElitesManager.ingest(...) with: commit_hash , per-file changed_files , metrics , island_id , repo_root and treeish , a structured metadata block that links the job, context, and evaluation summaries. Writes a compact ingestion state back under payload[\"ingestion\"][\"map_elites\"] , including: status ( \"succeeded\" or \"skipped\" ), delta , status_code , and message from the ingest result, a serialised view of the archive record (if any), retry bookkeeping ( attempts , last_attempt_at , reason ). This state allows ingestion to be retried safely and audited later without re-running the full evaluation.","title":"Ingesting succeeded jobs"},{"location":"loreley/scheduler/ingestion/#root-commit-initialisation","text":"When MAPELITES_EXPERIMENT_ROOT_COMMIT is set, EvolutionScheduler asks MapElitesIngestion to initialise that commit via initialise_root_commit(commit_hash) : _ensure_commit_available(...) guarantees the commit exists locally, fetching from remotes as needed. _ensure_root_commit_metadata(...) creates or updates a CommitMetadata row with: the commit's parent, author, and message, the current experiment_id , a default island id (from MAPELITES_DEFAULT_ISLAND_ID or \"main\" ), a rich extra_context block that links back to the experiment and repository. _ensure_root_commit_evaluated(...) runs a one-off evaluation for the root commit when no Metric rows already exist, writing baseline metrics into the metrics table and a compact root_evaluation block into CommitMetadata.extra_context . These metrics act as an experiment-wide baseline but do not insert the root commit into any MAP-Elites archive. Failures during root-commit initialisation are logged but do not prevent the scheduler from running; they simply mean the experiment may effectively start from the first successfully ingested commit instead.","title":"Root commit initialisation"},{"location":"loreley/scheduler/ingestion/#interaction-with-evolutionscheduler","text":"EvolutionScheduler.tick() uses MapElitesIngestion as the first stage in the pipeline: ingest_completed_jobs() ingests any newly succeeded jobs and annotates them with ingestion state. Later, when all jobs have finished and the global job limit has been reached, EvolutionScheduler uses MAP-Elites metrics and commit metadata to create a dedicated git branch for the current best-fitness commit. Separating this logic into MapElitesIngestion keeps the main scheduler loop small and clarifies the boundary between job lifecycle and archive maintenance .","title":"Interaction with EvolutionScheduler"},{"location":"loreley/scheduler/job_scheduler/","text":"loreley.scheduler.job_scheduler \u00b6 Job production and dispatch logic extracted from the central evolution scheduler. The JobScheduler class keeps all concerns related to how many jobs can be scheduled, which jobs should be dispatched next, and when they are submitted to the Dramatiq worker queue. JobScheduler \u00b6 from loreley.scheduler.job_scheduler import JobScheduler Purpose : encapsulate database interaction and Dramatiq calls for scheduling and dispatching evolution jobs, so that the main EvolutionScheduler can focus on orchestration. Construction : created by EvolutionScheduler with: a shared Settings instance, the interactive rich console, the experiment-scoped MapElitesSampler , the current experiment_id . Measuring unfinished work \u00b6 count_unfinished_jobs() : Counts all jobs whose status is one of PENDING , QUEUED , or RUNNING . Used by the main scheduler loop to decide how much new work (if any) can safely be created this tick. Scheduling new jobs \u00b6 schedule_jobs(unfinished_jobs: int, *, total_scheduled_jobs: int) -> int : Enforces SCHEDULER_MAX_UNFINISHED_JOBS as an upper bound across PENDING / QUEUED / RUNNING jobs. Respects the optional SCHEDULER_MAX_TOTAL_JOBS global cap using the total_scheduled_jobs counter maintained by EvolutionScheduler . Requests new work from MAP-Elites via MapElitesSampler.schedule_job(experiment_id=experiment_id) . Immediately transitions any newly created jobs to QUEUED and pushes them to Dramatiq using the private _enqueue_jobs(...) helper. Returns the number of jobs scheduled during this tick. If the sampler indicates that no archive cell currently wants new work, the console logs a short [yellow]Sampler returned no job[/] message and no rows are touched in the database. Dispatching pending jobs \u00b6 dispatch_pending_jobs() -> int : Selects up to SCHEDULER_DISPATCH_BATCH_SIZE jobs with status PENDING , ordered by: priority (descending), scheduled_at (ascending), created_at (ascending), so that higher-priority and older jobs drain first. Uses a SELECT ... FOR UPDATE window to safely promote eligible jobs to QUEUED and stamp their scheduled_at time. Sends each queued job id to the Dramatiq run_evolution_job actor. Returns the number of jobs successfully dispatched this tick. Any failures while enqueuing individual jobs are logged with Loguru and surfaced on the Rich console, but do not prevent other jobs from being dispatched. Interaction with EvolutionScheduler \u00b6 EvolutionScheduler.tick() calls into JobScheduler as follows: count_unfinished_jobs() to measure current load. schedule_jobs(...) to request new work from MAP-Elites, honouring both capacity and global job limits. dispatch_pending_jobs() to move ready jobs into the worker queue. This separation keeps the scheduler loop simple and makes it easier to test and evolve the job pipeline independently of the rest of the orchestration logic.","title":"Job scheduling"},{"location":"loreley/scheduler/job_scheduler/#loreleyschedulerjob_scheduler","text":"Job production and dispatch logic extracted from the central evolution scheduler. The JobScheduler class keeps all concerns related to how many jobs can be scheduled, which jobs should be dispatched next, and when they are submitted to the Dramatiq worker queue.","title":"loreley.scheduler.job_scheduler"},{"location":"loreley/scheduler/job_scheduler/#jobscheduler","text":"from loreley.scheduler.job_scheduler import JobScheduler Purpose : encapsulate database interaction and Dramatiq calls for scheduling and dispatching evolution jobs, so that the main EvolutionScheduler can focus on orchestration. Construction : created by EvolutionScheduler with: a shared Settings instance, the interactive rich console, the experiment-scoped MapElitesSampler , the current experiment_id .","title":"JobScheduler"},{"location":"loreley/scheduler/job_scheduler/#measuring-unfinished-work","text":"count_unfinished_jobs() : Counts all jobs whose status is one of PENDING , QUEUED , or RUNNING . Used by the main scheduler loop to decide how much new work (if any) can safely be created this tick.","title":"Measuring unfinished work"},{"location":"loreley/scheduler/job_scheduler/#scheduling-new-jobs","text":"schedule_jobs(unfinished_jobs: int, *, total_scheduled_jobs: int) -> int : Enforces SCHEDULER_MAX_UNFINISHED_JOBS as an upper bound across PENDING / QUEUED / RUNNING jobs. Respects the optional SCHEDULER_MAX_TOTAL_JOBS global cap using the total_scheduled_jobs counter maintained by EvolutionScheduler . Requests new work from MAP-Elites via MapElitesSampler.schedule_job(experiment_id=experiment_id) . Immediately transitions any newly created jobs to QUEUED and pushes them to Dramatiq using the private _enqueue_jobs(...) helper. Returns the number of jobs scheduled during this tick. If the sampler indicates that no archive cell currently wants new work, the console logs a short [yellow]Sampler returned no job[/] message and no rows are touched in the database.","title":"Scheduling new jobs"},{"location":"loreley/scheduler/job_scheduler/#dispatching-pending-jobs","text":"dispatch_pending_jobs() -> int : Selects up to SCHEDULER_DISPATCH_BATCH_SIZE jobs with status PENDING , ordered by: priority (descending), scheduled_at (ascending), created_at (ascending), so that higher-priority and older jobs drain first. Uses a SELECT ... FOR UPDATE window to safely promote eligible jobs to QUEUED and stamp their scheduled_at time. Sends each queued job id to the Dramatiq run_evolution_job actor. Returns the number of jobs successfully dispatched this tick. Any failures while enqueuing individual jobs are logged with Loguru and surfaced on the Rich console, but do not prevent other jobs from being dispatched.","title":"Dispatching pending jobs"},{"location":"loreley/scheduler/job_scheduler/#interaction-with-evolutionscheduler","text":"EvolutionScheduler.tick() calls into JobScheduler as follows: count_unfinished_jobs() to measure current load. schedule_jobs(...) to request new work from MAP-Elites, honouring both capacity and global job limits. dispatch_pending_jobs() to move ready jobs into the worker queue. This separation keeps the scheduler loop simple and makes it easier to test and evolve the job pipeline independently of the rest of the orchestration logic.","title":"Interaction with EvolutionScheduler"},{"location":"loreley/scheduler/main/","text":"loreley.scheduler.main \u00b6 Central orchestration loop that keeps the Loreley evolution pipeline moving by coupling the MAP-Elites archive, the PostgreSQL job store, and the Dramatiq worker queue. EvolutionScheduler \u00b6 Purpose : continuously monitors unfinished jobs ( pending , queued , running ), schedules new work from the MAP-Elites archive when capacity allows, dispatches pending jobs to the Dramatiq run_evolution_job actor, and backfills the archive with freshly evaluated commits. Construction : EvolutionScheduler(settings=None) loads loreley.config.Settings , resolves the target repository root (preferring SCHEDULER_REPO_ROOT and falling back to WORKER_REPO_WORKTREE ), initialises a git repository handle, derives a Repository / Experiment pair via loreley.core.experiments.get_or_create_experiment() , wires MapElitesManager (scoped to that experiment_id ) plus MapElitesSampler with the same settings, and, when MAPELITES_EXPERIMENT_ROOT_COMMIT is set, delegates root-commit registration and baseline evaluation to loreley.scheduler.ingestion.MapElitesIngestion . Lifecycle : tick() runs the ingest \u2192 dispatch \u2192 measure \u2192 seed \u2192 schedule pipeline and logs a concise summary for observability. Each stage is isolated so failures are logged and do not crash the loop. run_forever() installs SIGINT / SIGTERM handlers, runs tick() at the configured poll interval, and keeps looping until interrupted. --once CLI flag runs a single tick and exits, useful for cron jobs or tests. Job scheduling & dispatching : the scheduler delegates all capacity calculations, MAP-Elites sampling, and Dramatiq job submission to loreley.scheduler.job_scheduler.JobScheduler , which: counts unfinished jobs in the database, enforces SCHEDULER_MAX_UNFINISHED_JOBS and the optional SCHEDULER_MAX_TOTAL_JOBS cap, calls MapElitesSampler.schedule_job(experiment_id=experiment.id) to produce new work, and marks rows as QUEUED and sends them to the run_evolution_job actor in priority order. MAP-Elites maintenance : ingestion of succeeded jobs is handled by loreley.scheduler.ingestion.MapElitesIngestion , which: scans for SUCCEEDED jobs that have not yet been fully ingested, extracts their result.commit_hash , fetches the corresponding git commit, and computes per-file change counts, calls MapElitesManager.ingest(...) with metrics and contextual metadata, and writes a detailed ingestion status block back under payload[\"ingestion\"][\"map_elites\"] (including attempts, delta, placement, and any error messages). Configuration \u00b6 The scheduler consumes the following Settings fields (all exposed as environment variables): SCHEDULER_REPO_ROOT : optional path to a read-only clone of the evolved repository; defaults to WORKER_REPO_WORKTREE . SCHEDULER_POLL_INTERVAL_SECONDS : delay between scheduler ticks (default: 30 seconds). SCHEDULER_MAX_UNFINISHED_JOBS : hard cap on the number of jobs that are not yet finished ( pending , queued , running ). SCHEDULER_SCHEDULE_BATCH_SIZE : maximum number of new jobs sampled from MAP-Elites per tick (bounded by the unused capacity). SCHEDULER_DISPATCH_BATCH_SIZE : number of pending jobs promoted to QUEUED and sent to Dramatiq per tick. SCHEDULER_INGEST_BATCH_SIZE : number of newly succeeded jobs ingested into MAP-Elites per tick. MAPELITES_EXPERIMENT_ROOT_COMMIT : optional git commit hash used as the logical root for the current experiment. When set, the scheduler ensures a CommitMetadata row exists for that commit and runs a one-off baseline evaluation to populate Metric rows and a compact root_evaluation block on the metadata, treating it as an experiment-wide baseline rather than inserting it into any MAP-Elites archive. During cold-start, when the archive is empty and no jobs exist yet, the scheduler first generates up to MAPELITES_SEED_POPULATION_SIZE seed evolution jobs from this root commit to form the initial population before switching to regular MAP-Elites sampling. CLI usage \u00b6 uv run python -m loreley.scheduler.main # continuous loop uv run python -m loreley.scheduler.main --once # single tick (cron / smoke tests) For details about the dedicated CLI wrapper script (including logging setup and recommended usage), see docs/script/run_scheduler.md . Running the module imports loreley.tasks.workers , so the Dramatiq broker is configured before the first dispatch. Rich console output summarises each tick, while Loguru records detailed diagnostics for ingestion, scheduling, and dispatching via the dedicated job_scheduler and ingestion helper classes. This makes the scheduler easy to supervise either interactively or under a process manager. For more detailed information about these helper modules, see: loreley.scheduler.job_scheduler.JobScheduler \u2014 job production and dispatch pipeline. loreley.scheduler.ingestion.MapElitesIngestion \u2014 result ingestion, root-commit initialisation, and MAP-Elites maintenance.","title":"Overview"},{"location":"loreley/scheduler/main/#loreleyschedulermain","text":"Central orchestration loop that keeps the Loreley evolution pipeline moving by coupling the MAP-Elites archive, the PostgreSQL job store, and the Dramatiq worker queue.","title":"loreley.scheduler.main"},{"location":"loreley/scheduler/main/#evolutionscheduler","text":"Purpose : continuously monitors unfinished jobs ( pending , queued , running ), schedules new work from the MAP-Elites archive when capacity allows, dispatches pending jobs to the Dramatiq run_evolution_job actor, and backfills the archive with freshly evaluated commits. Construction : EvolutionScheduler(settings=None) loads loreley.config.Settings , resolves the target repository root (preferring SCHEDULER_REPO_ROOT and falling back to WORKER_REPO_WORKTREE ), initialises a git repository handle, derives a Repository / Experiment pair via loreley.core.experiments.get_or_create_experiment() , wires MapElitesManager (scoped to that experiment_id ) plus MapElitesSampler with the same settings, and, when MAPELITES_EXPERIMENT_ROOT_COMMIT is set, delegates root-commit registration and baseline evaluation to loreley.scheduler.ingestion.MapElitesIngestion . Lifecycle : tick() runs the ingest \u2192 dispatch \u2192 measure \u2192 seed \u2192 schedule pipeline and logs a concise summary for observability. Each stage is isolated so failures are logged and do not crash the loop. run_forever() installs SIGINT / SIGTERM handlers, runs tick() at the configured poll interval, and keeps looping until interrupted. --once CLI flag runs a single tick and exits, useful for cron jobs or tests. Job scheduling & dispatching : the scheduler delegates all capacity calculations, MAP-Elites sampling, and Dramatiq job submission to loreley.scheduler.job_scheduler.JobScheduler , which: counts unfinished jobs in the database, enforces SCHEDULER_MAX_UNFINISHED_JOBS and the optional SCHEDULER_MAX_TOTAL_JOBS cap, calls MapElitesSampler.schedule_job(experiment_id=experiment.id) to produce new work, and marks rows as QUEUED and sends them to the run_evolution_job actor in priority order. MAP-Elites maintenance : ingestion of succeeded jobs is handled by loreley.scheduler.ingestion.MapElitesIngestion , which: scans for SUCCEEDED jobs that have not yet been fully ingested, extracts their result.commit_hash , fetches the corresponding git commit, and computes per-file change counts, calls MapElitesManager.ingest(...) with metrics and contextual metadata, and writes a detailed ingestion status block back under payload[\"ingestion\"][\"map_elites\"] (including attempts, delta, placement, and any error messages).","title":"EvolutionScheduler"},{"location":"loreley/scheduler/main/#configuration","text":"The scheduler consumes the following Settings fields (all exposed as environment variables): SCHEDULER_REPO_ROOT : optional path to a read-only clone of the evolved repository; defaults to WORKER_REPO_WORKTREE . SCHEDULER_POLL_INTERVAL_SECONDS : delay between scheduler ticks (default: 30 seconds). SCHEDULER_MAX_UNFINISHED_JOBS : hard cap on the number of jobs that are not yet finished ( pending , queued , running ). SCHEDULER_SCHEDULE_BATCH_SIZE : maximum number of new jobs sampled from MAP-Elites per tick (bounded by the unused capacity). SCHEDULER_DISPATCH_BATCH_SIZE : number of pending jobs promoted to QUEUED and sent to Dramatiq per tick. SCHEDULER_INGEST_BATCH_SIZE : number of newly succeeded jobs ingested into MAP-Elites per tick. MAPELITES_EXPERIMENT_ROOT_COMMIT : optional git commit hash used as the logical root for the current experiment. When set, the scheduler ensures a CommitMetadata row exists for that commit and runs a one-off baseline evaluation to populate Metric rows and a compact root_evaluation block on the metadata, treating it as an experiment-wide baseline rather than inserting it into any MAP-Elites archive. During cold-start, when the archive is empty and no jobs exist yet, the scheduler first generates up to MAPELITES_SEED_POPULATION_SIZE seed evolution jobs from this root commit to form the initial population before switching to regular MAP-Elites sampling.","title":"Configuration"},{"location":"loreley/scheduler/main/#cli-usage","text":"uv run python -m loreley.scheduler.main # continuous loop uv run python -m loreley.scheduler.main --once # single tick (cron / smoke tests) For details about the dedicated CLI wrapper script (including logging setup and recommended usage), see docs/script/run_scheduler.md . Running the module imports loreley.tasks.workers , so the Dramatiq broker is configured before the first dispatch. Rich console output summarises each tick, while Loguru records detailed diagnostics for ingestion, scheduling, and dispatching via the dedicated job_scheduler and ingestion helper classes. This makes the scheduler easy to supervise either interactively or under a process manager. For more detailed information about these helper modules, see: loreley.scheduler.job_scheduler.JobScheduler \u2014 job production and dispatch pipeline. loreley.scheduler.ingestion.MapElitesIngestion \u2014 result ingestion, root-commit initialisation, and MAP-Elites maintenance.","title":"CLI usage"},{"location":"loreley/tasks/broker/","text":"loreley.tasks.broker \u00b6 Helpers for configuring the Dramatiq Redis broker used by Loreley workers. Public API \u00b6 build_redis_broker(settings: Settings | None = None) -> RedisBroker Constructs a RedisBroker instance from the Settings object. It prefers TASKS_REDIS_URL when set and otherwise falls back to the individual TASKS_REDIS_HOST , TASKS_REDIS_PORT , TASKS_REDIS_DB , and TASKS_REDIS_PASSWORD fields, always attaching TASKS_REDIS_NAMESPACE as the Dramatiq namespace. setup_broker(settings: Settings | None = None) -> RedisBroker Wraps build_redis_broker() and calls dramatiq.set_broker(...) so that Dramatiq actors use the configured Redis broker. It logs a sanitised representation of the Redis connection (scheme, host, port, and DB index) along with the logical namespace, explicitly avoiding logging any credentials from TASKS_REDIS_URL or TASKS_REDIS_PASSWORD . broker A module-level RedisBroker instance created eagerly by calling setup_broker() at import time. Importing loreley.tasks.broker is therefore sufficient to configure the global Dramatiq broker; this side effect is relied on by loreley.tasks.workers when running worker processes.","title":"Broker"},{"location":"loreley/tasks/broker/#loreleytasksbroker","text":"Helpers for configuring the Dramatiq Redis broker used by Loreley workers.","title":"loreley.tasks.broker"},{"location":"loreley/tasks/broker/#public-api","text":"build_redis_broker(settings: Settings | None = None) -> RedisBroker Constructs a RedisBroker instance from the Settings object. It prefers TASKS_REDIS_URL when set and otherwise falls back to the individual TASKS_REDIS_HOST , TASKS_REDIS_PORT , TASKS_REDIS_DB , and TASKS_REDIS_PASSWORD fields, always attaching TASKS_REDIS_NAMESPACE as the Dramatiq namespace. setup_broker(settings: Settings | None = None) -> RedisBroker Wraps build_redis_broker() and calls dramatiq.set_broker(...) so that Dramatiq actors use the configured Redis broker. It logs a sanitised representation of the Redis connection (scheme, host, port, and DB index) along with the logical namespace, explicitly avoiding logging any credentials from TASKS_REDIS_URL or TASKS_REDIS_PASSWORD . broker A module-level RedisBroker instance created eagerly by calling setup_broker() at import time. Importing loreley.tasks.broker is therefore sufficient to configure the global Dramatiq broker; this side effect is relied on by loreley.tasks.workers when running worker processes.","title":"Public API"},{"location":"loreley/tasks/workers/","text":"loreley.tasks.workers \u00b6 Dramatiq task actors that drive the Loreley evolution worker. Evolution worker \u00b6 run_evolution_job(job_id: str) -> None A Dramatiq actor that runs a single evolution job via loreley.core.worker.evolution.EvolutionWorker . The queue name, retry policy, and time limit are derived from the task-related settings in loreley.config.Settings ( TASKS_QUEUE_NAME , TASKS_WORKER_MAX_RETRIES , and TASKS_WORKER_TIME_LIMIT_SECONDS ). The time limit is configured in milliseconds at the actor level: positive TASKS_WORKER_TIME_LIMIT_SECONDS values set a hard wall-clock limit, while values <= 0 disable the time limit (no hard cap). On execution, the actor: Validates and normalises the job_id argument. Logs a \u201cjob started\u201d event to both the rich console and loguru . Delegates execution to EvolutionWorker.run(...) . Handles worker-specific exceptions with distinct behaviours: JobLockConflict : logs that the job was skipped due to a lock conflict and returns without raising. JobPreconditionError : logs a warning and skips the job without raising (treating it as a non-retriable business error). EvolutionWorkerError : logs an error and re-raises so Dramatiq can apply its retry policy. Any other unexpected exception: logs with a full stack trace and re-raises as a defensive fallback. Logs a \u201cjob complete\u201d event including the resulting candidate commit hash on success. Importing loreley.tasks.workers also imports loreley.tasks.broker , which configures the global Dramatiq broker using the Redis settings in loreley.config.Settings . For details about the dedicated worker CLI wrapper script (including how it starts a single-process, single-threaded Dramatiq worker), see docs/script/run_worker.md .","title":"Workers"},{"location":"loreley/tasks/workers/#loreleytasksworkers","text":"Dramatiq task actors that drive the Loreley evolution worker.","title":"loreley.tasks.workers"},{"location":"loreley/tasks/workers/#evolution-worker","text":"run_evolution_job(job_id: str) -> None A Dramatiq actor that runs a single evolution job via loreley.core.worker.evolution.EvolutionWorker . The queue name, retry policy, and time limit are derived from the task-related settings in loreley.config.Settings ( TASKS_QUEUE_NAME , TASKS_WORKER_MAX_RETRIES , and TASKS_WORKER_TIME_LIMIT_SECONDS ). The time limit is configured in milliseconds at the actor level: positive TASKS_WORKER_TIME_LIMIT_SECONDS values set a hard wall-clock limit, while values <= 0 disable the time limit (no hard cap). On execution, the actor: Validates and normalises the job_id argument. Logs a \u201cjob started\u201d event to both the rich console and loguru . Delegates execution to EvolutionWorker.run(...) . Handles worker-specific exceptions with distinct behaviours: JobLockConflict : logs that the job was skipped due to a lock conflict and returns without raising. JobPreconditionError : logs a warning and skips the job without raising (treating it as a non-retriable business error). EvolutionWorkerError : logs an error and re-raises so Dramatiq can apply its retry policy. Any other unexpected exception: logs with a full stack trace and re-raises as a defensive fallback. Logs a \u201cjob complete\u201d event including the resulting candidate commit hash on success. Importing loreley.tasks.workers also imports loreley.tasks.broker , which configures the global Dramatiq broker using the Redis settings in loreley.config.Settings . For details about the dedicated worker CLI wrapper script (including how it starts a single-process, single-threaded Dramatiq worker), see docs/script/run_worker.md .","title":"Evolution worker"},{"location":"script/run_scheduler/","text":"script/run_scheduler.py \u00b6 Thin CLI wrapper for running the Loreley evolution scheduler. Purpose \u00b6 Configure global Loguru logging based on loreley.config.Settings.log_level . Delegate all CLI parsing and control flow to loreley.scheduler.main.main . Provide a convenient entrypoint for process managers and local development without having to remember the module path. The underlying scheduling logic, MAP-Elites integration, and database interaction are all implemented in loreley.scheduler.main.EvolutionScheduler . Behaviour \u00b6 Calls get_settings() to load Settings and derive the desired log level (via LOG_LEVEL ). Resets Loguru sinks and installs a stderr sink at the configured level, disabling backtraces and diagnosis in production-style runs. Resolves a log directory under <BASE>/logs/scheduler where <BASE> is: LOGS_BASE_DIR (expanded as a path) when set. the current working directory when LOGS_BASE_DIR is unset. Adds a rotating file sink at scheduler-YYYYMMDD.log inside that directory with rotation=\"10 MB\" and retention=\"14 days\" , so scheduler output is always persisted for later debugging. Binds an informational logger with module name script.run_scheduler . Forwards the CLI arguments to loreley.scheduler.main.main(argv) , which supports the --once flag to run a single scheduler tick. Exit codes are determined by loreley.scheduler.main.main : on success it returns 0 , while unexpected exceptions bubble up and cause a non-zero exit. CLI usage \u00b6 Recommended usage with uv : uv run python script/run_scheduler.py # continuous loop uv run python script/run_scheduler.py --once # single tick (cron / smoke tests) The wrapper is equivalent to invoking the module directly: uv run python -m loreley.scheduler.main # continuous loop uv run python -m loreley.scheduler.main --once # single tick Configuration \u00b6 The script relies on loreley.config.Settings : LOG_LEVEL controls the global Loguru log level for the scheduler process. LOGS_BASE_DIR (optional) overrides the base directory used for scheduler log files; when unset, logs are written under ./logs/scheduler relative to the current working directory. All scheduler-related fields ( SCHEDULER_* ) and database/Redis settings are consumed by loreley.scheduler.main and loreley.tasks.broker . See docs/loreley/config.md and docs/loreley/scheduler/main.md for details. The examples/evol_circle_packing.py helper simply delegates to this script when running the scheduler, so its runs use the same logging configuration and log file locations.","title":"Running the scheduler"},{"location":"script/run_scheduler/#scriptrun_schedulerpy","text":"Thin CLI wrapper for running the Loreley evolution scheduler.","title":"script/run_scheduler.py"},{"location":"script/run_scheduler/#purpose","text":"Configure global Loguru logging based on loreley.config.Settings.log_level . Delegate all CLI parsing and control flow to loreley.scheduler.main.main . Provide a convenient entrypoint for process managers and local development without having to remember the module path. The underlying scheduling logic, MAP-Elites integration, and database interaction are all implemented in loreley.scheduler.main.EvolutionScheduler .","title":"Purpose"},{"location":"script/run_scheduler/#behaviour","text":"Calls get_settings() to load Settings and derive the desired log level (via LOG_LEVEL ). Resets Loguru sinks and installs a stderr sink at the configured level, disabling backtraces and diagnosis in production-style runs. Resolves a log directory under <BASE>/logs/scheduler where <BASE> is: LOGS_BASE_DIR (expanded as a path) when set. the current working directory when LOGS_BASE_DIR is unset. Adds a rotating file sink at scheduler-YYYYMMDD.log inside that directory with rotation=\"10 MB\" and retention=\"14 days\" , so scheduler output is always persisted for later debugging. Binds an informational logger with module name script.run_scheduler . Forwards the CLI arguments to loreley.scheduler.main.main(argv) , which supports the --once flag to run a single scheduler tick. Exit codes are determined by loreley.scheduler.main.main : on success it returns 0 , while unexpected exceptions bubble up and cause a non-zero exit.","title":"Behaviour"},{"location":"script/run_scheduler/#cli-usage","text":"Recommended usage with uv : uv run python script/run_scheduler.py # continuous loop uv run python script/run_scheduler.py --once # single tick (cron / smoke tests) The wrapper is equivalent to invoking the module directly: uv run python -m loreley.scheduler.main # continuous loop uv run python -m loreley.scheduler.main --once # single tick","title":"CLI usage"},{"location":"script/run_scheduler/#configuration","text":"The script relies on loreley.config.Settings : LOG_LEVEL controls the global Loguru log level for the scheduler process. LOGS_BASE_DIR (optional) overrides the base directory used for scheduler log files; when unset, logs are written under ./logs/scheduler relative to the current working directory. All scheduler-related fields ( SCHEDULER_* ) and database/Redis settings are consumed by loreley.scheduler.main and loreley.tasks.broker . See docs/loreley/config.md and docs/loreley/scheduler/main.md for details. The examples/evol_circle_packing.py helper simply delegates to this script when running the scheduler, so its runs use the same logging configuration and log file locations.","title":"Configuration"},{"location":"script/run_worker/","text":"script/run_worker.py \u00b6 CLI wrapper that runs the Loreley evolution worker as a single Dramatiq process. Purpose \u00b6 Configure global Loguru logging based on loreley.config.Settings.log_level . Initialise the global Dramatiq Redis broker ( loreley.tasks.broker.broker ). Import loreley.tasks.workers so that the run_evolution_job actor is registered. Start a single-threaded Dramatiq Worker bound to the configured queue. Behaviour \u00b6 On startup the script: Calls get_settings() to load Settings . Configures Loguru to log to stderr using LOG_LEVEL as the threshold. Resolves a log directory under <BASE>/logs/worker where <BASE> is: LOGS_BASE_DIR (expanded as a path) when set. the current working directory when LOGS_BASE_DIR is unset. Adds a rotating file sink at worker-YYYYMMDD.log inside that directory with rotation=\"10 MB\" and retention=\"14 days\" , so worker output is always persisted for later debugging. Imports loreley.tasks.broker (which constructs and registers the Redis broker) and loreley.tasks.workers (which defines the run_evolution_job actor and its queue settings). Logs a short \u201cworker online\u201d message including TASKS_QUEUE_NAME and WORKER_REPO_WORKTREE . Creates a dramatiq.Worker with: broker set to the global Redis broker instance. worker_threads=1 to ensure a single-threaded execution model. Installs SIGINT / SIGTERM handlers that call worker.stop() for a graceful shutdown. Starts the worker and blocks with worker.join() until the process is stopped. Keyboard interrupts ( Ctrl+C ) are handled explicitly with a friendly shutdown message. CLI usage \u00b6 Typical usage with uv : uv run python script/run_worker.py The worker will begin consuming messages for the queue specified by TASKS_QUEUE_NAME (default: loreley.evolution ) in a single process with a single worker thread. Jobs are expected to be created and dispatched by the scheduler ( loreley.scheduler.main ). Configuration \u00b6 The script uses loreley.config.Settings for: Logging LOG_LEVEL : global Loguru level for worker logs. LOGS_BASE_DIR (optional): overrides the base directory used for worker log files; when unset, logs are written under ./logs/worker relative to the current working directory. Task queue / broker TASKS_REDIS_URL or ( TASKS_REDIS_HOST , TASKS_REDIS_PORT , TASKS_REDIS_DB , TASKS_REDIS_PASSWORD , TASKS_REDIS_NAMESPACE ). TASKS_QUEUE_NAME : queue name for the run_evolution_job actor. TASKS_WORKER_MAX_RETRIES , TASKS_WORKER_TIME_LIMIT_SECONDS : consumed by loreley.tasks.workers when configuring the actor. Worker repository WORKER_REPO_REMOTE_URL , WORKER_REPO_BRANCH , WORKER_REPO_WORKTREE , and related WORKER_REPO_* options used by loreley.core.worker.repository.WorkerRepository . For a full description of these settings, see docs/loreley/config.md and the worker module documentation in docs/loreley/tasks/workers.md . The examples/evol_circle_packing.py helper simply delegates to this script when running the worker, so its runs use the same logging configuration and log file locations.","title":"Running the worker"},{"location":"script/run_worker/#scriptrun_workerpy","text":"CLI wrapper that runs the Loreley evolution worker as a single Dramatiq process.","title":"script/run_worker.py"},{"location":"script/run_worker/#purpose","text":"Configure global Loguru logging based on loreley.config.Settings.log_level . Initialise the global Dramatiq Redis broker ( loreley.tasks.broker.broker ). Import loreley.tasks.workers so that the run_evolution_job actor is registered. Start a single-threaded Dramatiq Worker bound to the configured queue.","title":"Purpose"},{"location":"script/run_worker/#behaviour","text":"On startup the script: Calls get_settings() to load Settings . Configures Loguru to log to stderr using LOG_LEVEL as the threshold. Resolves a log directory under <BASE>/logs/worker where <BASE> is: LOGS_BASE_DIR (expanded as a path) when set. the current working directory when LOGS_BASE_DIR is unset. Adds a rotating file sink at worker-YYYYMMDD.log inside that directory with rotation=\"10 MB\" and retention=\"14 days\" , so worker output is always persisted for later debugging. Imports loreley.tasks.broker (which constructs and registers the Redis broker) and loreley.tasks.workers (which defines the run_evolution_job actor and its queue settings). Logs a short \u201cworker online\u201d message including TASKS_QUEUE_NAME and WORKER_REPO_WORKTREE . Creates a dramatiq.Worker with: broker set to the global Redis broker instance. worker_threads=1 to ensure a single-threaded execution model. Installs SIGINT / SIGTERM handlers that call worker.stop() for a graceful shutdown. Starts the worker and blocks with worker.join() until the process is stopped. Keyboard interrupts ( Ctrl+C ) are handled explicitly with a friendly shutdown message.","title":"Behaviour"},{"location":"script/run_worker/#cli-usage","text":"Typical usage with uv : uv run python script/run_worker.py The worker will begin consuming messages for the queue specified by TASKS_QUEUE_NAME (default: loreley.evolution ) in a single process with a single worker thread. Jobs are expected to be created and dispatched by the scheduler ( loreley.scheduler.main ).","title":"CLI usage"},{"location":"script/run_worker/#configuration","text":"The script uses loreley.config.Settings for: Logging LOG_LEVEL : global Loguru level for worker logs. LOGS_BASE_DIR (optional): overrides the base directory used for worker log files; when unset, logs are written under ./logs/worker relative to the current working directory. Task queue / broker TASKS_REDIS_URL or ( TASKS_REDIS_HOST , TASKS_REDIS_PORT , TASKS_REDIS_DB , TASKS_REDIS_PASSWORD , TASKS_REDIS_NAMESPACE ). TASKS_QUEUE_NAME : queue name for the run_evolution_job actor. TASKS_WORKER_MAX_RETRIES , TASKS_WORKER_TIME_LIMIT_SECONDS : consumed by loreley.tasks.workers when configuring the actor. Worker repository WORKER_REPO_REMOTE_URL , WORKER_REPO_BRANCH , WORKER_REPO_WORKTREE , and related WORKER_REPO_* options used by loreley.core.worker.repository.WorkerRepository . For a full description of these settings, see docs/loreley/config.md and the worker module documentation in docs/loreley/tasks/workers.md . The examples/evol_circle_packing.py helper simply delegates to this script when running the worker, so its runs use the same logging configuration and log file locations.","title":"Configuration"}]}